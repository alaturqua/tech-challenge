{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This project simulates an integration process between two SAP systems, focusing on local material data and process order data. The aim is to harmonize and preprocess SAP data to make it ready for further analysis and reporting.</p>"},{"location":"#project-goals","title":"Project Goals","text":"<ul> <li>Integrate data from SAP System 1 and SAP System 2.</li> <li>Harmonize and clean data (removing duplicates, renaming columns, adding primary keys).</li> <li>Perform data quality checks and handle sensitive data.</li> <li>Save the processed data for downstream analytics.</li> </ul>"},{"location":"#solution","title":"Solution","text":"<p>Documentation about SAP Tables was looked up at the following link: SAP Datasheet</p> <p>The SAP Datasheet contains detailed information about various SAP tables, including their fields, data types, and usage within SAP systems. This is helpful for understanding how to integrate and transform data from SAP systems.</p>"},{"location":"#usage-of-sap-tables","title":"Usage of SAP Tables","text":"<p>The following SAP tables are used in the integration process:</p> <p>for Local Material Data:</p> <ul> <li><code>MARC</code> - Plant Data for Material</li> <li><code>MARA</code> - General Material Data</li> <li><code>MBEW</code> - Material Valuation</li> <li><code>T001</code> - Company Codes</li> <li><code>T001W</code> - Plants/Branches</li> <li><code>T001K</code> - Valuation Area</li> </ul> <p>for Process Order Data:</p> <ul> <li><code>AFKO</code> - Order Header Data</li> <li><code>AFPO</code> - Order Item Data</li> <li><code>AUFK</code> - Order Master Data</li> <li><code>MARA</code> - General Material Data</li> </ul>"},{"location":"#diagrams-data-flows","title":"Diagrams &amp; Data Flows","text":"<p>The <code>diagrams</code> folder contains the following diagrams created by draw.io:</p> <ul> <li>diagram_local_material.png</li> <li>diagram_process_order.png</li> </ul> <p>Dataflow diagrams are made using mermaid.js syntax.</p>"},{"location":"#diagram-local-material","title":"Diagram - Local Material","text":"<p>The diagram displays how the columns from SAP Tables are joined, renamed and integrated to create the final Local Material dataset.</p> <p></p>"},{"location":"#data-flow-local-material","title":"Data Flow - Local Material","text":"<pre><code>flowchart LR\n    A1@{ shape: doc, label: \"PRE_MARA\" } --&gt; id1[(SAP System 1)]\n    B1@{ shape: doc, label: \"PRD_MARA\" } --&gt; id2[(SAP System 2)]\n    C1@{ shape: doc, label: \"PRE_MARC\" } --&gt; id1[(SAP System 1)]\n    D1@{ shape: doc, label: \"PRD_MARC\" } --&gt; id2[(SAP System 2)]\n    E1@{ shape: doc, label: \"PRE_MBEW\" } --&gt; id1[(SAP System 1)]\n    F1@{ shape: doc, label: \"PRD_MBEW\" } --&gt; id2[(SAP System 2)]\n    G1@{ shape: doc, label: \"PRE_T001\" } --&gt; id1[(SAP System 1)]\n    H1@{ shape: doc, label: \"PRD_T001\" } --&gt; id2[(SAP System 2)]\n    I1@{ shape: doc, label: \"PRE_T001W\" } --&gt; id1[(SAP System 1)]\n    J1@{ shape: doc, label: \"PRD_T001W\" } --&gt; id2[(SAP System 2)]\n    K1@{ shape: doc, label: \"PRE_T001K\" } --&gt; id1[(SAP System 1)]\n    L1@{ shape: doc, label: \"PRD_T001K\" } --&gt; id2[(SAP System 2)]\n    id1[(SAP System 1)] --&gt;|preprocess| C@{ shape: processes, label: \"Local Material 1\" }\n    id2[(SAP System 2)] --&gt;|preprocess| D@{ shape: processes, label: \"Local Material 2\" }\n    C@{ shape: processes, label: \"Local Material 1\" } --&gt;|union/join| E[Integration]\n    D@{ shape: processes, label: \"Local Material 2\" } --&gt;|union/join| E[Integration]\n    E[Integration] --&gt;|postprocess| F[Local Material]\n    F[Local Material] --&gt;|rename columns| G[Local Material]</code></pre> <p>The process begins with data coming from two separate SAP systems, SAP System 1 and SAP System 2. The data is preprocessed and harmonized to create a unified dataset. The final dataset is then postprocessed and renamed to create the final Local Material. The final dataset is saved as a CSV file in the <code>data/output/local_material</code> folder.</p>"},{"location":"#diagram-process-order","title":"Diagram - Process Order","text":"<p>The Diagram displays how data is joined and integrated between different SAP tables to create the final Process Order dataset.</p> <p></p>"},{"location":"#data-flow-process-order","title":"Data Flow - Process Order","text":"<pre><code>flowchart LR\n    A1@{ shape: doc, label: \"PRE_MARA\" } --&gt; id1[(SAP System 1)]\n    B1@{ shape: doc, label: \"PRD_MARA\" } --&gt; id2[(SAP System 2)]\n    C1@{ shape: doc, label: \"PRE_AFKO\" } --&gt; id1[(SAP System 1)]\n    D1@{ shape: doc, label: \"PRD_AFKO\" } --&gt; id2[(SAP System 2)]\n    E1@{ shape: doc, label: \"PRE_AFPO\" } --&gt; id1[(SAP System 1)]\n    F1@{ shape: doc, label: \"PRD_AFPO\" } --&gt; id2[(SAP System 2)]\n    G1@{ shape: doc, label: \"PRE_AUFK\" } --&gt; id1[(SAP System 1)]  \n    H1@{ shape: doc, label: \"PRD_AUFK\" } --&gt; id2[(SAP System 2)]  \n    id1[(SAP System 1)] --&gt;|preprocess| C@{ shape: processes, label: \"Process Order 1\" }\n    id2[(SAP System 2)] --&gt;|preprocess| D@{ shape: processes, label: \"Process Order 2\" }\n    C@{ shape: processes, label: \"Process Order 1\" } --&gt;|union/join| E[Integration]\n    D@{ shape: processes, label: \"Process Order 2\" } --&gt;|union/join| E[Integration]\n    E[Integration] --&gt;|postprocess| F[Process Order]\n    F[Process Order] --&gt;|rename columns| G[Process Order]</code></pre> <p>The process begins with data coming from two separate SAP systems, SAP System 1 and SAP System 2. The data is preprocessed and harmonized to create a unified dataset. The final dataset is then postprocessed and renamed to create the final Process Order. The final dataset is saved as a CSV file in the <code>data/output/process_order</code> folder.</p>"},{"location":"#what-can-be-improved","title":"What can be improved?","text":"<ul> <li>Add Metadata Columns: Add metadata columns to the table to track changes and updates and lineage.<ul> <li>E.g. Created Date, Updated Date, Created By, Updated By, Source System, Source Table</li> </ul> </li> <li>Define Schema on Read: Schema can be defined at read time to avoid inferring schema. Schema parameter is already implemented in the <code>read_csv_file</code> function.</li> <li>Data Quality Checks: More data quality checks can be added to ensure data consistency and accuracy. E.g. checking for null values, data types, etc.</li> <li>Performance Optimization: Performance can be optimized by using partitioning, bucketing, and caching.</li> <li>Using broadcast joins: For smaller tables, broadcast joins can be used to improve performance.</li> <li>Spark Configuration: Spark configuration can be optimized for better performance.</li> <li>CI/CD Pipeline: Implement a CI/CD pipeline for automated code linting, testing and deployment with approval process.</li> <li>Save to Database: Save the final dataframes to a database for further analysis and reporting.E.g. Open Table Formats, Delta Lake, Apache Iceberg.</li> <li>Monitoring and Alerting: Implement monitoring and alerting for the Spark application to track performance and errors. E.g. Prometheus, Grafana, ELK Stack. As well for the data pipeline.</li> <li>Processing Database: Save runtime metadata to a processing database for tracking and monitoring.<ul> <li>E.g. Run id, Start Time, End Time, Status, Error Message, Processed Rows, Rejected Rows, Source Table, Target Table</li> </ul> </li> <li>Credentials Management: Use key vaults or secret management tools to store and manage credentials securely. E.g. AWS Secrets Manager, Azure Key Vault.</li> </ul>"},{"location":"#setup","title":"Setup","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Docker and Docker Compose are chosen as the primary tools for running the code. This makes it easier to run the code in a containerized environment. Otherwise setting up spark and other dependencies can be cumbersome on a local machine.</p> <ul> <li>Docker -&gt; Install Docker</li> <li>Docker Compose -&gt; Install Docker Compose</li> <li>Docker Image -&gt; jupyter/pyspark-notebook:python-3.11</li> <li>Python 3.11</li> </ul>"},{"location":"#code-setup-and-structure","title":"Code Setup and Structure","text":"<p>The code is structured as follows:</p> <ul> <li><code>code</code> folder contains code related files.<ul> <li><code>main.py</code> is the entrypoint for the code.</li> <li><code>modules</code> folder contains the main modules.<ul> <li><code>local_material.py</code> contains the code for local material.</li> <li><code>process_order.py</code> contains the code for process order.</li> <li><code>utils.py</code> contains the utility functions.</li> </ul> </li> <li><code>tests</code> folder contains the tests for the modules.<ul> <li><code>test_local_material</code> folder contains the tests for local material.</li> <li><code>test_process_order</code> folder contains the tests for process order.</li> <li><code>test_utils.py</code> contains the tests for the utility functions.</li> <li><code>conftest.py</code> contains the fixtures for the tests.</li> </ul> </li> </ul> </li> <li><code>data</code> folder contains the data.</li> <li><code>diagrams</code> folder contains the diagrams.</li> <li><code>docs</code> folder contains the documentation.</li> <li><code>scripts</code> folder contains the scripts.</li> <li><code>requirements.txt</code> contains the dependencies.</li> </ul> <p>Final dataframes  local_material and process_order are saved in the <code>data/output</code> folder as <code>csv</code> files.</p>"},{"location":"#data-quality-checks","title":"Data Quality Checks","text":"<p>Uniqueness checks are performed on the primary keys of the dataframes using <code>check_columns_unique</code>. More checks could be added as functions to check for null values, data types, etc.</p> <p>Besides custom checks spark test frameworks can be utilized. Some examples frameworks are <code>Great Expectations</code>, <code>spark-expectations</code> or <code>soda-spark</code>.</p>"},{"location":"#sensitive-data-handling","title":"Sensitive Data Handling","text":"<p>Masking is performed on the sensitive columns using <code>mask_sensitive_columns</code> function. This is done to ensure that sensitive data is not exposed in the final dataframes. Should be done as early as possible in the pipeline.</p>"},{"location":"#code-quality-style-and-linting","title":"Code Quality, Style and Linting","text":"<p><code>ruff</code> library is used for linting and code quality checks. You can run it using <code>ruff</code> command.</p> <p>It is configured to use Google Style Code and Docstrings.</p> <pre><code>ruff check ./code\n</code></pre>"},{"location":"#generating-and-viewing-documentation","title":"Generating and viewing documentation","text":"<p><code>mkdocs</code> library is used to generate documentation. </p> <p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.</p> <p>To generate and view the documentation, run the following command:</p> <pre><code>docker-compose up docs --build --no-log-prefix\n</code></pre> <p>And visit http://localhost:8000 in your browser.</p> <p>To generate a <code>site</code> folder at the root level containing the documentation, which can be published on GitHub Pages, following command can be used:</p> <p>Note: This is not implemented in docker-compose.yaml yet.</p> <pre><code>mkdocs build\n</code></pre>"},{"location":"#running-the-tests","title":"Running the tests","text":"<ul> <li>The code is tested using Pytest. You can run the tests using the following command:</li> </ul> <pre><code>docker-compose up spark-test --build --no-log-prefix\n</code></pre> <p>Test results are generated and exported as HTML to logs folder.</p>"},{"location":"#running-the-code","title":"Running the code","text":"<p>The code is written in Python 3.11. <code>jupyter/pyspark-notebook</code> docker image is being used to run the code. This helps in running the code in a containerized environment. And avoids installing dependencies like spark, java, hadoop utils on the local machine.</p> <p>The entypoint for the code is <code>main.py</code>.</p> <p>You can run the spark application using the following command:</p> <pre><code>docker-compose up spark-run --build --no-log-prefix\n</code></pre>"},{"location":"#cleaning-up","title":"Cleaning up","text":"<p>To clean up the docker containers, run the following command:</p> <pre><code>docker-compose down\n</code></pre>"},{"location":"#project-tree","title":"Project Tree","text":"<p>The project tree is as follows:</p> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 Technical-Case-Study.pdf\n\u251c\u2500\u2500 code\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 modules\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 local_material.py\n\u2502   \u2502   \u251c\u2500\u2500 process_order.py\n\u2502   \u2502   \u2514\u2500\u2500 utils.py\n\u2502   \u2514\u2500\u2500 tests\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 conftest.py\n\u2502       \u251c\u2500\u2500 test_local_material\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 test_derive_intra_and_inter_primary_key.py\n\u2502       \u2502   \u251c\u2500\u2500 test_integration.py\n\u2502       \u2502   \u251c\u2500\u2500 test_post_prep_local_material.py\n\u2502       \u2502   \u251c\u2500\u2500 test_prep_general_material_data.py\n\u2502       \u2502   \u251c\u2500\u2500 test_prep_material_valuation.py\n\u2502       \u2502   \u251c\u2500\u2500 test_prep_plant_and_branches.py\n\u2502       \u2502   \u251c\u2500\u2500 test_prep_plant_data_for_material.py\n\u2502       \u2502   \u2514\u2500\u2500 test_prep_valuation_data.py\n\u2502       \u251c\u2500\u2500 test_process_order\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 test_integration.py\n\u2502       \u2502   \u251c\u2500\u2500 test_post_prep_process_order.py\n\u2502       \u2502   \u251c\u2500\u2500 test_prep_sap_general_material_data.py\n\u2502       \u2502   \u251c\u2500\u2500 test_prep_sap_order_header_data.py\n\u2502       \u2502   \u251c\u2500\u2500 test_prep_sap_order_item_data.py\n\u2502       \u2502   \u2514\u2500\u2500 test_prep_sap_order_master_data.py\n\u2502       \u2514\u2500\u2500 test_utils.py\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 output\n\u2502   \u2502   \u251c\u2500\u2500 local_material\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 _SUCCESS\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 part-00000-5064d33b-f9c6-437a-8cc3-ffeeaf7edfb4-c000.csv\n\u2502   \u2502   \u2514\u2500\u2500 process_order\n\u2502   \u2502       \u251c\u2500\u2500 _SUCCESS\n\u2502   \u2502       \u2514\u2500\u2500 part-00000-384db7a3-a438-4fa6-a7db-be4874b4a3b0-c000.csv\n\u2502   \u251c\u2500\u2500 system_1\n\u2502   \u2502   \u251c\u2500\u2500 PRE_AFKO.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRE_AFPO.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRE_AUFK.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRE_MARA.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRE_MARC.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRE_MBEW.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRE_T001.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRE_T001K.csv\n\u2502   \u2502   \u2514\u2500\u2500 PRE_T001W.csv\n\u2502   \u251c\u2500\u2500 system_2\n\u2502   \u2502   \u251c\u2500\u2500 PRD_AFKO.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRD_AFPO.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRD_AUFK.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRD_MARA.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRD_MARC.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRD_MBEW.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRD_T001.csv\n\u2502   \u2502   \u251c\u2500\u2500 PRD_T001K.csv\n\u2502   \u2502   \u2514\u2500\u2500 PRD_T001W.csv\n\u2502   \u2514\u2500\u2500 test\n\u2502       \u2514\u2500\u2500 test_data.csv\n\u251c\u2500\u2500 diagrams\n\u2502   \u251c\u2500\u2500 diagram_local_material.drawio\n\u2502   \u251c\u2500\u2500 diagram_local_material.png\n\u2502   \u251c\u2500\u2500 diagram_process_order.drawio\n\u2502   \u2514\u2500\u2500 diagram_process_order.png\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u2514\u2500\u2500 image\n\u2502   \u2502       \u251c\u2500\u2500 diagram_local_material.png\n\u2502   \u2502       \u2514\u2500\u2500 diagram_process_order.png\n\u2502   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 ruff.toml\n\u2514\u2500\u2500 scripts\n    \u251c\u2500\u2500 entrypoint.sh\n    \u2514\u2500\u2500 gen_ref_nav.py\n</code></pre>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>If you run into issues with Docker, check the following:</p> <p>Ensure Docker is running and you have sufficient system resources allocated. Verify that all files are correctly mounted into the Docker containers. Check the logs for any errors or exceptions.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>code<ul> <li>main</li> <li>modules<ul> <li>local_material</li> <li>process_order</li> <li>utils</li> </ul> </li> <li>tests<ul> <li>conftest</li> <li>test_local_material<ul> <li>test_derive_intra_and_inter_primary_key</li> <li>test_integration</li> <li>test_post_prep_local_material</li> <li>test_prep_general_material_data</li> <li>test_prep_material_valuation</li> <li>test_prep_plant_and_branches</li> <li>test_prep_plant_data_for_material</li> <li>test_prep_valuation_data</li> </ul> </li> <li>test_process_order<ul> <li>test_integration</li> <li>test_post_prep_process_order</li> <li>test_prep_sap_general_material_data</li> <li>test_prep_sap_order_header_data</li> <li>test_prep_sap_order_item_data</li> <li>test_prep_sap_order_master_data</li> </ul> </li> <li>test_utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/code/","title":"code","text":""},{"location":"reference/code/main/","title":"main","text":"<p>Main script to run the technical challenge.</p> <p>Exercise Overview: You will work with data from two SAP systems that have similar data sources. Your task is to process and integrate this data to provide a unified view for supply chain insights.</p> <p>The exercise involves: * Processing Local Material data.Processing Process Order data. * Ensuring both datasets have the same schema for harmonization across systems. * Writing modular, reusable code with proper documentation. * Following best-in-class principles for flexibility and maintainability.</p> <p>Note: You will create two Python scripts (local_material.py and process_order.py) for each system, i.e. in total of four scripts (2 systems per modeled entity/event).</p> <p>General Instructions Work on both SAP systems: * Perform all the steps for both systems to ensure consistency * Enable and accomplish data harmonization through a common data model</p> <p>Focus on data fields and transformations: * Pay attention to the required fields and the transformations applied to them. * Document your code: Include comments explaining why certain modules and functions are used. * Follow best practices: Write modular code, handle exceptions, and ensure reusability.</p> <p>Detailed instructions see attached PDF</p>"},{"location":"reference/code/main/#code.main.harmonize_data","title":"<code>harmonize_data()</code>","text":"<p>Harmonize Local Material and Process Order data from two SAP systems.</p> Source code in <code>code/main.py</code> <pre><code>def harmonize_data():\n    \"\"\"Harmonize Local Material and Process Order data from two SAP systems.\"\"\"\n    # Process local material data\n    local_material = process_local_material()\n    local_material.printSchema()\n    local_material.show(5)\n    logger.info(f\"Local Material data processed successfully. Count: {local_material.count()}\")\n\n    # Harmonize column names for local material\n    local_material_rename_map = {\n        \"MATNR\": \"material_number\",\n        \"WERKS\": \"plant\",\n        \"SOURCE_SYSTEM_ERP\": \"source_system_erp\",\n        \"MANDT\": \"client\",\n        \"MEINS\": \"base_unit_of_measure\",\n        \"GLOBAL_MATERIAL_NUMBER\": \"global_material_number\",\n        \"BWKEY\": \"valuation_area\",\n        \"NAME1\": \"branch_name\",\n        \"VPRSV\": \"price_control_indicator\",\n        \"VERPR\": \"moving_average_price\",\n        \"STPRS\": \"standard_price\",\n        \"PEINH\": \"price_unit\",\n        \"BKLAS\": \"valuation_class\",\n        \"BUKRS\": \"company_code\",\n        \"WAERS\": \"currency_key\",\n        \"primary_key_intra\": \"primary_key_intra\",\n        \"primary_key_inter\": \"primary_key_inter\",\n        \"global_mtl_id\": \"global_mtl_id\",\n    }\n\n    for old_name, new_name in local_material_rename_map.items():\n        local_material = local_material.withColumnRenamed(old_name, new_name)\n\n    # Postprocess process order data\n    process_order = process_process_order()\n    process_order.printSchema()\n    process_order.show(5)\n    logger.info(f\"Process Order data processed successfully. Count: {process_order.count()}\")\n\n    # Harmonize column names for process order\n    process_order_rename_map = {\n        \"MATNR\": \"material_number\",\n        \"AUFNR\": \"order_number\",\n        \"SOURCE_SYSTEM_ERP\": \"source_system_erp\",\n        \"MANDT\": \"client\",\n        \"GLTRP\": \"planned_start_date\",\n        \"GSTRP\": \"planned_finish_date\",\n        \"FTRMS\": \"scheduled_start_date\",\n        \"GLTRS\": \"actual_start_date\",\n        \"GSTRS\": \"actual_finish_date\",\n        \"GSTRI\": \"planned_start_date_internal\",\n        \"GETRI\": \"planned_finish_date_internal\",\n        \"GLTRI\": \"actual_start_date_internal\",\n        \"FTRMI\": \"scheduled_start_date_internal\",\n        \"FTRMP\": \"scheduled_finish_date_internal\",\n        \"DISPO\": \"mrp_controller\",\n        \"FEVOR\": \"production_supervisor\",\n        \"PLGRP\": \"planner_group\",\n        \"FHORI\": \"scheduling_direction\",\n        \"AUFPL\": \"routing_number\",\n        \"start_date\": \"start_date\",\n        \"finish_date\": \"finish_date\",\n        \"POSNR\": \"item_number\",\n        \"DWERK\": \"plant\",\n        \"MEINS\": \"base_unit_of_measure\",\n        \"KDAUF\": \"sales_order\",\n        \"KDPOS\": \"sales_order_item\",\n        \"LTRMI\": \"requested_delivery_date\",\n        \"OBJNR\": \"object_number\",\n        \"ERDAT\": \"creation_date\",\n        \"ERNAM\": \"created_by\",\n        \"AUART\": \"order_type\",\n        \"ZZGLTRP_ORIG\": \"original_planned_start_date\",\n        \"ZZPRO_TEXT\": \"production_order_text\",\n        \"GLOBAL_MATERIAL_NUMBER\": \"global_material_number\",\n        \"NTGEW\": \"net_weight\",\n        \"MTART\": \"material_type\",\n        \"primary_key_intra\": \"primary_key_intra\",\n        \"primary_key_inter\": \"primary_key_inter\",\n        \"on_time_flag\": \"on_time_flag\",\n        \"actual_on_time_deviation\": \"actual_on_time_deviation\",\n        \"late_delivery_bucket\": \"late_delivery_bucket\",\n        \"mto_vs_mts_flag\": \"mto_vs_mts_flag\",\n        \"order_finish_timestamp\": \"order_finish_timestamp\",\n        \"order_start_timestamp\": \"order_start_timestamp\",\n    }\n\n    for old_name, new_name in process_order_rename_map.items():\n        process_order = process_order.withColumnRenamed(old_name, new_name)\n\n    local_material.printSchema()\n    local_material.show(5)\n    local_material.coalesce(1).write.csv(\n        str(parent_dir_name / \"data\" / \"output\" / \"local_material\"), mode=\"overwrite\", header=True\n    )\n    process_order.printSchema()\n    process_order.show(5)\n    process_order.coalesce(1).write.csv(\n        str(parent_dir_name / \"data\" / \"output\" / \"process_order\"), mode=\"overwrite\", header=True\n    )\n</code></pre>"},{"location":"reference/code/main/#code.main.main","title":"<code>main()</code>","text":"<p>Main function to run the technical challenge.</p> Source code in <code>code/main.py</code> <pre><code>def main():\n    \"\"\"Main function to run the technical challenge.\"\"\"\n    harmonize_data()\n</code></pre>"},{"location":"reference/code/main/#code.main.process_local_material","title":"<code>process_local_material()</code>","text":"<p>Process Local Material data from two SAP systems.</p> Source code in <code>code/main.py</code> <pre><code>def process_local_material() -&gt; DataFrame:\n    \"\"\"Process Local Material data from two SAP systems.\"\"\"\n    logger.info(\"Processing Local Material data\")\n    logger.info(\"Loading data from system_1\")\n\n    # Load data\n    pre_mara = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_MARA.csv\")\n    pre_mbew = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_MBEW.csv\")\n    pre_marc = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_MARC.csv\")\n    pre_t001k = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_T001K.csv\")\n    pre_t001w = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_T001W.csv\")\n    pre_t001 = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_T001.csv\")\n\n    # Process data\n    logger.info(\"Processing data from system_1\")\n    pre_mara = prep_general_material_data(\n        pre_mara,\n        col_mara_global_material_number=\"ZZMDGM\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n    pre_mbew = prep_material_valuation(pre_mbew)\n    pre_marc = prep_plant_data_for_material(pre_marc, check_deletion_flag_is_null=True, drop_duplicate_records=True)\n    pre_t001k = prep_valuation_data(pre_t001k)\n    pre_t001w = prep_plant_and_branches(pre_t001w)\n    pre_t001 = prep_company_codes(pre_t001)\n\n    logger.info(\"Loading data from system_2\")\n    # Load data\n    prd_mara = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_MARA.csv\")\n    prd_mbew = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_MBEW.csv\")\n    prd_marc = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_MARC.csv\")\n    prd_t001k = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_T001K.csv\")\n    prd_t001w = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_T001W.csv\")\n    prd_t001 = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_T001.csv\")\n\n    # Process data\n    logger.info(\"Processing data from system_2\")\n    prd_mara = prep_general_material_data(\n        prd_mara,\n        col_mara_global_material_number=\"ZZMDGM\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n    prd_mbew = prep_material_valuation(prd_mbew)\n    prd_marc = prep_plant_data_for_material(prd_marc, check_deletion_flag_is_null=True, drop_duplicate_records=True)\n    prd_t001k = prep_valuation_data(prd_t001k)\n    prd_t001w = prep_plant_and_branches(prd_t001w)\n    prd_t001 = prep_company_codes(prd_t001)\n\n    logger.info(\"Union data from both systems\")\n    union_mara = pre_mara.unionByName(prd_mara)\n    union_mbew = pre_mbew.unionByName(prd_mbew)\n    union_marc = pre_marc.unionByName(prd_marc)\n    union_t001k = pre_t001k.unionByName(prd_t001k)\n    union_t001w = pre_t001w.unionByName(prd_t001w)\n    union_t001 = pre_t001.unionByName(prd_t001)\n\n    # Post-process data\n    integrated_data = integration_local_material(\n        union_mara, union_mbew, union_marc, union_t001k, union_t001w, union_t001\n    )\n\n    local_material = post_prep_local_material(integrated_data)\n\n    schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"NAME1\", StringType(), True),\n            StructField(\"VPRSV\", StringType(), True),\n            StructField(\"VERPR\", DoubleType(), True),\n            StructField(\"STPRS\", DoubleType(), True),\n            StructField(\"PEINH\", DoubleType(), True),\n            StructField(\"BKLAS\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n            StructField(\"WAERS\", StringType(), True),\n            StructField(\"primary_key_intra\", StringType(), False),\n            StructField(\"primary_key_inter\", StringType(), False),\n            StructField(\"mtl_plant_emd\", StringType(), False),\n            StructField(\"global_mtl_id\", StringType(), True),\n        ]\n    )\n\n    sensitive_columns = [\n        \"GLOBAL_MATERIAL_NUMBER\",  # Global material number\n        \"NAME1\",  # Branch name\n        \"BUKRS\",  # Company code\n        \"mtl_plant_emd\",  # Material plant EMD\n        \"MATNR\",  # Material number\n    ]\n\n    local_material = mask_sensitive_columns(local_material, sensitive_columns)\n    check_columns_unique(local_material, [\"primary_key_intra\", \"primary_key_inter\"])\n\n    logger.info(\"Validating schema for Local Material data\")\n    assertSchemaEqual(local_material.schema, schema)\n\n    return local_material\n</code></pre>"},{"location":"reference/code/main/#code.main.process_process_order","title":"<code>process_process_order()</code>","text":"<p>Process Process Order data from two SAP systems.</p> Source code in <code>code/main.py</code> <pre><code>def process_process_order() -&gt; DataFrame:\n    \"\"\"Process Process Order data from two SAP systems.\"\"\"\n    # Load data\n    logger.info(\"Processing Process Order data\")\n    logger.info(\"Loading data from system_1\")\n\n    pre_afko = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_AFKO.csv\")\n    pre_afpo = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_AFPO.csv\")\n    pre_aufk = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_AUFK.csv\")\n    pre_mara = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_MARA.csv\")\n\n    # Process data\n    logger.info(\"Processing data from system_1\")\n    pre_afko = prep_sap_order_header_data(pre_afko)\n    pre_afpo = prep_sap_order_item(pre_afpo)\n    pre_aufk = prep_sap_order_master_data(pre_aufk)\n    pre_mara = prep_sap_general_material_data(pre_mara, col_global_material=\"ZZMDGM\")\n\n    logger.info(\"Loading data from system_2\")\n    prd_afko = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_AFKO.csv\")\n    prd_afpo = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_AFPO.csv\")\n    prd_aufk = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_AUFK.csv\")\n    prd_mara = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_MARA.csv\")\n\n    # Process data\n    logger.info(\"Processing data from system_2\")\n    prd_afko = prep_sap_order_header_data(prd_afko)\n    prd_afpo = prep_sap_order_item(prd_afpo)\n    prd_aufk = prep_sap_order_master_data(prd_aufk)\n    prd_mara = prep_sap_general_material_data(prd_mara, col_global_material=\"ZZMDGM\")\n\n    logger.info(\"Union data from both systems\")\n    union_afko = pre_afko.unionByName(prd_afko)\n    union_afpo = pre_afpo.unionByName(prd_afpo)\n    union_aufk = pre_aufk.unionByName(prd_aufk)\n    union_mara = pre_mara.unionByName(prd_mara)\n\n    # Post-process data\n    integrated_data = integration_process_order(union_afko, union_afpo, union_aufk, union_mara)\n\n    process_order = post_prep_process_order(integrated_data)\n\n    expected_schema = StructType(\n        [\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"GLTRP\", DateType(), True),\n            StructField(\"GSTRP\", DateType(), True),\n            StructField(\"FTRMS\", DateType(), True),\n            StructField(\"GLTRS\", DateType(), True),\n            StructField(\"GSTRS\", DateType(), True),\n            StructField(\"GSTRI\", DateType(), True),\n            StructField(\"GETRI\", DateType(), True),\n            StructField(\"GLTRI\", DateType(), True),\n            StructField(\"FTRMI\", DateType(), True),\n            StructField(\"FTRMP\", DateType(), True),\n            StructField(\"DISPO\", StringType(), True),\n            StructField(\"FEVOR\", StringType(), True),\n            StructField(\"PLGRP\", StringType(), True),\n            StructField(\"FHORI\", StringType(), True),\n            StructField(\"AUFPL\", StringType(), True),\n            StructField(\"start_date\", DateType(), True),\n            StructField(\"finish_date\", DateType(), True),\n            StructField(\"POSNR\", StringType(), True),\n            StructField(\"DWERK\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"KDAUF\", StringType(), True),\n            StructField(\"KDPOS\", StringType(), True),\n            StructField(\"LTRMI\", DateType(), True),\n            StructField(\"OBJNR\", StringType(), True),\n            StructField(\"ERDAT\", DateType(), True),\n            StructField(\"ERNAM\", StringType(), True),\n            StructField(\"AUART\", StringType(), True),\n            StructField(\"ZZGLTRP_ORIG\", DateType(), True),\n            StructField(\"ZZPRO_TEXT\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n            StructField(\"NTGEW\", DoubleType(), True),\n            StructField(\"MTART\", StringType(), True),\n            StructField(\"primary_key_intra\", StringType(), False),\n            StructField(\"primary_key_inter\", StringType(), False),\n            StructField(\"on_time_flag\", IntegerType(), True),\n            StructField(\"actual_on_time_deviation\", DoubleType(), True),\n            StructField(\"late_delivery_bucket\", StringType(), False),\n            StructField(\"mto_vs_mts_flag\", StringType(), False),\n            StructField(\"order_finish_timestamp\", TimestampType(), True),\n            StructField(\"order_start_timestamp\", TimestampType(), True),\n        ]\n    )\n\n    sensitive_columns = [\n        \"AUFNR\",  # Order number\n        \"MATNR\",  # Material number\n        \"KDPOS\",  # Sales order item\n        \"KDAUF\",  # Sales order\n        \"ERNAM\",  # Created by\n        \"OBJNR\",  # Object number\n    ]\n\n    process_order = mask_sensitive_columns(process_order, sensitive_columns)\n\n    check_columns_unique(process_order, [\"primary_key_intra\", \"primary_key_inter\"])\n    assertSchemaEqual(process_order.schema, expected_schema)\n    return process_order\n</code></pre>"},{"location":"reference/code/modules/","title":"modules","text":""},{"location":"reference/code/modules/local_material/","title":"local_material","text":"<p>Module to prepare local material data for harmonization across systems.</p>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.derive_intra_and_inter_primary_key","title":"<code>derive_intra_and_inter_primary_key(df)</code>","text":"<p>Derive primary keys for harmonized data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with harmonized data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with derived primary keys.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def derive_intra_and_inter_primary_key(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Derive primary keys for harmonized data.\n\n    Args:\n        df (DataFrame): DataFrame with harmonized data.\n\n    Returns:\n        DataFrame: DataFrame with derived primary keys.\n    \"\"\"\n    logger.info(\"Deriving primary keys\")\n    # Replace nulls with empty strings before concatenation\n    return df.withColumn(\n        \"primary_key_intra\",\n        F.concat_ws(\"-\", F.coalesce(F.col(\"MATNR\"), F.lit(\"\")), F.coalesce(F.col(\"WERKS\"), F.lit(\"\"))),\n    ).withColumn(\n        \"primary_key_inter\",\n        F.concat_ws(\n            \"-\",\n            F.coalesce(F.col(\"SOURCE_SYSTEM_ERP\"), F.lit(\"\")),\n            F.coalesce(F.col(\"MATNR\"), F.lit(\"\")),\n            F.coalesce(F.col(\"WERKS\"), F.lit(\"\")),\n        ),\n    )\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.integration","title":"<code>integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)</code>","text":"<p>Integrate data from various SAP tables to create a comprehensive dataset.</p> DataFrames from the following SAP tables: <p>\u25cb SAP MARA table (General Material Data) \u25cb SAP MBEW table (Material Valuation) \u25cb SAP MARC table (Plant Data for Material) \u25cb SAP T001K table (Valuation Area) \u25cb SAP T001W table (Plant/Branch) \u25cb SAP T001 table (Company Codes)</p> Transformation <p>\u25cb Join operations:     \u25aa sap_marc left join sap_mara on MATNR.     \u25aa left join sap_t001w on MANDT and WERKS.     \u25aa left join sap_mbew on MANDT, MATNR, and BWKEY.     \u25aa left join sap_t001k on MANDT and BWKEY.     \u25aa left join sap_t001 on MANDT and BUKRS.</p> <p>Parameters:</p> Name Type Description Default <code>sap_mara</code> <code>DataFrame</code> <p>General Material Data.</p> required <code>sap_mbew</code> <code>DataFrame</code> <p>Material Valuation Data.</p> required <code>sap_marc</code> <code>DataFrame</code> <p>Plant Data for Material.</p> required <code>sap_t001k</code> <code>DataFrame</code> <p>Valuation Area Data.</p> required <code>sap_t001w</code> <code>DataFrame</code> <p>Plant and Branches Data.</p> required <code>sap_t001</code> <code>DataFrame</code> <p>Company Codes Data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Integrated DataFrame with all necessary information.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def integration(\n    sap_mara: DataFrame,\n    sap_mbew: DataFrame,\n    sap_marc: DataFrame,\n    sap_t001k: DataFrame,\n    sap_t001w: DataFrame,\n    sap_t001: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"Integrate data from various SAP tables to create a comprehensive dataset.\n\n    Input Data: DataFrames from the following SAP tables:\n        \u25cb SAP MARA table (General Material Data)\n        \u25cb SAP MBEW table (Material Valuation)\n        \u25cb SAP MARC table (Plant Data for Material)\n        \u25cb SAP T001K table (Valuation Area)\n        \u25cb SAP T001W table (Plant/Branch)\n        \u25cb SAP T001 table (Company Codes)\n\n    Transformation:\n      \u25cb Join operations:\n          \u25aa sap_marc left join sap_mara on MATNR.\n          \u25aa left join sap_t001w on MANDT and WERKS.\n          \u25aa left join sap_mbew on MANDT, MATNR, and BWKEY.\n          \u25aa left join sap_t001k on MANDT and BWKEY.\n          \u25aa left join sap_t001 on MANDT and BUKRS.\n\n    Args:\n        sap_mara (DataFrame): General Material Data.\n        sap_mbew (DataFrame): Material Valuation Data.\n        sap_marc (DataFrame): Plant Data for Material.\n        sap_t001k (DataFrame): Valuation Area Data.\n        sap_t001w (DataFrame): Plant and Branches Data.\n        sap_t001 (DataFrame): Company Codes Data.\n\n    Returns:\n        DataFrame: Integrated DataFrame with all necessary information.\n    \"\"\"\n    logger.info(\"Joining SAP tables\")\n    # Join the DataFrames to create a comprehensive dataset\n    return (\n        sap_marc.join(sap_mara, on=\"MATNR\", how=\"left\")  # Join with sap_mara\n        .join(sap_t001w, on=[\"MANDT\", \"WERKS\"], how=\"left\")  # Join with sap_t001w\n        .join(sap_mbew, on=[\"MANDT\", \"MATNR\", \"BWKEY\"], how=\"left\")  # Join with sap_mbew\n        .join(sap_t001k, on=[\"MANDT\", \"BWKEY\"], how=\"left\")  # Join with sap_t001k\n        .join(sap_t001, on=[\"MANDT\", \"BUKRS\"], how=\"left\")  # Join with sap_t001\n        .select(\n            \"MANDT\",\n            \"SOURCE_SYSTEM_ERP\",\n            \"MATNR\",\n            \"WERKS\",\n            \"MEINS\",\n            \"GLOBAL_MATERIAL_NUMBER\",\n            \"BWKEY\",\n            \"NAME1\",\n            \"VPRSV\",\n            \"VERPR\",\n            \"STPRS\",\n            \"PEINH\",\n            \"BKLAS\",\n            \"BUKRS\",\n            \"WAERS\",\n        )\n    )\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.post_prep_local_material","title":"<code>post_prep_local_material(df)</code>","text":"<p>Post-process the integrated DataFrame to create final local material data.</p> <p>Input Data: Resulting DataFrame from the integration step.</p> Transformation <p>\u25cb Create mtl_plant_emd: Concatenate WERKS and NAME1 with a hyphen. \u25cb Assign global_mtl_id from MATNR or the global material number, as appropriate. \u25cb Derive two Derive the following keys (intra represents the primary key of one system and inter the primary key the harmonized view.</p> <pre><code>\u25aa Primary Key (primary_key_intra): Concatenate MATNR and WERKS.\n\u25aa Primary Key (primary_key_inter): Concatenate SOURCE_SYSTEM_ERP, MATNR, and WERKS.\n</code></pre> <p>\u25cb Handle Duplicates:     \u25aa Add a temporary column no_of_duplicates indicating duplicate counts.     \u25aa Drop Duplicates based on SOURCE_SYSTEM_ERP, MATNR, and WERKS.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Integrated DataFrame from the integration step.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Final DataFrame with post-processed local material data.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def post_prep_local_material(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Post-process the integrated DataFrame to create final local material data.\n\n    Input Data: Resulting DataFrame from the integration step.\n\n    Transformation:\n      \u25cb Create mtl_plant_emd: Concatenate WERKS and NAME1 with a hyphen.\n      \u25cb Assign global_mtl_id from MATNR or the global material number, as appropriate.\n      \u25cb Derive two Derive the following keys (intra represents the primary key of one system and inter the primary key\n      the harmonized view.\n\n          \u25aa Primary Key (primary_key_intra): Concatenate MATNR and WERKS.\n          \u25aa Primary Key (primary_key_inter): Concatenate SOURCE_SYSTEM_ERP, MATNR, and WERKS.\n      \u25cb Handle Duplicates:\n          \u25aa Add a temporary column no_of_duplicates indicating duplicate counts.\n          \u25aa Drop Duplicates based on SOURCE_SYSTEM_ERP, MATNR, and WERKS.\n\n    Args:\n        df (DataFrame): Integrated DataFrame from the integration step.\n\n    Returns:\n        DataFrame: Final DataFrame with post-processed local material data.\n    \"\"\"\n    logger.info(\"Post-processing Local Material data\")\n    # Add timestamp column to the DataFrame\n    df_with_timestamp = df.withColumn(\"timestamp\", F.current_timestamp())\n\n    # Window spec to get the latest record based on timestamp\n    window_spec = Window.partitionBy(\"SOURCE_SYSTEM_ERP\", \"MATNR\", \"WERKS\").orderBy(F.col(\"timestamp\").desc())\n\n    df_with_primary_keys = derive_intra_and_inter_primary_key(df_with_timestamp)\n\n    return (\n        df_with_primary_keys.withColumn(\"mtl_plant_emd\", F.concat_ws(\"-\", F.col(\"WERKS\"), F.col(\"NAME1\")))\n        .withColumn(\"global_mtl_id\", F.coalesce(F.col(\"GLOBAL_MATERIAL_NUMBER\"), F.col(\"MATNR\")))\n        .withColumn(\"row_number\", F.row_number().over(window_spec))\n        .filter(F.col(\"row_number\") == 1)\n        .drop(\"row_number\")\n        .drop(\"timestamp\")\n    )\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_company_codes","title":"<code>prep_company_codes(df)</code>","text":"<p>Prepare Company Codes data for harmonization across systems.</p> <p>Input Data: SAP T001 table (Company Codes)</p> Transformation <p>\u25cb Select the required columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Company Codes data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Company Codes data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def prep_company_codes(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Company Codes data for harmonization across systems.\n\n    Input Data: SAP T001 table (Company Codes)\n\n    Transformation:\n      \u25cb Select the required columns.\n\n    Args:\n        df (DataFrame): DataFrame with Company Codes data.\n\n    Returns:\n        DataFrame: DataFrame with prepared Company Codes data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    logger.info(\"Preparing Company Codes data\")\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"MANDT\",  # Client\n        \"BUKRS\",  # Company Code\n        \"WAERS\",  # Currency Key\n    ]\n\n    return df.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_general_material_data","title":"<code>prep_general_material_data(df, col_mara_global_material_number, check_old_material_number_is_valid=True, check_material_is_not_deleted=True)</code>","text":"<p>Prepare General Material data for harmonization across systems.</p> <p>Input Data: SAP MARA table (General Material Data)</p> Transformation <p>\u25cb Filter materials:     \u25aa Old Material Number (BISMT) is not in [\"ARCHIVE\", \"DUPLICATE\", \"RENUMBERED\"] or is null.     \u25aa Deletion flag (LVORM) is null or empty. \u25cb Select the required columns. \u25cb Rename the global material number column to a consistent name, if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with General Material data.</p> required <code>col_mara_global_material_number</code> <code>str</code> <p>Column name with the global material number.</p> required <code>check_old_material_number_is_valid</code> <code>bool</code> <p>Check if the old material number is valid.</p> <code>True</code> <code>check_material_is_not_deleted</code> <code>bool</code> <p>Check if the material is not deleted.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared General Material data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> <code>TypeError</code> <p>If the column name with the global material number is not a string.</p> <code>TypeError</code> <p>If the check for the old material number is not a boolean.</p> <code>TypeError</code> <p>If the check for the material deletion flag is not a boolean.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def prep_general_material_data(\n    df: DataFrame,\n    col_mara_global_material_number: str,\n    check_old_material_number_is_valid: bool = True,\n    check_material_is_not_deleted: bool = True,\n) -&gt; DataFrame:\n    \"\"\"Prepare General Material data for harmonization across systems.\n\n    Input Data: SAP MARA table (General Material Data)\n\n    Transformation:\n        \u25cb Filter materials:\n            \u25aa Old Material Number (BISMT) is not in [\"ARCHIVE\", \"DUPLICATE\", \"RENUMBERED\"] or is null.\n            \u25aa Deletion flag (LVORM) is null or empty.\n        \u25cb Select the required columns.\n        \u25cb Rename the global material number column to a consistent name, if necessary.\n\n    Args:\n        df (DataFrame): DataFrame with General Material data.\n        col_mara_global_material_number (str): Column name with the global material number.\n        check_old_material_number_is_valid (bool): Check if the old material number is valid.\n        check_material_is_not_deleted (bool): Check if the material is not deleted.\n\n    Returns:\n        DataFrame: DataFrame with prepared General Material data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n        TypeError: If the column name with the global material number is not a string.\n        TypeError: If the check for the old material number is not a boolean.\n        TypeError: If the check for the material deletion flag is not a boolean.\n    \"\"\"\n    logger.info(\"Preparing General Material data\")\n    if None or not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n    if None or not isinstance(col_mara_global_material_number, str):\n        error_message = \"col_mara_global_material_number must be a string\"\n        raise TypeError(error_message)\n    if None or not isinstance(check_old_material_number_is_valid, bool):\n        error_message = \"check_old_material_number_is_valid must be a boolean\"\n        raise TypeError(error_message)\n    if None or not isinstance(check_material_is_not_deleted, bool):\n        error_message = \"check_material_is_not_deleted must be a boolean\"\n        raise TypeError(error_message)\n\n    # Start with the original DataFrame\n    sap_mara = df\n\n    # Apply old material number check if enabled\n    if check_old_material_number_is_valid:\n        sap_mara = sap_mara.filter(\n            F.col(\"BISMT\").isNull() | ~F.col(\"BISMT\").isin([\"ARCHIVE\", \"DUPLICATE\", \"RENUMBERED\"])\n        )\n\n    # Apply deletion flag check if enabled\n    if check_material_is_not_deleted:\n        sap_mara = sap_mara.filter(F.col(\"LVORM\").isNull() | (F.trim(F.col(\"LVORM\")).cast(\"string\") == F.lit(\"\")))\n\n    # Select and rename columns\n    columns = [\n        \"MANDT\",  # Client\n        \"MATNR\",  # Material Number\n        \"MEINS\",  # Base Unit of Measure\n        F.col(col_mara_global_material_number).alias(\"GLOBAL_MATERIAL_NUMBER\"),  # Global Material Number\n    ]\n\n    return sap_mara.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_material_valuation","title":"<code>prep_material_valuation(df)</code>","text":"<p>Prepare Material Valuation data for harmonization across systems.</p> <p>Input Data: SAP MBEW table (Material Valuation)</p> Transformation <p>\u25cb Filter out materials that are flagged for deletion (LVORM is null). \u25cb Filter for entries with BWTAR (Valuation Type) as null to exclude split valuation materials. \u25cb Deduplicate records:   \u25aa Rule take the record having highest evaluated price LAEPR (Last Evaluated Price) at MATNR and BWKEY level</p> <p>Keep the first record per group.     \u25cb Select the required columns.     \u25cb Drop Duplicates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Material Valuation data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Material Valuation data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def prep_material_valuation(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Material Valuation data for harmonization across systems.\n\n    Input Data: SAP MBEW table (Material Valuation)\n\n    Transformation:\n      \u25cb Filter out materials that are flagged for deletion (LVORM is null).\n      \u25cb Filter for entries with BWTAR (Valuation Type) as null to exclude split valuation materials.\n      \u25cb Deduplicate records:\n        \u25aa Rule take the record having highest evaluated price LAEPR (Last Evaluated Price) at MATNR and BWKEY level\n    Keep the first record per group.\n        \u25cb Select the required columns.\n        \u25cb Drop Duplicates.\n\n    Args:\n        df (DataFrame): DataFrame with Material Valuation data.\n\n    Returns:\n        DataFrame: DataFrame with prepared Material Valuation data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    logger.info(\"Preparing Material Valuation data\")\n    if None or not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    window_spec = Window.partitionBy(\"MATNR\", \"BWKEY\").orderBy(F.col(\"LAEPR\").desc())\n\n    # Start with the original DataFrame\n    sap_mbew = (\n        df.filter(F.col(\"LVORM\").isNull())  # Filter out materials that are flagged for deletion (LVORM is null).\n        .filter(\n            F.col(\"BWTAR\").isNull()\n        )  # Filter for entries with BWTAR (Valuation Type) as null to exclude split valuation materials.\n        .withColumn(\n            \"ROW_NUMBER\", F.row_number().over(window_spec)\n        )  # Add row number based on the descending order of LAEPR.\n        .filter(F.col(\"ROW_NUMBER\") == 1)  # Filter for the latest price based on the row number.\n        .drop(\"ROW_NUMBER\")  # Drop the row number column.\n    )\n\n    # Select and rename columns\n    columns = [\n        \"MANDT\",  # Client\n        \"MATNR\",  # Material Number\n        \"BWKEY\",  # Valuation Area\n        \"VPRSV\",  # Price Control Indicator\n        \"VERPR\",  # Moving Average Price\n        \"STPRS\",  # Standard Price\n        \"PEINH\",  # Price Unit\n        \"BKLAS\",  # Valuation Class\n    ]\n\n    return sap_mbew.select(*columns).drop_duplicates()\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_plant_and_branches","title":"<code>prep_plant_and_branches(df)</code>","text":"<p>Prepare Plant and Branches data for harmonization across systems.</p> <p>Input Data: SAP T001W table (Plant/Branch)</p> Transformation <p>\u25cb Select the required columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Plant and Branches data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Plant and Branches data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def prep_plant_and_branches(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Plant and Branches data for harmonization across systems.\n\n    Input Data: SAP T001W table (Plant/Branch)\n\n    Transformation:\n      \u25cb Select the required columns.\n\n    Args:\n        df (DataFrame): DataFrame with Plant and Branches data.\n\n    Returns:\n        DataFrame:  DataFrame with prepared Plant and Branches data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    logger.info(\"Preparing Plant and Branches data\")\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"MANDT\",  # Client\n        \"WERKS\",  # Plant\n        \"BWKEY\",  # Valuation Area\n        \"NAME1\",  # Name of Plant/Branch\n    ]\n\n    return df.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_plant_data_for_material","title":"<code>prep_plant_data_for_material(df, check_deletion_flag_is_null, drop_duplicate_records, additional_fields=None)</code>","text":"<p>Prepare Plant data for harmonization across systems.</p> <p>Input Data: SAP MARC table (Plant Data for Material)</p> Transformation <p>\u25cb Filter records where the deletion flag (LVORM) is null. \u25cb Select the required columns. \u25cb Drop Duplicates if drop_duplicate_records is True.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Plant data for Material.</p> required <code>check_deletion_flag_is_null</code> <code>bool</code> <p>Check if the deletion flag is null.</p> required <code>drop_duplicate_records</code> <code>bool</code> <p>Drop duplicate records</p> required <code>additional_fields</code> <code>list</code> <p>Additional fields to include in the output DataFrame.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Plant data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> <code>TypeError</code> <p>If the check for the deletion flag is not a boolean.</p> <code>TypeError</code> <p>If the drop duplicate records flag is not a boolean.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def prep_plant_data_for_material(\n    df: DataFrame,\n    check_deletion_flag_is_null: bool,\n    drop_duplicate_records: bool,\n    additional_fields: list | None = None,\n) -&gt; DataFrame:\n    \"\"\"Prepare Plant data for harmonization across systems.\n\n    Input Data: SAP MARC table (Plant Data for Material)\n\n    Transformation:\n      \u25cb Filter records where the deletion flag (LVORM) is null.\n      \u25cb Select the required columns.\n      \u25cb Drop Duplicates if drop_duplicate_records is True.\n\n    Args:\n        df (DataFrame): DataFrame with Plant data for Material.\n        check_deletion_flag_is_null (bool): Check if the deletion flag is null.\n        drop_duplicate_records (bool): Drop duplicate records\n        additional_fields (list): Additional fields to include in the output DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with prepared Plant data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n        TypeError: If the check for the deletion flag is not a boolean.\n        TypeError: If the drop duplicate records flag is not a boolean.\n    \"\"\"\n    logger.info(\"Preparing Plant data for Material\")\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n    if not isinstance(check_deletion_flag_is_null, bool):\n        error_message = \"check_deletion_flag_is_null must be a boolean\"\n        raise TypeError(error_message)\n    if not isinstance(drop_duplicate_records, bool):\n        error_message = \"drop_duplicate_records must be a boolean\"\n        raise TypeError(error_message)\n    if additional_fields and not isinstance(additional_fields, list):\n        error_message = \"additional_fields must be a list of columns\"\n        raise TypeError(error_message)\n\n    sap_marc = df\n\n    # Apply deletion flag filter before selecting columns\n    if check_deletion_flag_is_null:\n        sap_marc = sap_marc.filter(F.col(\"LVORM\").isNull())\n\n    # Select columns\n    columns = [\n        \"SOURCE_SYSTEM_ERP\",  # Source ERP system identifier\n        \"MATNR\",  # Material Number\n        \"WERKS\",  # Plant\n    ]\n\n    if additional_fields:\n        columns.extend([F.col(field) for field in additional_fields])\n\n    sap_marc = sap_marc.select(*columns)\n\n    if drop_duplicate_records:\n        sap_marc = sap_marc.drop_duplicates()\n\n    return sap_marc\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_valuation_data","title":"<code>prep_valuation_data(df)</code>","text":"<p>Prepare Valuation data for harmonization across systems.</p> <p>Input Data: SAP T001K table (Valuation Area)</p> Transformation <p>\u25cb Select the required columns. \u25cb Drop Duplicates to ensure uniqueness.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Valuation data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Valuation data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code/modules/local_material.py</code> <pre><code>def prep_valuation_data(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Valuation data for harmonization across systems.\n\n    Input Data: SAP T001K table (Valuation Area)\n\n    Transformation:\n      \u25cb Select the required columns.\n      \u25cb Drop Duplicates to ensure uniqueness.\n\n    Args:\n        df (DataFrame): DataFrame with Valuation data.\n\n    Returns:\n        DataFrame: DataFrame with prepared Valuation data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    logger.info(\"Preparing Valuation data\")\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"MANDT\",  # Client\n        \"BWKEY\",  # Valuation Area\n        \"BUKRS\",  # Company Code\n    ]\n\n    return df.select(*columns).drop_duplicates()\n</code></pre>"},{"location":"reference/code/modules/process_order/","title":"process_order","text":"<p>Module to prepare SAP order data.</p>"},{"location":"reference/code/modules/process_order/#code.modules.process_order.integration","title":"<code>integration(sap_afko, sap_afpo, sap_aufk, sap_mara, sap_cdpos=None)</code>","text":"<p>Integration of SAP tables.</p> <p>\u25cf DataFrames to Integrate:     \u25cb Order Header Data (sap_afko)     \u25cb Order Item Data (sap_afpo)     \u25cb Order Master Data (sap_aufk)     \u25cb General Material Data (sap_mara)     \u25cb Header and Item (sap_cdpos, optional)</p> Transformation <p>\u25cb Join operations:     \u25aa sap_afko left join sap_afpo on AUFNR.     \u25aa Result left join sap_aufk on AUFNR.     \u25aa Result left join sap_mara on MATNR.     \u25aa If sap_cdpos is provided, result left join sap_cdpos on OBJNR. \u25cb Handle Missing Values:     \u25aa Use ZZGLTRP_ORIG if available; otherwise, use GLTRP.</p> <p>Parameters:</p> Name Type Description Default <code>sap_afko</code> <code>DataFrame</code> <p>SAP AFKO Dataframe</p> required <code>sap_afpo</code> <code>DataFrame</code> <p>SAP AFPO Dataframe</p> required <code>sap_aufk</code> <code>DataFrame</code> <p>SAP AUFK Dataframe</p> required <code>sap_mara</code> <code>DataFrame</code> <p>SAP MARA Dataframe</p> required <code>sap_cdpos</code> <code>DataFrame</code> <p>SAP CDPOS Dataframe. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Integrated DataFrame</p> Source code in <code>code/modules/process_order.py</code> <pre><code>def integration(\n    sap_afko: DataFrame,\n    sap_afpo: DataFrame,\n    sap_aufk: DataFrame,\n    sap_mara: DataFrame,\n    sap_cdpos: DataFrame | None = None,\n) -&gt; DataFrame:\n    \"\"\"Integration of SAP tables.\n\n    \u25cf DataFrames to Integrate:\n        \u25cb Order Header Data (sap_afko)\n        \u25cb Order Item Data (sap_afpo)\n        \u25cb Order Master Data (sap_aufk)\n        \u25cb General Material Data (sap_mara)\n        \u25cb Header and Item (sap_cdpos, optional)\n\n    Transformation:\n        \u25cb Join operations:\n            \u25aa sap_afko left join sap_afpo on AUFNR.\n            \u25aa Result left join sap_aufk on AUFNR.\n            \u25aa Result left join sap_mara on MATNR.\n            \u25aa If sap_cdpos is provided, result left join sap_cdpos on OBJNR.\n        \u25cb Handle Missing Values:\n            \u25aa Use ZZGLTRP_ORIG if available; otherwise, use GLTRP.\n\n    Args:\n        sap_afko (DataFrame): SAP AFKO Dataframe\n        sap_afpo (DataFrame): SAP AFPO Dataframe\n        sap_aufk (DataFrame): SAP AUFK Dataframe\n        sap_mara (DataFrame): SAP MARA Dataframe\n        sap_cdpos (DataFrame, optional): SAP CDPOS Dataframe. Defaults to None.\n\n    Returns:\n        DataFrame: Integrated DataFrame\n    \"\"\"\n    # Validate input types\n    for arg, name in zip(\n        [sap_afko, sap_afpo, sap_aufk, sap_mara], [\"sap_afko\", \"sap_afpo\", \"sap_aufk\", \"sap_mara\"], strict=True\n    ):\n        if not isinstance(arg, DataFrame):\n            error_message = f\"Expected DataFrame for {name}, got {type(arg).__name__} instead.\"\n            raise TypeError(error_message)\n\n    # Optional parameter check\n    if sap_cdpos is not None and not isinstance(sap_cdpos, DataFrame):\n        error_message = f\"Expected DataFrame or None for sap_cdpos, got {type(sap_cdpos).__name__} instead.\"\n        raise TypeError(error_message)\n\n    result = (\n        sap_afko.join(sap_afpo, on=\"AUFNR\", how=\"left\")\n        .join(sap_aufk, on=\"AUFNR\", how=\"left\")\n        .join(sap_mara, on=\"MATNR\", how=\"left\")\n    )\n    if sap_cdpos:\n        result = result.join(sap_cdpos, on=\"OBJNR\", how=\"left\")\n\n    return result.withColumn(\"ZZGLTRP_ORIG\", F.coalesce(F.col(\"ZZGLTRP_ORIG\"), F.col(\"GLTRP\")))\n</code></pre>"},{"location":"reference/code/modules/process_order/#code.modules.process_order.post_prep_process_order","title":"<code>post_prep_process_order(df)</code>","text":"<p>Post-process the Process Order Data.</p> <p>Input Data: Integrated DataFrame from the joined SAP tables.</p> Transformation <p>\u25cb Derive the following keys (intra represents the primary key of   one system and inter the primary key the harmonized view.     \u25aa Intra Primary Key (primary_key_intra): Concatenate AUFNR, POSNR, and DWERK.     \u25aa Inter Primary Key (primary_key_inter): Concatenate SOURCE_SYSTEM_ERP, AUFNR, POSNR, and DWERK. \u25cb Calculate On-Time Flag:     \u25aa Set on_time_flag to:         \u25cf 1 if ZZGLTRP_ORIG &gt;= LTRMI.         \u25cf 0 if ZZGLTRP_ORIG &lt; LTRMI.         \u25cf null if dates are missing. \u25cb Calculate On-Time Deviation and Late Delivery Bucket:     \u25aa Compute actual_on_time_deviation as ZZGLTRP_ORIG - LTRMI.     \u25aa Categorize late_delivery_bucket based on deviation days. \u25cb Ensure ZZGLTRP_ORIG is Present:     \u25aa Add ZZGLTRP_ORIG with null values if it's not in the DataFrame. \u25cb Derive MTO vs. MTS Flag:     \u25aa Set mto_vs_mts_flag to:         \u25cf \"MTO\" if KDAUF (Sales Order Number) is not null.         \u25cf \"MTS\" otherwise. \u25cb Convert Dates to Timestamps:     \u25aa Create order_finish_timestamp from LTRMI.     \u25aa Create order_start_timestamp from GSTRI.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Resulting DataFrame from the integration step.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Post-processed DataFrame with additional calculated fields.</p> Source code in <code>code/modules/process_order.py</code> <pre><code>def post_prep_process_order(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Post-process the Process Order Data.\n\n    Input Data: Integrated DataFrame from the joined SAP tables.\n\n    Transformation:\n        \u25cb Derive the following keys (intra represents the primary key of\n          one system and inter the primary key the harmonized\n        view.\n            \u25aa Intra Primary Key (primary_key_intra): Concatenate AUFNR, POSNR, and DWERK.\n            \u25aa Inter Primary Key (primary_key_inter): Concatenate SOURCE_SYSTEM_ERP, AUFNR, POSNR, and DWERK.\n        \u25cb Calculate On-Time Flag:\n            \u25aa Set on_time_flag to:\n                \u25cf 1 if ZZGLTRP_ORIG &gt;= LTRMI.\n                \u25cf 0 if ZZGLTRP_ORIG &lt; LTRMI.\n                \u25cf null if dates are missing.\n        \u25cb Calculate On-Time Deviation and Late Delivery Bucket:\n            \u25aa Compute actual_on_time_deviation as ZZGLTRP_ORIG - LTRMI.\n            \u25aa Categorize late_delivery_bucket based on deviation days.\n        \u25cb Ensure ZZGLTRP_ORIG is Present:\n            \u25aa Add ZZGLTRP_ORIG with null values if it's not in the DataFrame.\n        \u25cb Derive MTO vs. MTS Flag:\n            \u25aa Set mto_vs_mts_flag to:\n                \u25cf \"MTO\" if KDAUF (Sales Order Number) is not null.\n                \u25cf \"MTS\" otherwise.\n        \u25cb Convert Dates to Timestamps:\n            \u25aa Create order_finish_timestamp from LTRMI.\n            \u25aa Create order_start_timestamp from GSTRI.\n\n    Args:\n        df (DataFrame): Resulting DataFrame from the integration step.\n\n    Returns:\n        DataFrame: Post-processed DataFrame with additional calculated fields.\n    \"\"\"\n    # Ensure ZZGLTRP_ORIG is present\n    if \"ZZGLTRP_ORIG\" not in df.columns:\n        post_processed_data = df.withColumn(\"ZZGLTRP_ORIG\", F.lit(None).cast(\"date\"))\n\n    # Cast date fields to DateType\n    post_processed_data = (\n        df.withColumn(\"ZZGLTRP_ORIG\", F.col(\"ZZGLTRP_ORIG\").cast(\"date\"))\n        .withColumn(\"LTRMI\", F.col(\"LTRMI\").cast(\"date\"))\n        .withColumn(\"GSTRI\", F.col(\"GSTRI\").cast(\"date\"))\n    )\n\n    return (\n        post_processed_data\n        # Derive Primary Keys\n        .withColumn(\"primary_key_intra\", F.concat_ws(\"_\", F.col(\"AUFNR\"), F.col(\"POSNR\"), F.col(\"DWERK\")))\n        .withColumn(\n            \"primary_key_inter\",\n            F.concat_ws(\"_\", F.col(\"SOURCE_SYSTEM_ERP\"), F.col(\"AUFNR\"), F.col(\"POSNR\"), F.col(\"DWERK\")),\n        )\n        # Calculate On-Time Flag\n        .withColumn(\n            \"on_time_flag\",\n            F.when(\n                F.col(\"ZZGLTRP_ORIG\").isNotNull()\n                &amp; F.col(\"LTRMI\").isNotNull()\n                &amp; (F.col(\"ZZGLTRP_ORIG\") &gt;= F.col(\"LTRMI\")),\n                F.lit(1),\n            )\n            .when(\n                F.col(\"ZZGLTRP_ORIG\").isNotNull()\n                &amp; F.col(\"LTRMI\").isNotNull()\n                &amp; (F.col(\"ZZGLTRP_ORIG\") &lt; F.col(\"LTRMI\")),\n                F.lit(0),\n            )\n            .otherwise(F.lit(None)),\n        )\n        # Calculate On-Time Deviation\n        .withColumn(\n            \"actual_on_time_deviation\",\n            F.when(\n                F.col(\"ZZGLTRP_ORIG\").isNotNull() &amp; F.col(\"LTRMI\").isNotNull(),\n                F.datediff(F.col(\"ZZGLTRP_ORIG\"), F.col(\"LTRMI\")),\n            )\n            .otherwise(F.lit(None))\n            .cast(DoubleType()),\n        )\n        # Categorize Late Delivery Bucket\n        .withColumn(\n            \"late_delivery_bucket\",\n            F.when(F.col(\"actual_on_time_deviation\") &lt;= 0, F.lit(\"On Time\"))\n            .when(F.col(\"actual_on_time_deviation\").between(1, 7), F.lit(\"1-7 Days Late\"))\n            .when(F.col(\"actual_on_time_deviation\").between(8, 14), F.lit(\"8-14 Days Late\"))\n            .when(F.col(\"actual_on_time_deviation\") &gt; 14, F.lit(\"&gt;14 Days Late\"))\n            .otherwise(F.lit(\"Unknown\")),\n        )\n        # Derive MTO vs. MTS Flag\n        .withColumn(\n            \"mto_vs_mts_flag\",\n            F.when(F.col(\"KDAUF\").isNotNull(), F.lit(\"MTO\")).otherwise(F.lit(\"MTS\")),\n        )\n        # Convert Dates to Timestamps\n        .withColumn(\"order_finish_timestamp\", F.to_timestamp(F.col(\"LTRMI\")))\n        .withColumn(\"order_start_timestamp\", F.to_timestamp(F.col(\"GSTRI\")))\n    )\n</code></pre>"},{"location":"reference/code/modules/process_order/#code.modules.process_order.prep_sap_general_material_data","title":"<code>prep_sap_general_material_data(df, col_global_material)</code>","text":"<p>Prepare the SAP general material data.</p> <p>Input Data: SAP MARA table (General Material Data)</p> Transformation <p>\u25cb Filter materials:     \u25aa Old Material Number (BISMT) is not in [\"ARCHIVE\", \"DUPLICATE\", \"RENUMBERED\"] or is null.     \u25aa Deletion flag (LVORM) is null or empty. \u25cb Select the required columns. \u25cb Rename the global material number column to a consistent name, if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>col_global_material</code> <code>str</code> <p>Column name for Global Material</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input DataFrame is invalid</p> <code>TypeError</code> <p>If column name is invalid</p> Source code in <code>code/modules/process_order.py</code> <pre><code>def prep_sap_general_material_data(df: DataFrame, col_global_material: str) -&gt; DataFrame:\n    \"\"\"Prepare the SAP general material data.\n\n    Input Data: SAP MARA table (General Material Data)\n\n    Transformation:\n        \u25cb Filter materials:\n            \u25aa Old Material Number (BISMT) is not in [\"ARCHIVE\", \"DUPLICATE\", \"RENUMBERED\"] or is null.\n            \u25aa Deletion flag (LVORM) is null or empty.\n        \u25cb Select the required columns.\n        \u25cb Rename the global material number column to a consistent name, if necessary.\n\n    Args:\n        df (DataFrame): Input DataFrame\n        col_global_material (str): Column name for Global Material\n\n    Returns:\n        DataFrame: Processed DataFrame\n\n    Raises:\n        TypeError: If input DataFrame is invalid\n        TypeError: If column name is invalid\n    \"\"\"\n    if df is None or isinstance(df, DataFrame) is False:\n        error_message = \"Invalid input DataFrame\"\n        raise TypeError(error_message)\n    if col_global_material is None or not isinstance(col_global_material, str):\n        error_message = \"Invalid input column name\"\n        raise TypeError(error_message)\n\n    df_filtered = df.filter(\n        (F.col(\"BISMT\").isNull() | ~F.col(\"BISMT\").isin([\"ARCHIVE\", \"DUPLICATE\", \"RENUMBERED\"]))\n        &amp; (F.col(\"LVORM\").isNull() | (F.trim(F.col(\"LVORM\")).cast(\"string\") == F.lit(\"\")))\n    )\n\n    columns = [\n        \"MATNR\",  # Material Number\n        F.col(col_global_material).alias(\"GLOBAL_MATERIAL_NUMBER\"),  # Global Material Number\n        \"NTGEW\",  # Net Weight\n        \"MTART\",  # Material Type\n    ]\n\n    return df_filtered.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/process_order/#code.modules.process_order.prep_sap_order_header_data","title":"<code>prep_sap_order_header_data(df)</code>","text":"<p>Prepare the SAP order header data.</p> <p>Input Data: SAP AFKO table (Order Header Data)</p> Transformation <p>\u25cb Select the required columns. \u25cb Createstart_date and finish_date:     \u25aa Format GSTRP as 'yyyy-MM'.     \u25aa If GSTRP is null, use the current date.     \u25aa Concatenate'-01' to form full dates.     \u25aa Convert to date format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input DataFrame is invalid</p> Source code in <code>code/modules/process_order.py</code> <pre><code>def prep_sap_order_header_data(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare the SAP order header data.\n\n    Input Data: SAP AFKO table (Order Header Data)\n\n    Transformation:\n        \u25cb Select the required columns.\n        \u25cb Createstart_date and finish_date:\n            \u25aa Format GSTRP as 'yyyy-MM'.\n            \u25aa If GSTRP is null, use the current date.\n            \u25aa Concatenate'-01' to form full dates.\n            \u25aa Convert to date format.\n\n    Args:\n        df (DataFrame): Input DataFrame\n\n    Returns:\n        DataFrame: Processed DataFrame\n\n    Raises:\n        TypeError: If input DataFrame is invalid\n    \"\"\"\n    if df is None or isinstance(df, DataFrame) is False:\n        error_message = \"Invalid input DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"SOURCE_SYSTEM_ERP\",  # Source ERP system identifier\n        \"MANDT\",  # Client\n        \"AUFNR\",  # Order number\n        F.col(\"GLTRP\").cast(DateType()),  # Basic finish date\n        F.col(\"GSTRP\").cast(DateType()),  # Basic start date\n        F.col(\"FTRMS\").cast(DateType()),  # Scheduled finish date\n        F.col(\"GLTRS\").cast(DateType()),  # Actual finish date\n        F.col(\"GSTRS\").cast(DateType()),  # Actual start date\n        F.col(\"GSTRI\").cast(DateType()),  # Basic finish date\n        F.col(\"GETRI\").cast(DateType()),  # Basic start date\n        F.col(\"GLTRI\").cast(DateType()),  # Scheduled finish date\n        F.col(\"FTRMI\").cast(DateType()),  # Actual finish date\n        F.col(\"FTRMP\").cast(DateType()),  # Actual start date\n        \"DISPO\",  # MRP controller\n        \"FEVOR\",  # Production Supervisor\n        \"PLGRP\",  # Planner Group\n        \"FHORI\",  # Scheduling Margin Key for Floats\n        \"AUFPL\",  # Routing number of operations in the order\n    ]\n\n    return (\n        df.select(*columns)\n        .withColumn(\n            \"start_date\",\n            F.when(F.col(\"GSTRP\").isNull(), F.current_date()).otherwise(F.col(\"GSTRP\")),\n        )\n        .withColumn(\n            \"finish_date\",\n            F.when(F.col(\"GLTRP\").isNull(), F.current_date()).otherwise(F.col(\"GLTRP\")),\n        )\n        .withColumn(\n            \"start_date\",\n            F.date_format(\n                F.concat_ws(\"-\", F.year(F.col(\"start_date\")), F.month(F.col(\"start_date\")), F.lit(\"01\")), \"yyyy-MM-dd\"\n            ).cast(DateType()),\n        )\n        .withColumn(\n            \"finish_date\",\n            F.date_format(\n                F.concat_ws(\"-\", F.year(F.col(\"finish_date\")), F.month(F.col(\"finish_date\")), F.lit(\"01\")), \"yyyy-MM-dd\"\n            ).cast(DateType()),\n        )\n    )\n</code></pre>"},{"location":"reference/code/modules/process_order/#code.modules.process_order.prep_sap_order_item","title":"<code>prep_sap_order_item(df)</code>","text":"<p>Prepare the SAP order item data.</p> <p>Input Data: SAP AFPO table (Order Item Data) Transformation:     \u25cb Select the required columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input DataFrame is invalid</p> Source code in <code>code/modules/process_order.py</code> <pre><code>def prep_sap_order_item(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare the SAP order item data.\n\n    Input Data: SAP AFPO table (Order Item Data)\n    Transformation:\n        \u25cb Select the required columns.\n\n    Args:\n        df (DataFrame): Input DataFrame\n\n    Returns:\n        DataFrame: Processed DataFrame\n\n    Raises:\n        TypeError: If input DataFrame is invalid\n    \"\"\"\n    if df is None or isinstance(df, DataFrame) is False:\n        error_message = \"Invalid input DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"AUFNR\",  # Order number\n        \"POSNR\",  # Order Item number\n        \"DWERK\",  # Plant\n        \"MATNR\",  # Material number\n        \"MEINS\",  # Base unit of measure\n        \"KDAUF\",  # Sales order number\n        \"KDPOS\",  # Sales order item\n        F.col(\"LTRMI\").cast(DateType()),  # Actual delivery/finish date\n    ]\n\n    return df.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/process_order/#code.modules.process_order.prep_sap_order_master_data","title":"<code>prep_sap_order_master_data(df)</code>","text":"<p>Prepare the SAP order master data.</p> <p>Input Data: SAP AUFK table (Order Master Data)</p> Transformation <p>\u25cb Select the required columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Processed DataFrame</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input DataFrame is invalid</p> Source code in <code>code/modules/process_order.py</code> <pre><code>def prep_sap_order_master_data(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare the SAP order master data.\n\n    Input Data: SAP AUFK table (Order Master Data)\n\n    Transformation:\n        \u25cb Select the required columns.\n\n    Args:\n        df (DataFrame): Input DataFrame\n\n    Returns:\n        DataFrame: Processed DataFrame\n\n    Raises:\n        TypeError: If input DataFrame is invalid\n    \"\"\"\n    if df is None or isinstance(df, DataFrame) is False:\n        error_message = \"Invalid input DataFrame\"\n        raise TypeError(error_message)\n\n    # Selected columns\n    columns = [\n        \"AUFNR\",  # Order Number\n        \"OBJNR\",  # Object Number\n        F.col(\"ERDAT\").cast(DateType()),  # Creation Date\n        \"ERNAM\",  # Created By\n        \"AUART\",  # Order type\n        F.col(\"ZZGLTRP_ORIG\").cast(DateType()),  # Original Basic Finish Date,\n        \"ZZPRO_TEXT\",  # Project Text\n    ]\n\n    return df.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/utils/","title":"utils","text":"<p>Utility functions for the Spark application.</p>"},{"location":"reference/code/modules/utils/#code.modules.utils.check_columns_unique","title":"<code>check_columns_unique(df, columns)</code>","text":"<p>Checks if each column in the given list is unique in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>columns</code> <code>list</code> <p>List of column names to check for uniqueness.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any column contains duplicate values.</p> Source code in <code>code/modules/utils.py</code> <pre><code>def check_columns_unique(df: DataFrame, columns: list):\n    \"\"\"Checks if each column in the given list is unique in the DataFrame.\n\n    Args:\n        df (DataFrame): The input Spark DataFrame.\n        columns (list): List of column names to check for uniqueness.\n\n    Raises:\n        ValueError: If any column contains duplicate values.\n    \"\"\"\n    for column in columns:\n        if column not in df.columns:\n            error_message = f\"Column '{column}' is not present in the DataFrame.\"\n            raise ValueError(error_message)\n\n        # Count duplicate values for the column\n        duplicate_count = df.groupBy(column).count().filter(F.col(\"count\") &gt; 1).count()\n\n        if duplicate_count &gt; 0:\n            error_message = f\"Column '{column}' is not unique. Found {duplicate_count} duplicate values.\"\n            raise ValueError(error_message)\n\n    logger.info(f\"All specified columns {columns} are unique.\")\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.create_spark_session","title":"<code>create_spark_session(app_name)</code>","text":"<p>Create a Spark session.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the Spark application.</p> required <p>Returns:</p> Name Type Description <code>SparkSession</code> <code>SparkSession</code> <p>Spark session object.</p> Source code in <code>code/modules/utils.py</code> <pre><code>def create_spark_session(app_name: str) -&gt; SparkSession:\n    \"\"\"Create a Spark session.\n\n    Args:\n        app_name (str): Name of the Spark application.\n\n    Returns:\n        SparkSession: Spark session object.\n    \"\"\"\n    os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n\n    return SparkSession.builder.appName(app_name).config(\"spark.sql.repl.eagerEval.enablede\", True).getOrCreate()\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Create a logger object with both console and file handlers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: Logger object.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If unable to create logs directory or log file.</p> Source code in <code>code/modules/utils.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"Create a logger object with both console and file handlers.\n\n    Args:\n        name (str): Name of the logger.\n\n    Returns:\n        logging.Logger: Logger object.\n\n    Raises:\n        OSError: If unable to create logs directory or log file.\n    \"\"\"\n    try:\n        # Create logs directory if it doesn't exist\n        log_dir = Path(\"logs\")\n        log_dir.mkdir(exist_ok=True)\n\n        # Create logger\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.INFO)\n\n        # Clear existing handlers to avoid duplicates\n        if logger.handlers:\n            logger.handlers.clear()\n\n        # Create formatters\n        formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s [%(name)s.%(funcName)s:%(lineno)d] %(message)s\")\n\n        # File handler\n        file_handler = logging.FileHandler(filename=log_dir / \"spark.log\", encoding=\"utf-8\", mode=\"a\")\n        file_handler.setLevel(logging.INFO)\n        file_handler.setFormatter(formatter)\n\n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n        console_handler.setFormatter(formatter)\n\n        # Add handlers to logger\n        logger.addHandler(file_handler)\n        logger.addHandler(console_handler)\n\n    except OSError as e:\n        msg = f\"Failed to setup logger: {e!s}\"\n        raise OSError(msg) from e\n    else:\n        return logger\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.mask_sensitive_columns","title":"<code>mask_sensitive_columns(df, sensitive_columns)</code>","text":"<p>Masks sensitive columns in the given Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Spark DataFrame.</p> required <code>sensitive_columns</code> <code>list</code> <p>List of column names to mask.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Spark DataFrame with masked sensitive columns.</p> Source code in <code>code/modules/utils.py</code> <pre><code>def mask_sensitive_columns(df: DataFrame, sensitive_columns: list) -&gt; DataFrame:\n    \"\"\"Masks sensitive columns in the given Spark DataFrame.\n\n    Args:\n        df (DataFrame): Input Spark DataFrame.\n        sensitive_columns (list): List of column names to mask.\n\n    Returns:\n        DataFrame: Spark DataFrame with masked sensitive columns.\n    \"\"\"\n    logger.info(f\"Masking sensitive columns: {sensitive_columns}\")\n\n    # Start with the original DataFrame\n    df_masked = df\n\n    # Iterate through the sensitive columns and apply sha256 hashing\n    for col_name in sensitive_columns:\n        if col_name in df.columns:\n            # Apply hashing but only for non-null values\n            df_masked = df_masked.withColumn(\n                col_name,\n                F.when(F.col(col_name).isNotNull(), F.sha2(F.col(col_name).cast(\"string\"), 256)).otherwise(\n                    F.col(col_name)\n                ),\n            )\n        else:\n            logger.error(f\"Column '{col_name}' not found in DataFrame.\")\n            error_message = f\"Column '{col_name}' not found in DataFrame.\"\n            raise ValueError(error_message)\n\n    return df_masked\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.profile_data","title":"<code>profile_data(df)</code>","text":"<p>Profile the data in a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame object.</p> required Source code in <code>code/modules/utils.py</code> <pre><code>def profile_data(df: DataFrame) -&gt; None:\n    \"\"\"Profile the data in a Spark DataFrame.\n\n    Args:\n        df (DataFrame): Spark DataFrame object.\n    \"\"\"\n    try:\n        logger.info(\"Data profiling results:\")\n        df.printSchema()\n\n        df.describe().show()\n        df.show(5)\n\n    except Exception as e:\n        logger.exception(\"An error occurred while profiling the data.\", extra=e)\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.read_csv_file","title":"<code>read_csv_file(spark, file_directory, infer_schema=True, schema=None)</code>","text":"<p>Read a CSV file into a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>file_directory</code> <code>str</code> <p>Path to the CSV file.</p> required <code>infer_schema</code> <code>bool</code> <p>Whether to infer the schema of the CSV file.</p> <code>True</code> <code>schema</code> <code>str</code> <p>Schema of the CSV file.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Spark DataFrame object.</p> Source code in <code>code/modules/utils.py</code> <pre><code>def read_csv_file(\n    spark: SparkSession,\n    file_directory: str,\n    infer_schema: bool = True,\n    schema: str | None = None,\n) -&gt; DataFrame:\n    \"\"\"Read a CSV file into a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        file_directory (str): Path to the CSV file.\n        infer_schema (bool): Whether to infer the schema of the CSV file.\n        schema (str): Schema of the CSV file.\n\n    Returns:\n        DataFrame: Spark DataFrame object.\n    \"\"\"\n    encoding = \"utf-8\"\n\n    logger.info(f\"Reading CSV file: {file_directory}\")\n    try:\n        if infer_schema:\n            csv_df = spark.read.csv(f\"{file_directory}\", header=True, inferSchema=True, encoding=encoding)\n        else:\n            csv_df = spark.read.csv(f\"{file_directory}\", header=True, schema=schema, encoding=encoding)\n\n    except Exception as e:\n        logger.exception(\"An error occurred while reading the CSV file.\", extra=e)\n        return None\n    else:\n        return csv_df\n</code></pre>"},{"location":"reference/code/tests/","title":"tests","text":""},{"location":"reference/code/tests/conftest/","title":"conftest","text":"<p>This module contains the fixtures for the tests.</p>"},{"location":"reference/code/tests/conftest/#code.tests.conftest.spark","title":"<code>spark()</code>","text":"<p>Fixture for creating a Spark session.</p> <p>Yields:</p> Name Type Description <code>SparkSession</code> <code>Any</code> <p>Spark session object.</p> Source code in <code>code/tests/conftest.py</code> <pre><code>@pytest.fixture(scope=\"session\")\ndef spark() -&gt; Generator[Any, Any, Any]:\n    \"\"\"Fixture for creating a Spark session.\n\n    Yields:\n        SparkSession: Spark session object.\n    \"\"\"\n    findspark.init()\n    spark = SparkSession.builder.appName(\"pytest\").getOrCreate()\n\n    yield spark\n    spark.stop()\n</code></pre>"},{"location":"reference/code/tests/test_utils/","title":"test_utils","text":"<p>Tests for utils module.</p>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.duplicate_data","title":"<code>duplicate_data(spark)</code>","text":"<p>Fixture to provide data with duplicates. Args:     spark (SparkSession): Spark session object. Returns:     DataFrame: Spark DataFrame object.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>@pytest.fixture\ndef duplicate_data(spark: SparkSession) -&gt; DataFrame:\n    \"\"\"Fixture to provide data with duplicates.\n    Args:\n        spark (SparkSession): Spark session object.\n    Returns:\n        DataFrame: Spark DataFrame object.\n    \"\"\"\n    data = [\n        {\"MATNR\": \"MAT1\", \"AUFNR\": \"ORD1\", \"SOURCE_SYSTEM_ERP\": \"SYS1\"},\n        {\"MATNR\": \"MAT1\", \"AUFNR\": \"ORD2\", \"SOURCE_SYSTEM_ERP\": \"SYS1\"},  # Duplicate MATNR\n        {\"MATNR\": \"MAT3\", \"AUFNR\": \"ORD3\", \"SOURCE_SYSTEM_ERP\": \"SYS2\"},\n    ]\n    return spark.createDataFrame(data)\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.sample_data","title":"<code>sample_data(spark)</code>","text":"<p>Fixture to provide sample data for testing. Args:     spark (SparkSession): Spark session object.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Spark DataFrame object.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>@pytest.fixture\ndef sample_data(spark: SparkSession) -&gt; DataFrame:\n    \"\"\"Fixture to provide sample data for testing.\n    Args:\n        spark (SparkSession): Spark session object.\n\n    Returns:\n        DataFrame: Spark DataFrame object.\n    \"\"\"\n    data = [\n        {\"name\": \"Alice\", \"email\": \"alice@example.com\", \"phone\": \"1234567890\"},\n        {\"name\": \"Bob\", \"email\": \"bob@example.com\", \"phone\": \"0987654321\"},\n        {\"name\": \"Charlie\", \"email\": None, \"phone\": None},\n    ]\n    schema = [\"name\", \"email\", \"phone\"]\n    return spark.createDataFrame(data, schema=schema)\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_column_not_unique","title":"<code>test_column_not_unique(duplicate_data)</code>","text":"<p>Test that a ValueError is raised for non-unique columns.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_column_not_unique(duplicate_data):\n    \"\"\"Test that a ValueError is raised for non-unique columns.\"\"\"\n    with pytest.raises(ValueError, match=\"Column 'MATNR' is not unique.\"):\n        check_columns_unique(duplicate_data, [\"MATNR\"])\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_columns_unique_all_unique","title":"<code>test_columns_unique_all_unique(sample_data)</code>","text":"<p>Test that no error is raised when all columns are unique.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_columns_unique_all_unique(sample_data):\n    \"\"\"Test that no error is raised when all columns are unique.\"\"\"\n    try:\n        check_columns_unique(sample_data, [\"name\", \"email\", \"phone\"])\n    except ValueError as e:\n        pytest.fail(f\"Unexpected ValueError raised: {e}\")\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_create_spark_session","title":"<code>test_create_spark_session()</code>","text":"<p>Test create_spark_session function.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_create_spark_session():\n    \"\"\"Test create_spark_session function.\"\"\"\n    spark = create_spark_session(\"test\")\n\n    assert spark is not None, \"Spark session object is None\"\n    assert spark.conf.get(\"spark.app.name\") == \"test\", \"Spark application name is incorrect\"\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_empty_column_list","title":"<code>test_empty_column_list(sample_data)</code>","text":"<p>Test behavior when the column list is empty.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_empty_column_list(sample_data):\n    \"\"\"Test behavior when the column list is empty.\"\"\"\n    try:\n        check_columns_unique(sample_data, [])\n    except ValueError as e:\n        pytest.fail(f\"Unexpected ValueError raised for empty column list: {e}\")\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_mask_sensitive_columns","title":"<code>test_mask_sensitive_columns(spark, sample_data)</code>","text":"<p>Test masking of sensitive columns.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_mask_sensitive_columns(spark, sample_data):\n    \"\"\"Test masking of sensitive columns.\"\"\"\n    sensitive_columns = [\"email\", \"phone\"]\n    masked_df = mask_sensitive_columns(sample_data, sensitive_columns)\n\n    # Check that sensitive columns are masked\n    assert masked_df.select(\"email\").collect()[0][0] != \"alice@example.com\", \"Email should be masked\"\n    assert masked_df.select(\"phone\").collect()[0][0] != \"1234567890\", \"Phone should not be masked\"\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_mask_sensitive_columns_missing_column","title":"<code>test_mask_sensitive_columns_missing_column(sample_data)</code>","text":"<p>Test that a ValueError is raised if a sensitive column is not present in the DataFrame.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_mask_sensitive_columns_missing_column(sample_data):\n    \"\"\"Test that a ValueError is raised if a sensitive column is not present in the DataFrame.\"\"\"\n    sensitive_columns = [\"non_existent_column\"]\n    with pytest.raises(ValueError, match=\"Column 'non_existent_column' not found in DataFrame.\"):\n        mask_sensitive_columns(sample_data, sensitive_columns)\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_mask_sensitive_columns_with_null_values","title":"<code>test_mask_sensitive_columns_with_null_values(sample_data)</code>","text":"<p>Test that null values in sensitive columns are not masked.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_mask_sensitive_columns_with_null_values(sample_data):\n    \"\"\"Test that null values in sensitive columns are not masked.\"\"\"\n    sensitive_columns = [\"email\", \"phone\"]\n    masked_df = mask_sensitive_columns(sample_data, sensitive_columns)\n\n    # Check that null values are not masked\n    for col in sensitive_columns:\n        null_count = sample_data.filter(F.col(col).isNull()).count()\n        masked_null_count = masked_df.filter(F.col(col).isNull()).count()\n        assert null_count == masked_null_count, f\"Null values in column '{col}' should not be masked\"\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_missing_column","title":"<code>test_missing_column(sample_data)</code>","text":"<p>Test that a ValueError is raised for missing columns.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_missing_column(sample_data):\n    \"\"\"Test that a ValueError is raised for missing columns.\"\"\"\n    with pytest.raises(ValueError, match=\"Column 'NON_EXISTENT' is not present in the DataFrame.\"):\n        check_columns_unique(sample_data, [\"NON_EXISTENT\"])\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_multiple_columns_mixed","title":"<code>test_multiple_columns_mixed(duplicate_data)</code>","text":"<p>Test a combination of valid and invalid column uniqueness.</p> Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_multiple_columns_mixed(duplicate_data):\n    \"\"\"Test a combination of valid and invalid column uniqueness.\"\"\"\n    with pytest.raises(ValueError, match=\"Column 'MATNR' is not unique.\"):\n        check_columns_unique(duplicate_data, [\"MATNR\", \"AUFNR\"])\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_read_csv_file_with_inferred_schema","title":"<code>test_read_csv_file_with_inferred_schema(spark)</code>","text":"<p>Test read_csv_file function.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_read_csv_file_with_inferred_schema(spark: SparkSession):\n    \"\"\"Test read_csv_file function.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    # Test reading a CSV file with inferred schema\n\n    # CSV file content\n    # name,age\n    # Alice,25\n    # Bob,30\n    # Charlie,35\n\n    file_directory = \"data/test/test_data.csv\"\n    df = read_csv_file(spark, file_directory, infer_schema=True)\n\n    assert df is not None, \"DataFrame is None\"\n    assert df.count() == 3, \"DataFrame count is incorrect\"\n    assert df.columns == [\"name\", \"age\"], \"DataFrame columns are incorrect\"\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_read_csv_file_with_specified_schema","title":"<code>test_read_csv_file_with_specified_schema(spark)</code>","text":"<p>Test read_csv_file function with specified schema.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_utils.py</code> <pre><code>def test_read_csv_file_with_specified_schema(spark: SparkSession):\n    \"\"\"Test read_csv_file function with specified schema.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    # Test reading a CSV file with specified schema\n\n    # CSV file content\n    # name,age\n    # Alice,25\n    # Bob,30\n    # Charlie,35\n\n    file_directory = \"data/test/test_data.csv\"\n\n    # Define schema\n    schema = StructType(\n        [\n            StructField(\"name\", StringType(), True),\n            StructField(\"age\", IntegerType(), True),\n        ]\n    )\n\n    # Read CSV file with specified schema\n    df = read_csv_file(spark, file_directory, infer_schema=False, schema=schema)\n\n    # Assert results\n    assert df is not None\n    assert df.count() == 3\n    assert df.columns == [\"name\", \"age\"]\n    assertSchemaEqual(df.schema, schema)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/","title":"test_local_material","text":""},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/","title":"test_derive_intra_and_inter_primary_key","text":""},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys","title":"<code>test_derive_primary_keys(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function.</p> Source code in <code>code/tests/test_local_material/test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function.\"\"\"\n    # Setup schema\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    # Test case 1: Normal case\n    data = [(\"SYS1\", \"MAT1\", \"PLANT1\"), (\"SYS2\", \"MAT2\", \"PLANT2\")]\n    input_df = spark.createDataFrame(data, schema)\n    result = derive_intra_and_inter_primary_key(input_df)\n\n    # Verify results\n    assert \"primary_key_intra\" in result.columns, \"Result should contain primary_key_intra column\"\n    assert \"primary_key_inter\" in result.columns, \"Result should contain primary_key_inter column\"\n\n    first_row = result.first()\n    assert first_row[\"primary_key_intra\"] == \"MAT1-PLANT1\", \"Primary key should be derived correctly\"\n    assert first_row[\"primary_key_inter\"] == \"SYS1-MAT1-PLANT1\", \"Primary key should be derived correctly\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys_empty_df","title":"<code>test_derive_primary_keys_empty_df(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function with an empty DataFrame.</p> Source code in <code>code/tests/test_local_material/test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys_empty_df(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function with an empty DataFrame.\"\"\"\n    # Test empty DataFrame\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n    empty_df = spark.createDataFrame([], schema)\n    result = derive_intra_and_inter_primary_key(empty_df)\n    assert result.count() == 0, \"Result DataFrame should be empty\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys_missing_columns","title":"<code>test_derive_primary_keys_missing_columns(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function with missing required columns.</p> Source code in <code>code/tests/test_local_material/test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys_missing_columns(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function with missing required columns.\"\"\"\n    # Test missing required columns\n    invalid_schema = StructType(\n        [StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True), StructField(\"MATNR\", StringType(), True)]\n    )\n    invalid_df = spark.createDataFrame([(\"SYS1\", \"MAT1\")], invalid_schema)\n\n    with pytest.raises(Exception):\n        derive_intra_and_inter_primary_key(invalid_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys_with_nulls","title":"<code>test_derive_primary_keys_with_nulls(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function with null values.</p> Source code in <code>code/tests/test_local_material/test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys_with_nulls(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function with null values.\"\"\"\n    # Setup schema\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    # Test with various null combinations\n    data = [(\"SYS1\", None, \"PLANT1\"), (\"SYS2\", \"MAT2\", None), (None, \"MAT3\", \"PLANT3\")]\n    input_df = spark.createDataFrame(data, schema)\n    result = derive_intra_and_inter_primary_key(input_df)\n\n    # Check results row by row\n    rows = result.collect()\n\n    # First row: null MATNR\n    assert rows[0][\"primary_key_intra\"] == \"-PLANT1\", \"Primary key should be derived correctly\"\n    assert rows[0][\"primary_key_inter\"] == \"SYS1--PLANT1\", \"Primary key should be derived correctly\"\n\n    # Second row: null WERKS\n    assert rows[1][\"primary_key_intra\"] == \"MAT2-\", \"Primary key should be derived correctly\"\n    assert rows[1][\"primary_key_inter\"] == \"SYS2-MAT2-\", \"Primary key should be derived correctly\"\n\n    # Third row: null SOURCE_SYSTEM_ERP\n    assert rows[2][\"primary_key_intra\"] == \"MAT3-PLANT3\", \"Primary key should be derived correctly\"\n    assert rows[2][\"primary_key_inter\"] == \"-MAT3-PLANT3\", \"Primary key should be derived correctly\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_integration/","title":"test_integration","text":""},{"location":"reference/code/tests/test_local_material/test_integration/#code.tests.test_local_material.test_integration.test_integration_empty_dataframes","title":"<code>test_integration_empty_dataframes(spark)</code>","text":"<p>Test integration with empty DataFrames.</p> Source code in <code>code/tests/test_local_material/test_integration.py</code> <pre><code>def test_integration_empty_dataframes(spark):\n    \"\"\"Test integration with empty DataFrames.\"\"\"\n    # Define schemas\n    sap_mara_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n        ]\n    )\n\n    sap_mbew_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"VPRSV\", StringType(), True),\n            StructField(\"VERPR\", DoubleType(), True),\n            StructField(\"STPRS\", DoubleType(), True),\n            StructField(\"PEINH\", LongType(), True),\n            StructField(\"BKLAS\", StringType(), True),\n        ]\n    )\n\n    sap_marc_schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    sap_t001k_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n        ]\n    )\n\n    sap_t001w_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"NAME1\", StringType(), True),\n        ]\n    )\n\n    sap_t001_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n            StructField(\"WAERS\", StringType(), True),\n        ]\n    )\n\n    # Create empty DataFrames with schemas\n    sap_mara = spark.createDataFrame([], sap_mara_schema)\n    sap_mbew = spark.createDataFrame([], sap_mbew_schema)\n    sap_marc = spark.createDataFrame([], sap_marc_schema)\n    sap_t001k = spark.createDataFrame([], sap_t001k_schema)\n    sap_t001w = spark.createDataFrame([], sap_t001w_schema)\n    sap_t001 = spark.createDataFrame([], sap_t001_schema)\n\n    # Execute integration\n    result_df = integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)\n\n    # Verify result is empty DataFrame\n    assert result_df.count() == 0\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_integration/#code.tests.test_local_material.test_integration.test_integration_happy_path","title":"<code>test_integration_happy_path(spark)</code>","text":"<p>Test integration with complete matching data.</p> Source code in <code>code/tests/test_local_material/test_integration.py</code> <pre><code>def test_integration_happy_path(spark: SparkSession):\n    \"\"\"Test integration with complete matching data.\"\"\"\n    # Setup test data\n    sap_mara_data = [(\"100\", \"MAT1\", \"UOM1\", \"GMN1\")]\n    sap_mbew_data = [(\"100\", \"MAT1\", \"VAL1\", \"PC1\", 10.0, 20.0, 1, \"VC1\")]\n    sap_marc_data = [(\"ERP1\", \"MAT1\", \"PLANT1\")]\n    sap_t001k_data = [(\"100\", \"VAL1\", \"COMP1\")]\n    sap_t001w_data = [(\"100\", \"PLANT1\", \"VAL1\", \"Plant Name\")]\n    sap_t001_data = [(\"100\", \"COMP1\", \"USD\")]\n\n    # Create DataFrames\n    sap_mara = spark.createDataFrame(sap_mara_data, [\"MANDT\", \"MATNR\", \"MEINS\", \"GLOBAL_MATERIAL_NUMBER\"])\n    sap_mbew = spark.createDataFrame(\n        sap_mbew_data, [\"MANDT\", \"MATNR\", \"BWKEY\", \"VPRSV\", \"VERPR\", \"STPRS\", \"PEINH\", \"BKLAS\"]\n    )\n    sap_marc = spark.createDataFrame(sap_marc_data, [\"SOURCE_SYSTEM_ERP\", \"MATNR\", \"WERKS\"])\n    sap_t001k = spark.createDataFrame(sap_t001k_data, [\"MANDT\", \"BWKEY\", \"BUKRS\"])\n    sap_t001w = spark.createDataFrame(sap_t001w_data, [\"MANDT\", \"WERKS\", \"BWKEY\", \"NAME1\"])\n    sap_t001 = spark.createDataFrame(sap_t001_data, [\"MANDT\", \"BUKRS\", \"WAERS\"])\n\n    # Execute integration\n    result_df = integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)\n\n    # Expected data\n    expected_data = [\n        (\n            \"ERP1\",\n            \"MAT1\",\n            \"PLANT1\",\n            \"100\",\n            \"UOM1\",\n            \"GMN1\",\n            \"VAL1\",\n            \"Plant Name\",\n            \"PC1\",\n            10.0,\n            20.0,\n            1,\n            \"VC1\",\n            \"COMP1\",\n            \"USD\",\n        )\n    ]\n\n    # Create expected DataFrame with explicit column order\n    columns = [\n        \"SOURCE_SYSTEM_ERP\",\n        \"MATNR\",\n        \"WERKS\",\n        \"MANDT\",\n        \"MEINS\",\n        \"GLOBAL_MATERIAL_NUMBER\",\n        \"BWKEY\",\n        \"NAME1\",\n        \"VPRSV\",\n        \"VERPR\",\n        \"STPRS\",\n        \"PEINH\",\n        \"BKLAS\",\n        \"BUKRS\",\n        \"WAERS\",\n    ]\n\n    expected_df = spark.createDataFrame(expected_data, columns)\n\n    # Ensure result has same column order before comparison\n    result_df_ordered = result_df.select(*columns)\n\n    assertDataFrameEqual(result_df_ordered, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_integration/#code.tests.test_local_material.test_integration.test_integration_null_values","title":"<code>test_integration_null_values(spark)</code>","text":"<p>Test integration with null values.</p> Source code in <code>code/tests/test_local_material/test_integration.py</code> <pre><code>def test_integration_null_values(spark):\n    \"\"\"Test integration with null values.\"\"\"\n    # Define schemas with explicit types\n    sap_mara_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n        ]\n    )\n\n    sap_mbew_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"VPRSV\", StringType(), True),\n            StructField(\"VERPR\", DoubleType(), True),\n            StructField(\"STPRS\", DoubleType(), True),\n            StructField(\"PEINH\", LongType(), True),\n            StructField(\"BKLAS\", StringType(), True),\n        ]\n    )\n\n    sap_marc_schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    sap_t001k_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n        ]\n    )\n\n    sap_t001w_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"NAME1\", StringType(), True),\n        ]\n    )\n\n    sap_t001_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n            StructField(\"WAERS\", StringType(), True),\n        ]\n    )\n\n    # Test data with nulls\n    sap_mara = spark.createDataFrame([(\"100\", \"MAT1\", None, None)], sap_mara_schema)\n    sap_mbew = spark.createDataFrame([(\"100\", \"MAT1\", \"VAL1\", None, None, None, None, None)], sap_mbew_schema)\n    sap_marc = spark.createDataFrame([(\"ERP1\", \"MAT1\", \"PLANT1\")], sap_marc_schema)\n    sap_t001k = spark.createDataFrame([(\"100\", \"VAL1\", None)], sap_t001k_schema)\n    sap_t001w = spark.createDataFrame([(\"100\", \"PLANT1\", \"VAL1\", None)], sap_t001w_schema)\n    sap_t001 = spark.createDataFrame([(\"100\", None, None)], sap_t001_schema)\n\n    # Execute integration\n    result_df = integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)\n\n    # Verify results\n    assert result_df.count() &gt; 0, \"Result DataFrame should not be empty\"\n\n    # Get first row\n    first_row = result_df.first()\n    assert first_row is not None, \"First row should not be None\"\n\n    # Verify null values in specific columns\n    assert first_row[\"MEINS\"] is None, \"MEINS column should be None\"\n    assert first_row[\"GLOBAL_MATERIAL_NUMBER\"] is None, \"GLOBAL_MATERIAL_NUMBER column should be None\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_post_prep_local_material/","title":"test_post_prep_local_material","text":""},{"location":"reference/code/tests/test_local_material/test_post_prep_local_material/#code.tests.test_local_material.test_post_prep_local_material.test_post_prep_local_material","title":"<code>test_post_prep_local_material(spark)</code>","text":"<p>Test post-preparation for Local Material data.</p> Source code in <code>code/tests/test_local_material/test_post_prep_local_material.py</code> <pre><code>def test_post_prep_local_material(spark: SparkSession):\n    \"\"\"Test post-preparation for Local Material data.\"\"\"\n    # Setup test schema\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n            StructField(\"NAME1\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n        ]\n    )\n\n    # Test case 1: Normal case\n    test_data = [(\"SYS1\", \"MAT1\", \"PLANT1\", \"Plant Name 1\", \"GMAT1\"), (\"SYS1\", \"MAT2\", \"PLANT2\", \"Plant Name 2\", None)]\n    input_df = spark.createDataFrame(test_data, schema)\n    result = post_prep_local_material(input_df)\n\n    # Verify results\n    assert result.count() == 2\n    assert \"mtl_plant_emd\" in result.columns\n    assert \"global_mtl_id\" in result.columns\n    assert \"primary_key_intra\" in result.columns\n    assert \"primary_key_inter\" in result.columns\n\n    # Test case 2: Duplicate handling\n    dup_data = [\n        (\"SYS1\", \"MAT1\", \"PLANT1\", \"Plant Name 1\", \"GMAT1\"),\n        (\"SYS1\", \"MAT1\", \"PLANT1\", \"Plant Name 1\", \"GMAT1\"),\n    ]\n    dup_df = spark.createDataFrame(dup_data, schema)\n    dup_result = post_prep_local_material(dup_df)\n    assert dup_result.count() == 1\n\n    # Test case 3: Null values\n    null_data = [(\"SYS1\", \"MAT1\", \"PLANT1\", None, None)]\n    null_df = spark.createDataFrame(null_data, schema)\n    null_result = post_prep_local_material(null_df)\n    assert null_result.filter(F.col(\"global_mtl_id\") == \"MAT1\").count() == 1\n\n    # Test case 4: Empty DataFrame\n    empty_df = spark.createDataFrame([], schema)\n    empty_result = post_prep_local_material(empty_df)\n    assert empty_result.count() == 0\n\n    # Test case 5: Column validation\n    invalid_schema = StructType(\n        [StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True), StructField(\"MATNR\", StringType(), True)]\n    )\n    invalid_df = spark.createDataFrame([], invalid_schema)\n    with pytest.raises(Exception):\n        post_prep_local_material(invalid_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/","title":"test_prep_general_material_data","text":""},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_bismt_filtering","title":"<code>test_prep_general_material_data_bismt_filtering(spark)</code>","text":"<p>Test prep_general_material_data with BISMT filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_bismt_filtering(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with BISMT filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\", None, None),\n        (\"100\", \"10000002\", \"EA\", \"10000002\", \"ARCHIVE\", None),\n        (\"100\", \"10000003\", \"EA\", \"10000003\", \"DUPLICATE\", None),\n        (\"100\", \"10000004\", \"EA\", \"10000004\", \"RENUMBERED\", None),\n        (\"100\", \"10000005\", \"EA\", \"10000005\", \"VALID\", None),\n    ]\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\"),\n        (\"100\", \"10000005\", \"EA\", \"10000005\"),\n    ]\n    expected_schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_general_material_data(\n        input_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_empty_df","title":"<code>test_prep_general_material_data_empty_df(spark)</code>","text":"<p>Test prep_general_material_data with empty DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_empty_df(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with empty DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    empty_df = spark.createDataFrame([], schema=schema)\n\n    result_df = prep_general_material_data(\n        empty_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assert result_df.count() == 0\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_invalid_input","title":"<code>test_prep_general_material_data_invalid_input()</code>","text":"<p>Test prep_general_material_data with invalid input type.</p> <p>This test should raise a TypeError.</p> Source code in <code>code/tests/test_local_material/test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_invalid_input():\n    \"\"\"Test prep_general_material_data with invalid input type.\n\n    This test should raise a TypeError.\n    \"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_general_material_data(\n            None,\n            col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n            check_old_material_number_is_valid=True,\n            check_material_is_not_deleted=True,\n        )\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_lvorm_filtering","title":"<code>test_prep_general_material_data_lvorm_filtering(spark)</code>","text":"<p>Test prep_general_material_data with LVORM filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_lvorm_filtering(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with LVORM filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\", None, None),\n        (\"100\", \"10000002\", \"EA\", \"10000002\", None, \"X\"),\n        (\"100\", \"10000003\", \"EA\", \"10000003\", None, \"\"),\n    ]\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\"),\n        (\"100\", \"10000003\", \"EA\", \"10000003\"),\n    ]\n    expected_schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_general_material_data(\n        input_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_valid_input","title":"<code>test_prep_general_material_data_valid_input(spark)</code>","text":"<p>Test prep_general_material_data with valid input data.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_valid_input(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with valid input data.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\n            \"100\",\n            \"10000001\",\n            \"EA\",\n            \"10000001\",\n            None,\n            None,\n        ),  # Should pass - null BISMT, null LVORM\n        (\n            \"100\",\n            \"10000002\",\n            \"EA\",\n            \"10000002\",\n            \"ARCHIVE\",\n            None,\n        ),  # Should fail - ARCHIVE in BISMT\n        (\n            \"100\",\n            \"10000003\",\n            \"EA\",\n            \"10000003\",\n            None,\n            \"X\",\n        ),  # Should fail - non-empty LVORM\n    ]\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [(\"100\", \"10000001\", \"EA\", \"10000001\")]\n    expected_schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_general_material_data(\n        input_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/","title":"test_prep_material_valuation","text":""},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_bwtar_filtering","title":"<code>test_prep_material_valuation_bwtar_filtering(spark)</code>","text":"<p>Test prep_material_valuation with BWTAR filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_bwtar_filtering(spark: SparkSession):\n    \"\"\"Test prep_material_valuation with BWTAR filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\n            \"100\",\n            \"10000001\",\n            \"1000\",\n            \"S\",\n            15.0,\n            25.0,\n            1,\n            \"3000\",\n            None,\n            \"001\",\n            \"20240102\",\n        ),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\"),\n    ]\n    expected_schema = (\n        \"MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING, VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\"\n    )\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_complex_scenario","title":"<code>test_prep_material_valuation_complex_scenario(spark)</code>","text":"<p>Test combination of filtering, row numbering and deduplication.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_complex_scenario(spark: SparkSession):\n    \"\"\"Test combination of filtering, row numbering and deduplication.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        # Group 1: Valid rows with different LAEPR\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        # Group 1: Duplicate of latest record\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        # Group 1: Row with LVORM flag (should be filtered)\n        (\"100\", \"10000001\", \"1000\", \"S\", 20.0, 30.0, 1, \"3000\", \"X\", None, \"20240103\"),\n        # Group 1: Row with BWTAR (should be filtered)\n        (\n            \"100\",\n            \"10000001\",\n            \"1000\",\n            \"S\",\n            25.0,\n            35.0,\n            1,\n            \"3000\",\n            None,\n            \"001\",\n            \"20240104\",\n        ),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    # Should select only the latest valid record\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\"),\n    ]\n    expected_schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_deduplication","title":"<code>test_prep_material_valuation_deduplication(spark)</code>","text":"<p>Test deduplication of identical rows.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_deduplication(spark: SparkSession):\n    \"\"\"Test deduplication of identical rows.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        # Duplicate rows with same values\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    # Should remove duplicates\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\"),\n    ]\n    expected_schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_empty_df","title":"<code>test_prep_material_valuation_empty_df(spark)</code>","text":"<p>Test prep_material_valuation with empty DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_empty_df(spark: SparkSession):\n    \"\"\"Test prep_material_valuation with empty DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    empty_df = spark.createDataFrame([], schema=schema)\n    result_df = prep_material_valuation(empty_df)\n    assert result_df.count() == 0\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_invalid_input","title":"<code>test_prep_material_valuation_invalid_input()</code>","text":"<p>Test prep_material_valuation with invalid input type.</p> Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_invalid_input():\n    \"\"\"Test prep_material_valuation with invalid input type.\"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_material_valuation(None)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_lvorm_filtering","title":"<code>test_prep_material_valuation_lvorm_filtering(spark)</code>","text":"<p>Test prep_material_valuation with LVORM filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_lvorm_filtering(spark: SparkSession):\n    \"\"\"Test prep_material_valuation with LVORM filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", \"X\", None, \"20240102\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\"),\n    ]\n    expected_schema = (\n        \"MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING, VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\"\n    )\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_row_numbering","title":"<code>test_prep_material_valuation_row_numbering(spark)</code>","text":"<p>Test row numbering logic based on LAEPR ordering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_row_numbering(spark: SparkSession):\n    \"\"\"Test row numbering logic based on LAEPR ordering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        # Same MATNR/BWKEY group with different LAEPR dates\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        # Different MATNR/BWKEY group\n        (\"100\", \"10000002\", \"2000\", \"V\", 5.0, 10.0, 1, \"4000\", None, None, \"20240101\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\", None, None, \"20240102\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    # Should select latest LAEPR for each MATNR/BWKEY group\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\"),\n    ]\n    expected_schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_valid_input","title":"<code>test_prep_material_valuation_valid_input(spark)</code>","text":"<p>Test prep_material_valuation with valid input data.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_valid_input(spark: SparkSession):\n    \"\"\"Test prep_material_valuation with valid input data.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 5.0, 10.0, 1, \"4000\", None, None, \"20240101\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\", None, None, \"20240102\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\"),\n    ]\n    expected_schema = (\n        \"MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING, VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\"\n    )\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/","title":"test_prep_plant_and_branches","text":""},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/#code.tests.test_local_material.test_prep_plant_and_branches.test_prep_plant_and_branches_invalid_input","title":"<code>test_prep_plant_and_branches_invalid_input()</code>","text":"<p>Test prep_plant_and_branches with invalid input type.</p> Source code in <code>code/tests/test_local_material/test_prep_plant_and_branches.py</code> <pre><code>def test_prep_plant_and_branches_invalid_input():\n    \"\"\"Test prep_plant_and_branches with invalid input type.\"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_plant_and_branches(None)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/#code.tests.test_local_material.test_prep_plant_and_branches.test_prep_plant_and_branches_missing_columns","title":"<code>test_prep_plant_and_branches_missing_columns(spark)</code>","text":"<p>Test prep_plant_and_branches with missing columns in input DataFrame.</p> Source code in <code>code/tests/test_local_material/test_prep_plant_and_branches.py</code> <pre><code>def test_prep_plant_and_branches_missing_columns(spark: SparkSession):\n    \"\"\"Test prep_plant_and_branches with missing columns in input DataFrame.\"\"\"\n    data = [\n        (\"100\", \"PLANT1\", \"VAL1\"),\n        (\"100\", \"PLANT2\", \"VAL2\"),\n    ]\n    schema = \"MANDT STRING, WERKS STRING, BWKEY STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    with pytest.raises(Exception):\n        prep_plant_and_branches(input_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/#code.tests.test_local_material.test_prep_plant_and_branches.test_prep_plant_and_branches_valid_input","title":"<code>test_prep_plant_and_branches_valid_input(spark)</code>","text":"<p>Test prep_plant_and_branches with valid input data.</p> Source code in <code>code/tests/test_local_material/test_prep_plant_and_branches.py</code> <pre><code>def test_prep_plant_and_branches_valid_input(spark: SparkSession):\n    \"\"\"Test prep_plant_and_branches with valid input data.\"\"\"\n    data = [\n        (\"100\", \"PLANT1\", \"VAL1\", \"Plant 1\"),\n        (\"100\", \"PLANT2\", \"VAL2\", \"Plant 2\"),\n    ]\n    schema = \"MANDT STRING, WERKS STRING, BWKEY STRING, NAME1 STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"PLANT1\", \"VAL1\", \"Plant 1\"),\n        (\"100\", \"PLANT2\", \"VAL2\", \"Plant 2\"),\n    ]\n    expected_schema = \"MANDT STRING, WERKS STRING, BWKEY STRING, NAME1 STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_and_branches(input_df)\n\n    assertDataFrameEqual(result_df, expected_df), \"DataFrames are not equal\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/","title":"test_prep_plant_data_for_material","text":""},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_additional_fields","title":"<code>test_prep_plant_data_for_material_additional_fields(spark)</code>","text":"<p>Test additional fields handling.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_additional_fields(spark: SparkSession):\n    \"\"\"Test additional fields handling.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None, \"CAT1\"),\n        (\"SYS1\", \"10000002\", \"PLANT2\", None, \"CAT2\"),\n    ]\n    schema = \"\"\"\n        SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING,\n        LVORM STRING, CATEGORY STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", \"CAT1\"),\n        (\"SYS1\", \"10000002\", \"PLANT2\", \"CAT2\"),\n    ]\n    expected_schema = \"\"\"\n        SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING,\n        CATEGORY STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df,\n        check_deletion_flag_is_null=True,\n        drop_duplicate_records=False,\n        additional_fields=[\"CATEGORY\"],\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_deletion_flag","title":"<code>test_prep_plant_data_for_material_deletion_flag(spark)</code>","text":"<p>Test deletion flag filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_deletion_flag(spark: SparkSession):\n    \"\"\"Test deletion flag filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n        (\"SYS1\", \"10000002\", \"PLANT2\", \"X\"),\n    ]\n    schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [(\"SYS1\", \"10000001\", \"PLANT1\")]\n    expected_schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df, check_deletion_flag_is_null=True, drop_duplicate_records=False\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_duplicates","title":"<code>test_prep_plant_data_for_material_duplicates(spark)</code>","text":"<p>Test duplicate record handling.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_duplicates(spark: SparkSession):\n    \"\"\"Test duplicate record handling.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n    ]\n    schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [(\"SYS1\", \"10000001\", \"PLANT1\")]\n    expected_schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df, check_deletion_flag_is_null=True, drop_duplicate_records=True\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_invalid_input","title":"<code>test_prep_plant_data_for_material_invalid_input(spark)</code>","text":"<p>Test with invalid input types.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_invalid_input(spark: SparkSession):\n    \"\"\"Test with invalid input types.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_plant_data_for_material(\n            None, check_deletion_flag_is_null=True, drop_duplicate_records=False\n        )\n\n    with pytest.raises(\n        TypeError, match=\"check_deletion_flag_is_null must be a boolean\"\n    ):\n        prep_plant_data_for_material(\n            spark.createDataFrame([], \"SOURCE_SYSTEM_ERP STRING\"),\n            check_deletion_flag_is_null=\"True\",\n            drop_duplicate_records=False,\n        )\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_valid_input","title":"<code>test_prep_plant_data_for_material_valid_input(spark)</code>","text":"<p>Test with valid input and default parameters.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code/tests/test_local_material/test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_valid_input(spark: SparkSession):\n    \"\"\"Test with valid input and default parameters.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n        (\"SYS1\", \"10000002\", \"PLANT2\", None),\n    ]\n    schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\"),\n        (\"SYS1\", \"10000002\", \"PLANT2\"),\n    ]\n    expected_schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df, check_deletion_flag_is_null=True, drop_duplicate_records=False\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/","title":"test_prep_valuation_data","text":"<p>Tests for prep_valuation_data function in local_material module.</p>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_duplicates","title":"<code>test_prep_valuation_data_duplicates(spark)</code>","text":"<p>Test prep_valuation_data with duplicate records.</p> Source code in <code>code/tests/test_local_material/test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_duplicates(spark: SparkSession):\n    \"\"\"Test prep_valuation_data with duplicate records.\"\"\"\n    data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n        (\"100\", \"VAL1\", \"COMP1\"),\n    ]\n    schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n    ]\n    expected_schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_valuation_data(input_df)\n\n    assertDataFrameEqual(result_df, expected_df), \"DataFrames are not equal\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_invalid_input","title":"<code>test_prep_valuation_data_invalid_input()</code>","text":"<p>Test prep_valuation_data with invalid input type.</p> Source code in <code>code/tests/test_local_material/test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_invalid_input():\n    \"\"\"Test prep_valuation_data with invalid input type.\"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_valuation_data(None)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_missing_columns","title":"<code>test_prep_valuation_data_missing_columns(spark)</code>","text":"<p>Test prep_valuation_data with missing columns in input DataFrame.</p> Source code in <code>code/tests/test_local_material/test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_missing_columns(spark: SparkSession):\n    \"\"\"Test prep_valuation_data with missing columns in input DataFrame.\"\"\"\n    data = [\n        (\"100\", \"VAL1\"),\n        (\"100\", \"VAL2\"),\n    ]\n    schema = \"MANDT STRING, BWKEY STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    with pytest.raises(Exception):\n        prep_valuation_data(input_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_valid_input","title":"<code>test_prep_valuation_data_valid_input(spark)</code>","text":"<p>Test prep_valuation_data with valid input data.</p> Source code in <code>code/tests/test_local_material/test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_valid_input(spark: SparkSession):\n    \"\"\"Test prep_valuation_data with valid input data.\"\"\"\n    data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n        (\"100\", \"VAL2\", \"COMP2\"),\n    ]\n    schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n        (\"100\", \"VAL2\", \"COMP2\"),\n    ]\n    expected_schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_valuation_data(input_df)\n\n    assertDataFrameEqual(result_df, expected_df), \"DataFrames are not equal\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/","title":"test_process_order","text":""},{"location":"reference/code/tests/test_process_order/test_integration/","title":"test_integration","text":""},{"location":"reference/code/tests/test_process_order/test_integration/#code.tests.test_process_order.test_integration.sample_data","title":"<code>sample_data(spark)</code>","text":"<p>Fixture to create sample data for tests.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict[str, DataFrame]: Sample DataFrames for testing.</p> Source code in <code>code/tests/test_process_order/test_integration.py</code> <pre><code>@pytest.fixture\ndef sample_data(spark: SparkSession) -&gt; dict:\n    \"\"\"Fixture to create sample data for tests.\n\n    Args:\n        spark (SparkSession): The Spark session.\n\n    Returns:\n        dict[str, DataFrame]: Sample DataFrames for testing.\n    \"\"\"\n    sap_afko = spark.createDataFrame(\n        data=[(\"1000\", \"OBJ1\", \"2024-12-31\")],\n        schema=StructType(\n            [\n                StructField(\"AUFNR\", StringType(), True),\n                StructField(\"OBJNR\", StringType(), True),\n                StructField(\"GLTRP\", StringType(), True),\n            ]\n        ),\n    )\n    sap_afpo = spark.createDataFrame(\n        data=[(\"1000\", \"MATERIAL1\")],\n        schema=StructType(\n            [\n                StructField(\"AUFNR\", StringType(), True),\n                StructField(\"MATNR\", StringType(), True),\n            ]\n        ),\n    )\n    sap_aufk = spark.createDataFrame(\n        data=[(\"1000\", None)],\n        schema=StructType(\n            [\n                StructField(\"AUFNR\", StringType(), True),\n                StructField(\"ZZGLTRP_ORIG\", StringType(), True),\n            ]\n        ),\n    )\n    sap_mara = spark.createDataFrame(\n        data=[(\"MATERIAL1\", \"Sample Material\")],\n        schema=StructType(\n            [\n                StructField(\"MATNR\", StringType(), True),\n                StructField(\"DESCRIPTION\", StringType(), True),\n            ]\n        ),\n    )\n    sap_cdpos = spark.createDataFrame(\n        data=[(\"OBJ1\", \"Change1\")],\n        schema=StructType(\n            [\n                StructField(\"OBJNR\", StringType(), True),\n                StructField(\"CHANGE\", StringType(), True),\n            ]\n        ),\n    )\n    return {\n        \"sap_afko\": sap_afko,\n        \"sap_afpo\": sap_afpo,\n        \"sap_aufk\": sap_aufk,\n        \"sap_mara\": sap_mara,\n        \"sap_cdpos\": sap_cdpos,\n    }\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_integration/#code.tests.test_process_order.test_integration.test_integration_invalid_input","title":"<code>test_integration_invalid_input(spark)</code>","text":"<p>Test <code>integration</code> function with invalid input.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session.</p> required Source code in <code>code/tests/test_process_order/test_integration.py</code> <pre><code>def test_integration_invalid_input(spark: SparkSession):\n    \"\"\"Test `integration` function with invalid input.\n\n    Args:\n        spark (SparkSession): The Spark session.\n    \"\"\"\n    with pytest.raises(TypeError, match=\"Expected DataFrame for sap_afko\"):\n        integration(\n            sap_afko=\"not_a_dataframe\",\n            sap_afpo=None,\n            sap_aufk=None,\n            sap_mara=None,\n        )\n\n    with pytest.raises(TypeError, match=\"Expected DataFrame or None for sap_cdpos\"):\n        integration(\n            sap_afko=spark.createDataFrame([Row(AUFNR=\"1000\")]),\n            sap_afpo=spark.createDataFrame([Row(AUFNR=\"1000\")]),\n            sap_aufk=spark.createDataFrame([Row(AUFNR=\"1000\")]),\n            sap_mara=spark.createDataFrame([Row(AUFNR=\"1000\")]),\n            sap_cdpos=\"not_a_dataframe\",\n        )\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_integration/#code.tests.test_process_order.test_integration.test_integration_required_dataframes","title":"<code>test_integration_required_dataframes(sample_data)</code>","text":"<p>Test <code>integration</code> function with all required DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session.</p> required <code>sample_data</code> <code>dict</code> <p>Sample data for testing.</p> required Source code in <code>code/tests/test_process_order/test_integration.py</code> <pre><code>def test_integration_required_dataframes(sample_data):\n    \"\"\"Test `integration` function with all required DataFrames.\n\n    Args:\n        spark (SparkSession): The Spark session.\n        sample_data (dict): Sample data for testing.\n    \"\"\"\n    result = integration(\n        sap_afko=sample_data[\"sap_afko\"],\n        sap_afpo=sample_data[\"sap_afpo\"],\n        sap_aufk=sample_data[\"sap_aufk\"],\n        sap_mara=sample_data[\"sap_mara\"],\n    )\n    assert isinstance(result, DataFrame), \"Result should be a DataFrame.\"\n    assert result.count() == 1, \"Result should contain 1 row.\"\n    assert \"ZZGLTRP_ORIG\" in result.columns, \"Result should have 'ZZGLTRP_ORIG' column.\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_integration/#code.tests.test_process_order.test_integration.test_integration_with_optional_dataframe","title":"<code>test_integration_with_optional_dataframe(sample_data)</code>","text":"<p>Test <code>integration</code> function with an optional DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session.</p> required <code>sample_data</code> <code>dict</code> <p>Sample data for testing.</p> required Source code in <code>code/tests/test_process_order/test_integration.py</code> <pre><code>def test_integration_with_optional_dataframe(sample_data):\n    \"\"\"Test `integration` function with an optional DataFrame.\n\n    Args:\n        spark (SparkSession): The Spark session.\n        sample_data (dict): Sample data for testing.\n    \"\"\"\n    result = integration(\n        sap_afko=sample_data[\"sap_afko\"],\n        sap_afpo=sample_data[\"sap_afpo\"],\n        sap_aufk=sample_data[\"sap_aufk\"],\n        sap_mara=sample_data[\"sap_mara\"],\n        sap_cdpos=sample_data[\"sap_cdpos\"],\n    )\n    assert \"CHANGE\" in result.columns, \"Result should include columns from optional DataFrame.\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_post_prep_process_order/","title":"test_post_prep_process_order","text":""},{"location":"reference/code/tests/test_process_order/test_post_prep_process_order/#code.tests.test_process_order.test_post_prep_process_order.input_df","title":"<code>input_df(spark)</code>","text":"<p>Create sample input DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>SparkSession fixture</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Sample process order data</p> Source code in <code>code/tests/test_process_order/test_post_prep_process_order.py</code> <pre><code>@pytest.fixture\ndef input_df(spark: SparkSession) -&gt; DataFrame:\n    \"\"\"Create sample input DataFrame.\n\n    Args:\n        spark: SparkSession fixture\n\n    Returns:\n        DataFrame: Sample process order data\n    \"\"\"\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"POSNR\", StringType(), True),\n            StructField(\"DWERK\", StringType(), True),\n            StructField(\"ZZGLTRP_ORIG\", DateType(), True),\n            StructField(\"LTRMI\", DateType(), True),\n            StructField(\"GSTRI\", DateType(), True),\n            StructField(\"KDAUF\", StringType(), True),\n        ]\n    )\n\n    data = [\n        # On time MTO\n        (\"ERP1\", \"ORD1\", \"10\", \"PLANT1\", datetime(2023, 1, 10), datetime(2023, 1, 15), datetime(2023, 1, 1), \"SALES1\"),\n        # Late MTS\n        (\"ERP1\", \"ORD2\", \"20\", \"PLANT2\", datetime(2023, 2, 25), datetime(2023, 2, 20), datetime(2023, 2, 1), None),\n        # Very late MTO\n        (\"ERP1\", \"ORD3\", \"30\", \"PLANT1\", datetime(2023, 3, 25), datetime(2023, 3, 5), datetime(2023, 3, 1), \"SALES2\"),\n        # Null dates\n        (\"ERP1\", \"ORD4\", \"40\", \"PLANT2\", None, None, None, None),\n    ]\n\n    return spark.createDataFrame(data, schema)\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_post_prep_process_order/#code.tests.test_process_order.test_post_prep_process_order.test_post_prep_process_order_late_buckets","title":"<code>test_post_prep_process_order_late_buckets(input_df)</code>","text":"<p>Test late delivery bucket categorization.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>Input DataFrame fixture</p> required Source code in <code>code/tests/test_process_order/test_post_prep_process_order.py</code> <pre><code>def test_post_prep_process_order_late_buckets(input_df: DataFrame):\n    \"\"\"Test late delivery bucket categorization.\n\n    Args:\n        input_df: Input DataFrame fixture\n    \"\"\"\n    result = post_prep_process_order(input_df)\n\n    assert result.filter(result.AUFNR == \"ORD1\").first()[\"late_delivery_bucket\"] == \"On Time\"\n    assert result.filter(result.AUFNR == \"ORD2\").first()[\"late_delivery_bucket\"] == \"1-7 Days Late\"\n    assert result.filter(result.AUFNR == \"ORD3\").first()[\"late_delivery_bucket\"] == \"&gt;14 Days Late\"\n    assert result.filter(result.AUFNR == \"ORD4\").first()[\"late_delivery_bucket\"] == \"Unknown\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_post_prep_process_order/#code.tests.test_process_order.test_post_prep_process_order.test_post_prep_process_order_mto_mts","title":"<code>test_post_prep_process_order_mto_mts(input_df)</code>","text":"<p>Test MTO vs MTS flag derivation.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>Input DataFrame fixture</p> required Source code in <code>code/tests/test_process_order/test_post_prep_process_order.py</code> <pre><code>def test_post_prep_process_order_mto_mts(input_df: DataFrame):\n    \"\"\"Test MTO vs MTS flag derivation.\n\n    Args:\n        input_df: Input DataFrame fixture\n    \"\"\"\n    result = post_prep_process_order(input_df)\n\n    assert result.filter(result.KDAUF == \"SALES1\").first()[\"mto_vs_mts_flag\"] == \"MTO\"\n    assert result.filter(result.KDAUF.isNull()).first()[\"mto_vs_mts_flag\"] == \"MTS\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_post_prep_process_order/#code.tests.test_process_order.test_post_prep_process_order.test_post_prep_process_order_null_handling","title":"<code>test_post_prep_process_order_null_handling(input_df)</code>","text":"<p>Test handling of null values.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>Input DataFrame fixture</p> required Source code in <code>code/tests/test_process_order/test_post_prep_process_order.py</code> <pre><code>def test_post_prep_process_order_null_handling(input_df: DataFrame):\n    \"\"\"Test handling of null values.\n\n    Args:\n        input_df: Input DataFrame fixture\n    \"\"\"\n    result = post_prep_process_order(input_df)\n    null_row = result.filter(result.AUFNR == \"ORD4\").first()\n\n    assert null_row[\"on_time_flag\"] is None\n    assert null_row[\"actual_on_time_deviation\"] is None\n    assert null_row[\"order_finish_timestamp\"] is None\n    assert null_row[\"order_start_timestamp\"] is None\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_post_prep_process_order/#code.tests.test_process_order.test_post_prep_process_order.test_post_prep_process_order_on_time_calcs","title":"<code>test_post_prep_process_order_on_time_calcs(input_df)</code>","text":"<p>Test on-time delivery calculations.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>Input DataFrame fixture</p> required Source code in <code>code/tests/test_process_order/test_post_prep_process_order.py</code> <pre><code>def test_post_prep_process_order_on_time_calcs(input_df: DataFrame):\n    \"\"\"Test on-time delivery calculations.\n\n    Args:\n        input_df: Input DataFrame fixture\n    \"\"\"\n    result = post_prep_process_order(input_df)\n\n    # On time case\n    on_time = result.filter(result.AUFNR == \"ORD1\").first()\n    assert on_time[\"on_time_flag\"] == 0\n    assert on_time[\"actual_on_time_deviation\"] == -5\n\n    # Late case\n    late = result.filter(result.AUFNR == \"ORD2\").first()\n    assert late[\"on_time_flag\"] == 1\n    assert late[\"actual_on_time_deviation\"] == 5\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_post_prep_process_order/#code.tests.test_process_order.test_post_prep_process_order.test_post_prep_process_order_primary_keys","title":"<code>test_post_prep_process_order_primary_keys(input_df)</code>","text":"<p>Test primary key generation.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>Input DataFrame fixture</p> required Source code in <code>code/tests/test_process_order/test_post_prep_process_order.py</code> <pre><code>def test_post_prep_process_order_primary_keys(input_df: DataFrame):\n    \"\"\"Test primary key generation.\n\n    Args:\n        input_df: Input DataFrame fixture\n    \"\"\"\n    result = post_prep_process_order(input_df)\n\n    row = result.filter(result.AUFNR == \"ORD1\").first()\n    assert row[\"primary_key_intra\"] == \"ORD1_10_PLANT1\"\n    assert row[\"primary_key_inter\"] == \"ERP1_ORD1_10_PLANT1\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_general_material_data/","title":"test_prep_sap_general_material_data","text":"<p>Tests for prep_sap_general_material_data function.</p>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_general_material_data/#code.tests.test_process_order.test_prep_sap_general_material_data.test_prep_sap_general_material_data_column_renaming","title":"<code>test_prep_sap_general_material_data_column_renaming(spark)</code>","text":"<p>Test prep_sap_general_material_data with column renaming.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_general_material_data.py</code> <pre><code>def test_prep_sap_general_material_data_column_renaming(spark: SparkSession):\n    \"\"\"Test prep_sap_general_material_data with column renaming.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"CUSTOM_GLOBAL\", StringType(), True),\n            StructField(\"NTGEW\", DoubleType(), True),\n            StructField(\"MTART\", StringType(), True),\n            StructField(\"BISMT\", StringType(), True),\n            StructField(\"LVORM\", StringType(), True),\n        ]\n    )\n\n    data = [(\"MAT1\", \"GMAT1\", 10.5, \"TYPE1\", None, None)]\n\n    df = spark.createDataFrame(data, schema)\n    result = prep_sap_general_material_data(df, \"CUSTOM_GLOBAL\")\n\n    assert \"GLOBAL_MATERIAL_NUMBER\" in result.columns\n    assert result.first()[\"GLOBAL_MATERIAL_NUMBER\"] == \"GMAT1\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_general_material_data/#code.tests.test_process_order.test_prep_sap_general_material_data.test_prep_sap_general_material_data_filtering","title":"<code>test_prep_sap_general_material_data_filtering(spark)</code>","text":"<p>Test prep_sap_general_material_data with filtering.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_general_material_data.py</code> <pre><code>def test_prep_sap_general_material_data_filtering(spark: SparkSession):\n    \"\"\"Test prep_sap_general_material_data with filtering.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"GLOBAL_MAT\", StringType(), True),\n            StructField(\"NTGEW\", DoubleType(), True),\n            StructField(\"MTART\", StringType(), True),\n            StructField(\"BISMT\", StringType(), True),\n            StructField(\"LVORM\", StringType(), True),\n        ]\n    )\n\n    data = [\n        (\"MAT1\", \"GMAT1\", 10.5, \"TYPE1\", None, None),  # Should be included\n        (\"MAT2\", \"GMAT2\", 20.0, \"TYPE2\", \"ARCHIVE\", None),  # Should be excluded\n        (\"MAT3\", \"GMAT3\", 30.0, \"TYPE3\", \"DUPLICATE\", \"\"),  # Should be excluded\n        (\"MAT4\", \"GMAT4\", 40.0, \"TYPE4\", None, \"X\"),  # Should be excluded\n    ]\n\n    df = spark.createDataFrame(data, schema)\n    result = prep_sap_general_material_data(df, \"GLOBAL_MAT\")\n\n    assert result.count() == 1\n    assert result.first()[\"MATNR\"] == \"MAT1\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_general_material_data/#code.tests.test_process_order.test_prep_sap_general_material_data.test_prep_sap_general_material_data_invalid_inputs","title":"<code>test_prep_sap_general_material_data_invalid_inputs(spark)</code>","text":"<p>Test prep_sap_general_material_data with invalid inputs.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_general_material_data.py</code> <pre><code>def test_prep_sap_general_material_data_invalid_inputs(spark: SparkSession):\n    \"\"\"Test prep_sap_general_material_data with invalid inputs.\"\"\"\n    with pytest.raises(TypeError, match=\"Invalid input DataFrame\"):\n        prep_sap_general_material_data(None, \"GLOBAL_MAT\")\n\n    with pytest.raises(TypeError, match=\"Invalid input DataFrame\"):\n        prep_sap_general_material_data(\"not a dataframe\", \"GLOBAL_MAT\")\n\n    with pytest.raises(TypeError, match=\"Invalid input column name\"):\n        prep_sap_general_material_data(spark.createDataFrame([], StructType([])), None)\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_general_material_data/#code.tests.test_process_order.test_prep_sap_general_material_data.test_prep_sap_general_material_data_valid_input","title":"<code>test_prep_sap_general_material_data_valid_input(spark)</code>","text":"<p>Test prep_sap_general_material_data with valid input data.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_general_material_data.py</code> <pre><code>def test_prep_sap_general_material_data_valid_input(spark: SparkSession):\n    \"\"\"Test prep_sap_general_material_data with valid input data.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"GLOBAL_MAT\", StringType(), True),\n            StructField(\"NTGEW\", DoubleType(), True),\n            StructField(\"MTART\", StringType(), True),\n            StructField(\"BISMT\", StringType(), True),\n            StructField(\"LVORM\", StringType(), True),\n        ]\n    )\n\n    data = [(\"MAT1\", \"GMAT1\", 10.5, \"TYPE1\", None, None), (\"MAT2\", \"GMAT2\", 20.0, \"TYPE2\", \"VALID\", \"\")]\n\n    df = spark.createDataFrame(data, schema)\n    result = prep_sap_general_material_data(df, \"GLOBAL_MAT\")\n\n    assert result.count() == 2\n    assert set(result.columns) == {\"MATNR\", \"GLOBAL_MATERIAL_NUMBER\", \"NTGEW\", \"MTART\"}\n    first_row = result.first()\n    assert first_row[\"MATNR\"] == \"MAT1\"\n    assert first_row[\"GLOBAL_MATERIAL_NUMBER\"] == \"GMAT1\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_header_data/","title":"test_prep_sap_order_header_data","text":"<p>Tests for prep_sap_order_header_data function.</p>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_header_data/#code.tests.test_process_order.test_prep_sap_order_header_data.test_prep_sap_order_header_data_empty_df","title":"<code>test_prep_sap_order_header_data_empty_df(spark)</code>","text":"<p>Test prep_sap_order_header_data with empty DataFrame.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_header_data.py</code> <pre><code>def test_prep_sap_order_header_data_empty_df(spark: SparkSession):\n    \"\"\"Test prep_sap_order_header_data with empty DataFrame.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"GLTRP\", DateType(), True),\n            StructField(\"GSTRP\", DateType(), True),\n            StructField(\"FTRMS\", DateType(), True),\n            StructField(\"GLTRS\", DateType(), True),\n            StructField(\"GSTRS\", DateType(), True),\n            StructField(\"GSTRI\", DateType(), True),\n            StructField(\"GETRI\", DateType(), True),\n            StructField(\"GLTRI\", DateType(), True),\n            StructField(\"FTRMI\", DateType(), True),\n            StructField(\"FTRMP\", DateType(), True),\n            StructField(\"DISPO\", StringType(), True),\n            StructField(\"FEVOR\", StringType(), True),\n            StructField(\"PLGRP\", StringType(), True),\n            StructField(\"FHORI\", StringType(), True),\n            StructField(\"AUFPL\", StringType(), True),\n        ]\n    )\n\n    df = spark.createDataFrame([], schema)\n    result = prep_sap_order_header_data(df)\n\n    assert result.count() == 0\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_header_data/#code.tests.test_process_order.test_prep_sap_order_header_data.test_prep_sap_order_header_data_invalid_input","title":"<code>test_prep_sap_order_header_data_invalid_input()</code>","text":"<p>Test prep_sap_order_header_data with invalid input data.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_header_data.py</code> <pre><code>def test_prep_sap_order_header_data_invalid_input():\n    \"\"\"Test prep_sap_order_header_data with invalid input data.\"\"\"\n    with pytest.raises(TypeError):\n        prep_sap_order_header_data(None)\n\n    with pytest.raises(TypeError):\n        prep_sap_order_header_data(\"not a dataframe\")\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_header_data/#code.tests.test_process_order.test_prep_sap_order_header_data.test_prep_sap_order_header_data_null_dates","title":"<code>test_prep_sap_order_header_data_null_dates(spark)</code>","text":"<p>Test prep_sap_order_header_data with null dates.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_header_data.py</code> <pre><code>def test_prep_sap_order_header_data_null_dates(spark: SparkSession):\n    \"\"\"Test prep_sap_order_header_data with null dates.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"GLTRP\", DateType(), True),\n            StructField(\"GSTRP\", DateType(), True),\n            StructField(\"FTRMS\", DateType(), True),\n            StructField(\"GLTRS\", DateType(), True),\n            StructField(\"GSTRS\", DateType(), True),\n            StructField(\"GSTRI\", DateType(), True),\n            StructField(\"GETRI\", DateType(), True),\n            StructField(\"GLTRI\", DateType(), True),\n            StructField(\"FTRMI\", DateType(), True),\n            StructField(\"FTRMP\", DateType(), True),\n            StructField(\"DISPO\", StringType(), True),\n            StructField(\"FEVOR\", StringType(), True),\n            StructField(\"PLGRP\", StringType(), True),\n            StructField(\"FHORI\", StringType(), True),\n            StructField(\"AUFPL\", StringType(), True),\n        ]\n    )\n\n    data = [\n        (\n            \"ERP1\",\n            \"100\",\n            \"ORDER1\",\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            \"D1\",\n            \"S1\",\n            \"P1\",\n            \"H1\",\n            \"A1\",\n        )\n    ]\n\n    df = spark.createDataFrame(data, schema)\n    result = prep_sap_order_header_data(df)\n\n    assert result.count() == 1\n    assert not result.select(\"start_date\").first()[0] is None\n    assert not result.select(\"finish_date\").first()[0] is None\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_header_data/#code.tests.test_process_order.test_prep_sap_order_header_data.test_prep_sap_order_header_data_valid_input","title":"<code>test_prep_sap_order_header_data_valid_input(spark)</code>","text":"<p>Test prep_sap_order_header_data with valid input data.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_header_data.py</code> <pre><code>def test_prep_sap_order_header_data_valid_input(spark: SparkSession):\n    \"\"\"Test prep_sap_order_header_data with valid input data.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"GLTRP\", DateType(), True),\n            StructField(\"GSTRP\", DateType(), True),\n            StructField(\"FTRMS\", DateType(), True),\n            StructField(\"GLTRS\", DateType(), True),\n            StructField(\"GSTRS\", DateType(), True),\n            StructField(\"GSTRI\", DateType(), True),\n            StructField(\"GETRI\", DateType(), True),\n            StructField(\"GLTRI\", DateType(), True),\n            StructField(\"FTRMI\", DateType(), True),\n            StructField(\"FTRMP\", DateType(), True),\n            StructField(\"DISPO\", StringType(), True),\n            StructField(\"FEVOR\", StringType(), True),\n            StructField(\"PLGRP\", StringType(), True),\n            StructField(\"FHORI\", StringType(), True),\n            StructField(\"AUFPL\", StringType(), True),\n        ]\n    )\n\n    data = [\n        (\n            \"ERP1\",\n            \"100\",\n            \"ORDER1\",\n            datetime(2023, 12, 31),\n            datetime(2023, 1, 1),\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n            \"D1\",\n            \"S1\",\n            \"P1\",\n            \"H1\",\n            \"A1\",\n        )\n    ]\n\n    df = spark.createDataFrame(data, schema)\n    result = prep_sap_order_header_data(df)\n\n    assert result.count() == 1\n    assert \"start_date\" in result.columns\n    assert \"finish_date\" in result.columns\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_item_data/","title":"test_prep_sap_order_item_data","text":"<p>Tests for prep_sap_order_item function in process_order module.</p>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_item_data/#code.tests.test_process_order.test_prep_sap_order_item_data.test_prep_sap_order_item_empty_df","title":"<code>test_prep_sap_order_item_empty_df(spark)</code>","text":"<p>Test prep_sap_order_item with empty DataFrame.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_item_data.py</code> <pre><code>def test_prep_sap_order_item_empty_df(spark: SparkSession):\n    \"\"\"Test prep_sap_order_item with empty DataFrame.\"\"\"\n    # Setup schema\n    schema = StructType(\n        [\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"POSNR\", LongType(), True),\n            StructField(\"DWERK\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"KDAUF\", StringType(), True),\n            StructField(\"KDPOS\", StringType(), True),\n            StructField(\"LTRMI\", StringType(), True),\n        ]\n    )\n\n    input_df = spark.createDataFrame([], schema)\n    result = prep_sap_order_item(input_df)\n\n    assert result.count() == 0\n    assert len(result.columns) == 8\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_item_data/#code.tests.test_process_order.test_prep_sap_order_item_data.test_prep_sap_order_item_null_values","title":"<code>test_prep_sap_order_item_null_values(spark)</code>","text":"<p>Test prep_sap_order_item with null values in input data.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_item_data.py</code> <pre><code>def test_prep_sap_order_item_null_values(spark):\n    \"\"\"Test prep_sap_order_item with null values in input data.\"\"\"\n    # Setup schema with nullable fields\n    schema = StructType(\n        [\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"POSNR\", LongType(), True),\n            StructField(\"DWERK\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"KDAUF\", StringType(), True),\n            StructField(\"KDPOS\", StringType(), True),\n            StructField(\"LTRMI\", StringType(), True),\n        ]\n    )\n\n    # Test data with null values\n    data = [(\"ORDER1\", 10, None, \"MAT1\", None, \"SALES1\", None, None)]\n\n    input_df = spark.createDataFrame(data, schema)\n    result = prep_sap_order_item(input_df)\n\n    assert result.count() == 1\n    assert result.select(\"DWERK\").first()[0] is None\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_item_data/#code.tests.test_process_order.test_prep_sap_order_item_data.test_prep_sap_order_item_valid_input","title":"<code>test_prep_sap_order_item_valid_input(spark)</code>","text":"<p>Test prep_sap_order_item with valid input data.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_item_data.py</code> <pre><code>def test_prep_sap_order_item_valid_input(spark: SparkSession):\n    \"\"\"Test prep_sap_order_item with valid input data.\"\"\"\n    # Setup schema\n    schema = StructType(\n        [\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"POSNR\", LongType(), True),\n            StructField(\"DWERK\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"KDAUF\", StringType(), True),\n            StructField(\"KDPOS\", StringType(), True),\n            StructField(\"LTRMI\", StringType(), True),\n            StructField(\"EXTRA_COL\", StringType(), True),  # Extra column that should be filtered out\n        ]\n    )\n\n    # Test data\n    data = [\n        (\"ORDER1\", 10, \"PLANT1\", \"MAT1\", \"PC\", \"SALES1\", \"ITEM1\", \"2023-10-01\", \"EXTRA1\"),\n        (\"ORDER2\", 20, \"PLANT2\", \"MAT2\", \"KG\", \"SALES2\", \"ITEM2\", \"2023-10-02\", \"EXTRA2\"),\n    ]\n\n    input_df = spark.createDataFrame(data, schema)\n    result = prep_sap_order_item(input_df)\n\n    # Verify results\n    assert result.count() == 2\n    assert len(result.columns) == 8  # Check only required columns are present\n    required_columns = [\"AUFNR\", \"POSNR\", \"DWERK\", \"MATNR\", \"MEINS\", \"KDAUF\", \"KDPOS\", \"LTRMI\"]\n    assert all(col in result.columns for col in required_columns)\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_master_data/","title":"test_prep_sap_order_master_data","text":"<p>Tests for prep_sap_order_master_data function.</p>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_master_data/#code.tests.test_process_order.test_prep_sap_order_master_data.test_prep_sap_order_master_data_empty_df","title":"<code>test_prep_sap_order_master_data_empty_df(spark)</code>","text":"<p>Test with empty DataFrame.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_master_data.py</code> <pre><code>def test_prep_sap_order_master_data_empty_df(spark: SparkSession):\n    \"\"\"Test with empty DataFrame.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"OBJNR\", StringType(), True),\n            StructField(\"ERDAT\", DateType(), True),\n            StructField(\"ERNAM\", StringType(), True),\n            StructField(\"AUART\", StringType(), True),\n            StructField(\"ZZGLTRP_ORIG\", StringType(), True),\n            StructField(\"ZZPRO_TEXT\", StringType(), True),\n        ]\n    )\n\n    df = spark.createDataFrame([], schema)\n    result = prep_sap_order_master_data(df)\n\n    assert result.count() == 0\n    assert len(result.columns) == 7\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_master_data/#code.tests.test_process_order.test_prep_sap_order_master_data.test_prep_sap_order_master_data_invalid_input","title":"<code>test_prep_sap_order_master_data_invalid_input()</code>","text":"<p>Test with invalid input.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_master_data.py</code> <pre><code>def test_prep_sap_order_master_data_invalid_input():\n    \"\"\"Test with invalid input.\"\"\"\n    with pytest.raises(TypeError, match=\"Invalid input DataFrame\"):\n        prep_sap_order_master_data(None)\n\n    with pytest.raises(TypeError, match=\"Invalid input DataFrame\"):\n        prep_sap_order_master_data(\"not a dataframe\")\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_master_data/#code.tests.test_process_order.test_prep_sap_order_master_data.test_prep_sap_order_master_data_null_values","title":"<code>test_prep_sap_order_master_data_null_values(spark)</code>","text":"<p>Test with null values.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_master_data.py</code> <pre><code>def test_prep_sap_order_master_data_null_values(spark: SparkSession):\n    \"\"\"Test with null values.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"OBJNR\", StringType(), True),\n            StructField(\"ERDAT\", DateType(), True),\n            StructField(\"ERNAM\", StringType(), True),\n            StructField(\"AUART\", StringType(), True),\n            StructField(\"ZZGLTRP_ORIG\", StringType(), True),\n            StructField(\"ZZPRO_TEXT\", StringType(), True),\n        ]\n    )\n\n    data = [(None, None, None, None, None, None, None)]\n    df = spark.createDataFrame(data, schema)\n    result = prep_sap_order_master_data(df)\n\n    assert result.count() == 1\n    for col in result.columns:\n        assert result.select(col).first()[0] is None\n</code></pre>"},{"location":"reference/code/tests/test_process_order/test_prep_sap_order_master_data/#code.tests.test_process_order.test_prep_sap_order_master_data.test_prep_sap_order_master_data_valid_input","title":"<code>test_prep_sap_order_master_data_valid_input(spark)</code>","text":"<p>Test with valid input data.</p> Source code in <code>code/tests/test_process_order/test_prep_sap_order_master_data.py</code> <pre><code>def test_prep_sap_order_master_data_valid_input(spark: SparkSession):\n    \"\"\"Test with valid input data.\"\"\"\n    schema = StructType(\n        [\n            StructField(\"AUFNR\", StringType(), True),\n            StructField(\"OBJNR\", StringType(), True),\n            StructField(\"ERDAT\", DateType(), True),\n            StructField(\"ERNAM\", StringType(), True),\n            StructField(\"AUART\", StringType(), True),\n            StructField(\"ZZGLTRP_ORIG\", StringType(), True),\n            StructField(\"ZZPRO_TEXT\", StringType(), True),\n            StructField(\"EXTRA_COL\", StringType(), True),\n        ]\n    )\n\n    data = [(\"ORDER1\", \"OBJ1\", datetime(2023, 1, 1), \"USER1\", \"TYPE1\", \"ORIG1\", \"PROJECT1\", \"EXTRA1\")]\n\n    df = spark.createDataFrame(data, schema)\n    result = prep_sap_order_master_data(df)\n\n    assert result.count() == 1\n    assert len(result.columns) == 7\n    expected_columns = [\"AUFNR\", \"OBJNR\", \"ERDAT\", \"ERNAM\", \"AUART\", \"ZZGLTRP_ORIG\", \"ZZPRO_TEXT\"]\n    assert all(col in result.columns for col in expected_columns)\n</code></pre>"}]}