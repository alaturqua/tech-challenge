{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>code<ul> <li>main</li> <li>modules<ul> <li>local_material</li> <li>process_order</li> <li>transformations</li> <li>utils</li> </ul> </li> <li>tests<ul> <li>conftest</li> <li>test_local_material<ul> <li>test_derive_intra_and_inter_primary_key</li> <li>test_integration</li> <li>test_post_prep_local_material</li> <li>test_prep_general_material_data</li> <li>test_prep_material_valuation</li> <li>test_prep_plant_and_branches</li> <li>test_prep_plant_data_for_material</li> <li>test_prep_valuation_data</li> </ul> </li> <li>test_process_order</li> <li>test_utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/code/","title":"code","text":""},{"location":"reference/code/main/","title":"main","text":"<p>Main script to run the technical challenge.</p> <p>Exercise Overview: You will work with data from two SAP systems that have similar data sources. Your task is to process and integrate this data to provide a unified view for supply chain insights.</p> <p>The exercise involves: * Processing Local Material data.Processing Process Order data. * Ensuring both datasets have the same schema for harmonization across systems. * Writing modular, reusable code with proper documentation. * Following best-in-class principles for flexibility and maintainability.</p> <p>Note: You will create two Python scripts (local_material.py and process_order.py) for each system, i.e. in total of four scripts (2 systems per modeled entity/event).</p> <p>General Instructions Work on both SAP systems: * Perform all the steps for both systems to ensure consistency * Enable and accomplish data harmonization through a common data model</p> <p>Focus on data fields and transformations: * Pay attention to the required fields and the transformations applied to them. * Document your code: Include comments explaining why certain modules and functions are used. * Follow best practices: Write modular code, handle exceptions, and ensure reusability.</p> <p>Detailed instructions see attached PDF</p>"},{"location":"reference/code/main/#code.main.main","title":"<code>main()</code>","text":"<p>Main function to run the technical challenge.</p> Source code in <code>code\\main.py</code> <pre><code>def main():\n    \"\"\"Main function to run the technical challenge.\"\"\"\n    process_local_material()\n</code></pre>"},{"location":"reference/code/main/#code.main.process_local_material","title":"<code>process_local_material()</code>","text":"<p>Process Local Material data from two SAP systems.</p> Source code in <code>code\\main.py</code> <pre><code>def process_local_material():\n    \"\"\"Process Local Material data from two SAP systems.\"\"\"\n    # Load data\n    logger.info(\"Loading data from system_1...\")\n\n    pre_mara = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_MARA.csv\")\n    pre_mbew = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_MBEW.csv\")\n    pre_marc = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_MARC.csv\")\n    pre_t001k = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_T001K.csv\")\n    pre_t001w = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_T001W.csv\")\n    pre_t001 = read_csv_file(spark, parent_dir_name / \"data\" / \"system_1\" / \"PRE_T001.csv\")\n\n    # Process data\n    pre_mara = prep_general_material_data(\n        pre_mara,\n        col_mara_global_material_number=\"ZZMDGM\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n    pre_mbew = prep_material_valuation(pre_mbew)\n    pre_marc = prep_plant_data_for_material(pre_marc, check_deletion_flag_is_null=True, drop_duplicate_records=True)\n    pre_t001k = prep_valuation_data(pre_t001k)\n    pre_t001w = prep_plant_and_branches(pre_t001w)\n    pre_t001 = prep_company_codes(pre_t001)\n\n    logger.info(\"Loading data from system_2...\")\n    # Load data\n    prd_mara = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_MARA.csv\")\n    prd_mbew = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_MBEW.csv\")\n    prd_marc = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_MARC.csv\")\n    prd_t001k = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_T001K.csv\")\n    prd_t001w = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_T001W.csv\")\n    prd_t001 = read_csv_file(spark, parent_dir_name / \"data\" / \"system_2\" / \"PRD_T001.csv\")\n\n    # Process data\n    prd_mara = prep_general_material_data(\n        prd_mara,\n        col_mara_global_material_number=\"ZZMDGM\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n    prd_mbew = prep_material_valuation(prd_mbew)\n    prd_marc = prep_plant_data_for_material(prd_marc, check_deletion_flag_is_null=True, drop_duplicate_records=True)\n    prd_t001k = prep_valuation_data(prd_t001k)\n    prd_t001w = prep_plant_and_branches(prd_t001w)\n    prd_t001 = prep_company_codes(prd_t001)\n\n    union_mara = pre_mara.unionByName(prd_mara)\n    union_mbew = pre_mbew.unionByName(prd_mbew)\n    union_marc = pre_marc.unionByName(prd_marc)\n    union_t001k = pre_t001k.unionByName(prd_t001k)\n    union_t001w = pre_t001w.unionByName(prd_t001w)\n    union_t001 = pre_t001.unionByName(prd_t001)\n\n    # Post-process data\n    logger.info(\"integrating data...\")\n    integrated_data = integration(union_mara, union_mbew, union_marc, union_t001k, union_t001w, union_t001)\n\n    local_material = post_prep_local_material(integrated_data)\n\n    integrated_data.printSchema()\n\n    local_material.printSchema()\n\n    integrated_data.show()\n    local_material.show()\n</code></pre>"},{"location":"reference/code/modules/","title":"modules","text":""},{"location":"reference/code/modules/local_material/","title":"local_material","text":"<p>Module to prepare local material data for harmonization across systems.</p>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.derive_intra_and_inter_primary_key","title":"<code>derive_intra_and_inter_primary_key(df)</code>","text":"<p>Derive primary keys for harmonized data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with harmonized data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with derived primary keys.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def derive_intra_and_inter_primary_key(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Derive primary keys for harmonized data.\n\n    Args:\n        df (DataFrame): DataFrame with harmonized data.\n\n    Returns:\n        DataFrame: DataFrame with derived primary keys.\n    \"\"\"\n    # Replace nulls with empty strings before concatenation\n    return df.withColumn(\n        \"primary_key_intra\",\n        F.concat_ws(\"-\", F.coalesce(F.col(\"MATNR\"), F.lit(\"\")), F.coalesce(F.col(\"WERKS\"), F.lit(\"\"))),\n    ).withColumn(\n        \"primary_key_inter\",\n        F.concat_ws(\n            \"-\",\n            F.coalesce(F.col(\"SOURCE_SYSTEM_ERP\"), F.lit(\"\")),\n            F.coalesce(F.col(\"MATNR\"), F.lit(\"\")),\n            F.coalesce(F.col(\"WERKS\"), F.lit(\"\")),\n        ),\n    )\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.integration","title":"<code>integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)</code>","text":"<p>Integrate data from various SAP tables to create a comprehensive dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sap_mara</code> <code>DataFrame</code> <p>General Material Data.</p> required <code>sap_mbew</code> <code>DataFrame</code> <p>Material Valuation Data.</p> required <code>sap_marc</code> <code>DataFrame</code> <p>Plant Data for Material.</p> required <code>sap_t001k</code> <code>DataFrame</code> <p>Valuation Area Data.</p> required <code>sap_t001w</code> <code>DataFrame</code> <p>Plant and Branches Data.</p> required <code>sap_t001</code> <code>DataFrame</code> <p>Company Codes Data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Integrated DataFrame with all necessary information.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def integration(\n    sap_mara: DataFrame,\n    sap_mbew: DataFrame,\n    sap_marc: DataFrame,\n    sap_t001k: DataFrame,\n    sap_t001w: DataFrame,\n    sap_t001: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"Integrate data from various SAP tables to create a comprehensive dataset.\n\n    Args:\n        sap_mara (DataFrame): General Material Data.\n        sap_mbew (DataFrame): Material Valuation Data.\n        sap_marc (DataFrame): Plant Data for Material.\n        sap_t001k (DataFrame): Valuation Area Data.\n        sap_t001w (DataFrame): Plant and Branches Data.\n        sap_t001 (DataFrame): Company Codes Data.\n\n    Returns:\n        DataFrame: Integrated DataFrame with all necessary information.\n    \"\"\"\n    return (\n        sap_marc.join(sap_mara, on=\"MATNR\", how=\"left\")\n        .join(sap_t001w, on=[\"MANDT\", \"WERKS\"], how=\"left\")\n        .join(sap_mbew, on=[\"MANDT\", \"MATNR\", \"BWKEY\"], how=\"left\")\n        .join(sap_t001k, on=[\"MANDT\", \"BWKEY\"], how=\"left\")\n        .join(sap_t001, on=[\"MANDT\", \"BUKRS\"], how=\"left\")\n    )\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.post_prep_local_material","title":"<code>post_prep_local_material(df)</code>","text":"<p>Post-process the integrated DataFrame to create final local material data.</p> <p>Input Data: Resulting DataFrame from the integration step.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Integrated DataFrame from the integration step.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Final DataFrame with post-processed local material data.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def post_prep_local_material(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Post-process the integrated DataFrame to create final local material data.\n\n    Input Data: Resulting DataFrame from the integration step.\n\n    Args:\n        df (DataFrame): Integrated DataFrame from the integration step.\n\n    Returns:\n        DataFrame: Final DataFrame with post-processed local material data.\n    \"\"\"\n    # Add timestamp column to the DataFrame\n    df_with_timestamp = df.withColumn(\"timestamp\", F.current_timestamp())\n\n    # Window spec to get the latest record based on timestamp\n    window_spec = Window.partitionBy(\"SOURCE_SYSTEM_ERP\", \"MATNR\", \"WERKS\").orderBy(F.col(\"timestamp\").desc())\n\n    df_with_primary_keys = derive_intra_and_inter_primary_key(df_with_timestamp)\n\n    return (\n        df_with_primary_keys.withColumn(\"mtl_plant_emd\", F.concat_ws(\"-\", F.col(\"WERKS\"), F.col(\"NAME1\")))\n        .withColumn(\"global_mtl_id\", F.coalesce(F.col(\"GLOBAL_MATERIAL_NUMBER\"), F.col(\"MATNR\")))\n        .withColumn(\"row_number\", F.row_number().over(window_spec))\n        .filter(F.col(\"row_number\") == 1)\n        .drop(\"row_number\")\n        .drop(\"timestamp\")\n    )\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_company_codes","title":"<code>prep_company_codes(df)</code>","text":"<p>Prepare Company Codes data for harmonization across systems.</p> <p>Input Data: SAP T001 table (Company Codes)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Company Codes data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Company Codes data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def prep_company_codes(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Company Codes data for harmonization across systems.\n\n    Input Data: SAP T001 table (Company Codes)\n\n    Args:\n        df (DataFrame): DataFrame with Company Codes data.\n\n    Returns:\n        DataFrame: DataFrame with prepared Company Codes data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"MANDT\",  # Client\n        \"BUKRS\",  # Company Code\n        \"WAERS\",  # Currency Key\n    ]\n\n    return df.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_general_material_data","title":"<code>prep_general_material_data(df, col_mara_global_material_number, check_old_material_number_is_valid=True, check_material_is_not_deleted=True)</code>","text":"<p>Prepare General Material data for harmonization across systems.</p> <p>Input Data: SAP MARA table (General Material Data)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with General Material data.</p> required <code>col_mara_global_material_number</code> <code>str</code> <p>Column name with the global material number.</p> required <code>check_old_material_number_is_valid</code> <code>bool</code> <p>Check if the old material number is valid.</p> <code>True</code> <code>check_material_is_not_deleted</code> <code>bool</code> <p>Check if the material is not deleted.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared General Material data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> <code>TypeError</code> <p>If the column name with the global material number is not a string.</p> <code>TypeError</code> <p>If the check for the old material number is not a boolean.</p> <code>TypeError</code> <p>If the check for the material deletion flag is not a boolean.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def prep_general_material_data(\n    df: DataFrame,\n    col_mara_global_material_number: str,\n    check_old_material_number_is_valid: bool = True,\n    check_material_is_not_deleted: bool = True,\n) -&gt; DataFrame:\n    \"\"\"Prepare General Material data for harmonization across systems.\n\n    Input Data: SAP MARA table (General Material Data)\n\n    Args:\n        df (DataFrame): DataFrame with General Material data.\n        col_mara_global_material_number (str): Column name with the global material number.\n        check_old_material_number_is_valid (bool): Check if the old material number is valid.\n        check_material_is_not_deleted (bool): Check if the material is not deleted.\n\n    Returns:\n        DataFrame: DataFrame with prepared General Material data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n        TypeError: If the column name with the global material number is not a string.\n        TypeError: If the check for the old material number is not a boolean.\n        TypeError: If the check for the material deletion flag is not a boolean.\n    \"\"\"\n    if None or not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n    if None or not isinstance(col_mara_global_material_number, str):\n        error_message = \"col_mara_global_material_number must be a string\"\n        raise TypeError(error_message)\n    if None or not isinstance(check_old_material_number_is_valid, bool):\n        error_message = \"check_old_material_number_is_valid must be a boolean\"\n        raise TypeError(error_message)\n    if None or not isinstance(check_material_is_not_deleted, bool):\n        error_message = \"check_material_is_not_deleted must be a boolean\"\n        raise TypeError(error_message)\n\n    # Start with the original DataFrame\n    sap_mara = df\n\n    # Apply old material number check if enabled\n    if check_old_material_number_is_valid:\n        sap_mara = sap_mara.filter(\n            F.col(\"BISMT\").isNull() | ~F.col(\"BISMT\").isin([\"ARCHIVE\", \"DUPLICATE\", \"RENUMBERED\"])\n        )\n\n    # Apply deletion flag check if enabled\n    if check_material_is_not_deleted:\n        sap_mara = sap_mara.filter(F.col(\"LVORM\").isNull() | (F.col(\"LVORM\") == \"\"))\n\n    # Select and rename columns\n    columns = [\n        \"MANDT\",  # Client\n        \"MATNR\",  # Material Number\n        \"MEINS\",  # Base Unit of Measure\n        F.col(col_mara_global_material_number).alias(\"GLOBAL_MATERIAL_NUMBER\"),  # Global Material Number\n    ]\n\n    return sap_mara.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_material_valuation","title":"<code>prep_material_valuation(df)</code>","text":"<p>Prepare Material Valuation data for harmonization across systems.</p> <p>Input Data: SAP MBEW table (Material Valuation)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Material Valuation data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Material Valuation data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def prep_material_valuation(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Material Valuation data for harmonization across systems.\n\n    Input Data: SAP MBEW table (Material Valuation)\n\n    Args:\n        df (DataFrame): DataFrame with Material Valuation data.\n\n    Returns:\n        DataFrame: DataFrame with prepared Material Valuation data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    if None or not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    window_spec = Window.partitionBy(\"MATNR\", \"BWKEY\").orderBy(F.col(\"LAEPR\").desc())\n\n    # Start with the original DataFrame\n    sap_mbew = (\n        df.filter(F.col(\"LVORM\").isNull())  # Filter out materials that are flagged for deletion (LVORM is null).\n        .filter(\n            F.col(\"BWTAR\").isNull()\n        )  # Filter for entries with BWTAR (Valuation Type) as null to exclude split valuation materials.\n        .withColumn(\n            \"ROW_NUMBER\", F.row_number().over(window_spec)\n        )  # Add row number based on the descending order of LAEPR.\n        .filter(F.col(\"ROW_NUMBER\") == 1)  # Filter for the latest price based on the row number.\n        .drop(\"ROW_NUMBER\")  # Drop the row number column.\n    )\n\n    # Select and rename columns\n    columns = [\n        \"MANDT\",  # Client\n        \"MATNR\",  # Material Number\n        \"BWKEY\",  # Valuation Area\n        \"VPRSV\",  # Price Control Indicator\n        \"VERPR\",  # Moving Average Price\n        \"STPRS\",  # Standard Price\n        \"PEINH\",  # Price Unit\n        \"BKLAS\",  # Valuation Class\n    ]\n\n    return sap_mbew.select(*columns).drop_duplicates()\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_plant_and_branches","title":"<code>prep_plant_and_branches(df)</code>","text":"<p>Prepare Plant and Branches data for harmonization across systems.</p> <p>Input Data: SAP T001W table (Plant/Branch)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Plant and Branches data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Plant and Branches data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def prep_plant_and_branches(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Plant and Branches data for harmonization across systems.\n\n    Input Data: SAP T001W table (Plant/Branch)\n\n    Args:\n        df (DataFrame): DataFrame with Plant and Branches data.\n\n    Returns:\n        DataFrame:  DataFrame with prepared Plant and Branches data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"MANDT\",  # Client\n        \"WERKS\",  # Plant\n        \"BWKEY\",  # Valuation Area\n        \"NAME1\",  # Name of Plant/Branch\n    ]\n\n    return df.select(*columns)\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_plant_data_for_material","title":"<code>prep_plant_data_for_material(df, check_deletion_flag_is_null, drop_duplicate_records, additional_fields=None)</code>","text":"<p>Prepare Plant data for harmonization across systems.</p> <p>Input Data: SAP MARC table (Plant Data for Material)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Plant data for Material.</p> required <code>check_deletion_flag_is_null</code> <code>bool</code> <p>Check if the deletion flag is null.</p> required <code>drop_duplicate_records</code> <code>bool</code> <p>Drop duplicate records</p> required <code>additional_fields</code> <code>list</code> <p>Additional fields to include in the output DataFrame.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Plant data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> <code>TypeError</code> <p>If the check for the deletion flag is not a boolean.</p> <code>TypeError</code> <p>If the drop duplicate records flag is not a boolean.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def prep_plant_data_for_material(\n    df: DataFrame,\n    check_deletion_flag_is_null: bool,\n    drop_duplicate_records: bool,\n    additional_fields: list | None = None,\n) -&gt; DataFrame:\n    \"\"\"Prepare Plant data for harmonization across systems.\n\n    Input Data: SAP MARC table (Plant Data for Material)\n\n    Args:\n        df (DataFrame): DataFrame with Plant data for Material.\n        check_deletion_flag_is_null (bool): Check if the deletion flag is null.\n        drop_duplicate_records (bool): Drop duplicate records\n        additional_fields (list): Additional fields to include in the output DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with prepared Plant data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n        TypeError: If the check for the deletion flag is not a boolean.\n        TypeError: If the drop duplicate records flag is not a boolean.\n    \"\"\"\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n    if not isinstance(check_deletion_flag_is_null, bool):\n        error_message = \"check_deletion_flag_is_null must be a boolean\"\n        raise TypeError(error_message)\n    if not isinstance(drop_duplicate_records, bool):\n        error_message = \"drop_duplicate_records must be a boolean\"\n        raise TypeError(error_message)\n    if additional_fields and not isinstance(additional_fields, list):\n        error_message = \"additional_fields must be a list of columns\"\n        raise TypeError(error_message)\n\n    sap_marc = df\n\n    # Apply deletion flag filter before selecting columns\n    if check_deletion_flag_is_null:\n        sap_marc = sap_marc.filter(F.col(\"LVORM\").isNull())\n\n    # Select and rename columns\n    columns = [\n        \"SOURCE_SYSTEM_ERP\",  # Source ERP system identifier\n        \"MATNR\",  # Material Number\n        \"WERKS\",  # Plant\n    ]\n\n    if additional_fields:\n        columns.extend([F.col(field) for field in additional_fields])\n\n    sap_marc = sap_marc.select(*columns)\n\n    if drop_duplicate_records:\n        sap_marc = sap_marc.drop_duplicates()\n\n    return sap_marc\n</code></pre>"},{"location":"reference/code/modules/local_material/#code.modules.local_material.prep_valuation_data","title":"<code>prep_valuation_data(df)</code>","text":"<p>Prepare Valuation data for harmonization across systems.</p> <p>Input Data: SAP T001K table (Valuation Area)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Valuation data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with prepared Valuation data.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input DataFrame is not a DataFrame.</p> Source code in <code>code\\modules\\local_material.py</code> <pre><code>def prep_valuation_data(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Prepare Valuation data for harmonization across systems.\n\n    Input Data: SAP T001K table (Valuation Area)\n\n    Args:\n        df (DataFrame): DataFrame with Valuation data.\n\n    Returns:\n        DataFrame: DataFrame with prepared Valuation data.\n\n    Raises:\n        TypeError: If the input DataFrame is not a DataFrame.\n    \"\"\"\n    if not isinstance(df, DataFrame):\n        error_message = \"df must be a DataFrame\"\n        raise TypeError(error_message)\n\n    columns = [\n        \"MANDT\",  # Client\n        \"BWKEY\",  # Valuation Area\n        \"BUKRS\",  # Company Code\n    ]\n\n    return df.select(*columns).drop_duplicates()\n</code></pre>"},{"location":"reference/code/modules/process_order/","title":"process_order","text":"<p>This module contains processes for order management.</p>"},{"location":"reference/code/modules/transformations/","title":"transformations","text":""},{"location":"reference/code/modules/utils/","title":"utils","text":"<p>Utility functions for the Spark application.</p>"},{"location":"reference/code/modules/utils/#code.modules.utils.create_spark_session","title":"<code>create_spark_session(app_name)</code>","text":"<p>Create a Spark session.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the Spark application.</p> required <p>Returns:</p> Name Type Description <code>SparkSession</code> <code>SparkSession</code> <p>Spark session object.</p> Source code in <code>code\\modules\\utils.py</code> <pre><code>def create_spark_session(app_name: str) -&gt; SparkSession:\n    \"\"\"Create a Spark session.\n\n    Args:\n        app_name (str): Name of the Spark application.\n\n    Returns:\n        SparkSession: Spark session object.\n    \"\"\"\n    os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n\n    return SparkSession.builder.appName(app_name).config(\"spark.sql.repl.eagerEval.enablede\", True).getOrCreate()\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Create a logger object with both console and file handlers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: Logger object.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If unable to create logs directory or log file.</p> Source code in <code>code\\modules\\utils.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"Create a logger object with both console and file handlers.\n\n    Args:\n        name (str): Name of the logger.\n\n    Returns:\n        logging.Logger: Logger object.\n\n    Raises:\n        OSError: If unable to create logs directory or log file.\n    \"\"\"\n    try:\n        # Create logs directory if it doesn't exist\n        log_dir = Path(\"logs\")\n        log_dir.mkdir(exist_ok=True)\n\n        # Create logger\n        logger = logging.getLogger(name)\n        logger.setLevel(logging.INFO)\n\n        # Clear existing handlers to avoid duplicates\n        if logger.handlers:\n            logger.handlers.clear()\n\n        # Create formatters\n        formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n        # File handler\n        file_handler = logging.FileHandler(filename=log_dir / \"spark.log\", encoding=\"utf-8\", mode=\"a\")\n        file_handler.setLevel(logging.INFO)\n        file_handler.setFormatter(formatter)\n\n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n        console_handler.setFormatter(formatter)\n\n        # Add handlers to logger\n        logger.addHandler(file_handler)\n        logger.addHandler(console_handler)\n\n    except OSError as e:\n        msg = f\"Failed to setup logger: {e!s}\"\n        raise OSError(msg) from e\n    else:\n        return logger\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.profile_data","title":"<code>profile_data(df)</code>","text":"<p>Profile the data in a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame object.</p> required Source code in <code>code\\modules\\utils.py</code> <pre><code>def profile_data(df: DataFrame) -&gt; None:\n    \"\"\"Profile the data in a Spark DataFrame.\n\n    Args:\n        df (DataFrame): Spark DataFrame object.\n    \"\"\"\n    try:\n        logger.info(\"Data profiling results:\")\n        df.printSchema()\n\n        df.describe().show()\n        df.show(5)\n\n    except Exception as e:\n        logger.exception(\"An error occurred while profiling the data.\", extra=e)\n</code></pre>"},{"location":"reference/code/modules/utils/#code.modules.utils.read_csv_file","title":"<code>read_csv_file(spark, file_directory, infer_schema=True, schema=None)</code>","text":"<p>Read a CSV file into a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required <code>file_directory</code> <code>str</code> <p>Path to the CSV file.</p> required <code>infer_schema</code> <code>bool</code> <p>Whether to infer the schema of the CSV file.</p> <code>True</code> <code>schema</code> <code>str</code> <p>Schema of the CSV file.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Spark DataFrame object.</p> Source code in <code>code\\modules\\utils.py</code> <pre><code>def read_csv_file(\n    spark: SparkSession,\n    file_directory: str,\n    infer_schema: bool = True,\n    schema: str | None = None,\n) -&gt; DataFrame:\n    \"\"\"Read a CSV file into a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session object.\n        file_directory (str): Path to the CSV file.\n        infer_schema (bool): Whether to infer the schema of the CSV file.\n        schema (str): Schema of the CSV file.\n\n    Returns:\n        DataFrame: Spark DataFrame object.\n    \"\"\"\n    encoding = \"utf-8\"\n\n    try:\n        if infer_schema:\n            csv_df = spark.read.csv(f\"{file_directory}\", header=True, inferSchema=True, encoding=encoding)\n        else:\n            csv_df = spark.read.csv(f\"{file_directory}\", header=True, schema=schema, encoding=encoding)\n\n    except Exception as e:\n        logger.exception(\"An error occurred while reading the CSV file.\", extra=e)\n        return None\n    else:\n        return csv_df\n</code></pre>"},{"location":"reference/code/tests/","title":"tests","text":""},{"location":"reference/code/tests/conftest/","title":"conftest","text":"<p>This module contains the fixtures for the tests.</p>"},{"location":"reference/code/tests/conftest/#code.tests.conftest.spark","title":"<code>spark()</code>","text":"<p>Fixture for creating a Spark session.</p> <p>Yields:</p> Name Type Description <code>SparkSession</code> <code>Any</code> <p>Spark session object.</p> Source code in <code>code\\tests\\conftest.py</code> <pre><code>@pytest.fixture(scope=\"session\")\ndef spark() -&gt; Generator[Any, Any, Any]:\n    \"\"\"Fixture for creating a Spark session.\n\n    Yields:\n        SparkSession: Spark session object.\n    \"\"\"\n    findspark.init()\n    spark = SparkSession.builder.appName(\"pytest\").getOrCreate()\n\n    yield spark\n    spark.stop()\n</code></pre>"},{"location":"reference/code/tests/test_utils/","title":"test_utils","text":"<p>Tests for utils module.</p>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_create_spark_session","title":"<code>test_create_spark_session()</code>","text":"<p>Test create_spark_session function.</p> Source code in <code>code\\tests\\test_utils.py</code> <pre><code>def test_create_spark_session():\n    \"\"\"Test create_spark_session function.\"\"\"\n    spark = create_spark_session(\"test\")\n\n    assert spark is not None, \"Spark session object is None\"\n    assert (\n        spark.conf.get(\"spark.app.name\") == \"test\"\n    ), \"Spark application name is incorrect\"\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_read_csv_file_with_inferred_schema","title":"<code>test_read_csv_file_with_inferred_schema(spark)</code>","text":"<p>Test read_csv_file function.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_utils.py</code> <pre><code>def test_read_csv_file_with_inferred_schema(spark: SparkSession):\n    \"\"\"Test read_csv_file function.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    # Test reading a CSV file with inferred schema\n\n    # CSV file content\n    # name,age\n    # Alice,25\n    # Bob,30\n    # Charlie,35\n\n    file_directory = \"data/test/test_data.csv\"\n    df = read_csv_file(spark, file_directory, infer_schema=True)\n\n    assert df is not None, \"DataFrame is None\"\n    assert df.count() == 3, \"DataFrame count is incorrect\"\n    assert df.columns == [\"name\", \"age\"], \"DataFrame columns are incorrect\"\n</code></pre>"},{"location":"reference/code/tests/test_utils/#code.tests.test_utils.test_read_csv_file_with_specified_schema","title":"<code>test_read_csv_file_with_specified_schema(spark)</code>","text":"<p>Test read_csv_file function with specified schema.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_utils.py</code> <pre><code>def test_read_csv_file_with_specified_schema(spark: SparkSession):\n    \"\"\"Test read_csv_file function with specified schema.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    # Test reading a CSV file with specified schema\n\n    # CSV file content\n    # name,age\n    # Alice,25\n    # Bob,30\n    # Charlie,35\n\n    file_directory = \"data/test/test_data.csv\"\n\n    # Define schema\n    schema = StructType(\n        [\n            StructField(\"name\", StringType(), True),\n            StructField(\"age\", IntegerType(), True),\n        ]\n    )\n\n    # Read CSV file with specified schema\n    df = read_csv_file(spark, file_directory, infer_schema=False, schema=schema)\n\n    # Assert results\n    assert df is not None\n    assert df.count() == 3\n    assert df.columns == [\"name\", \"age\"]\n    assertSchemaEqual(df.schema, schema)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/","title":"test_local_material","text":""},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/","title":"test_derive_intra_and_inter_primary_key","text":""},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys","title":"<code>test_derive_primary_keys(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function.</p> Source code in <code>code\\tests\\test_local_material\\test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function.\"\"\"\n    # Setup schema\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    # Test case 1: Normal case\n    data = [(\"SYS1\", \"MAT1\", \"PLANT1\"), (\"SYS2\", \"MAT2\", \"PLANT2\")]\n    input_df = spark.createDataFrame(data, schema)\n    result = derive_intra_and_inter_primary_key(input_df)\n\n    # Verify results\n    assert \"primary_key_intra\" in result.columns, \"Result should contain primary_key_intra column\"\n    assert \"primary_key_inter\" in result.columns, \"Result should contain primary_key_inter column\"\n\n    first_row = result.first()\n    assert first_row[\"primary_key_intra\"] == \"MAT1-PLANT1\", \"Primary key should be derived correctly\"\n    assert first_row[\"primary_key_inter\"] == \"SYS1-MAT1-PLANT1\", \"Primary key should be derived correctly\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys_empty_df","title":"<code>test_derive_primary_keys_empty_df(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function with an empty DataFrame.</p> Source code in <code>code\\tests\\test_local_material\\test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys_empty_df(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function with an empty DataFrame.\"\"\"\n    # Test empty DataFrame\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n    empty_df = spark.createDataFrame([], schema)\n    result = derive_intra_and_inter_primary_key(empty_df)\n    assert result.count() == 0, \"Result DataFrame should be empty\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys_missing_columns","title":"<code>test_derive_primary_keys_missing_columns(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function with missing required columns.</p> Source code in <code>code\\tests\\test_local_material\\test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys_missing_columns(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function with missing required columns.\"\"\"\n    # Test missing required columns\n    invalid_schema = StructType(\n        [StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True), StructField(\"MATNR\", StringType(), True)]\n    )\n    invalid_df = spark.createDataFrame([(\"SYS1\", \"MAT1\")], invalid_schema)\n\n    with pytest.raises(Exception):\n        derive_intra_and_inter_primary_key(invalid_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_derive_intra_and_inter_primary_key/#code.tests.test_local_material.test_derive_intra_and_inter_primary_key.test_derive_primary_keys_with_nulls","title":"<code>test_derive_primary_keys_with_nulls(spark)</code>","text":"<p>Test derive_intra_and_inter_primary_key function with null values.</p> Source code in <code>code\\tests\\test_local_material\\test_derive_intra_and_inter_primary_key.py</code> <pre><code>def test_derive_primary_keys_with_nulls(spark):\n    \"\"\"Test derive_intra_and_inter_primary_key function with null values.\"\"\"\n    # Setup schema\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    # Test with various null combinations\n    data = [(\"SYS1\", None, \"PLANT1\"), (\"SYS2\", \"MAT2\", None), (None, \"MAT3\", \"PLANT3\")]\n    input_df = spark.createDataFrame(data, schema)\n    result = derive_intra_and_inter_primary_key(input_df)\n\n    # Check results row by row\n    rows = result.collect()\n\n    # First row: null MATNR\n    assert rows[0][\"primary_key_intra\"] == \"-PLANT1\", \"Primary key should be derived correctly\"\n    assert rows[0][\"primary_key_inter\"] == \"SYS1--PLANT1\", \"Primary key should be derived correctly\"\n\n    # Second row: null WERKS\n    assert rows[1][\"primary_key_intra\"] == \"MAT2-\", \"Primary key should be derived correctly\"\n    assert rows[1][\"primary_key_inter\"] == \"SYS2-MAT2-\", \"Primary key should be derived correctly\"\n\n    # Third row: null SOURCE_SYSTEM_ERP\n    assert rows[2][\"primary_key_intra\"] == \"MAT3-PLANT3\", \"Primary key should be derived correctly\"\n    assert rows[2][\"primary_key_inter\"] == \"-MAT3-PLANT3\", \"Primary key should be derived correctly\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_integration/","title":"test_integration","text":""},{"location":"reference/code/tests/test_local_material/test_integration/#code.tests.test_local_material.test_integration.test_integration_empty_dataframes","title":"<code>test_integration_empty_dataframes(spark)</code>","text":"<p>Test integration with empty DataFrames.</p> Source code in <code>code\\tests\\test_local_material\\test_integration.py</code> <pre><code>def test_integration_empty_dataframes(spark):\n    \"\"\"Test integration with empty DataFrames.\"\"\"\n    # Define schemas\n    sap_mara_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n        ]\n    )\n\n    sap_mbew_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"VPRSV\", StringType(), True),\n            StructField(\"VERPR\", DoubleType(), True),\n            StructField(\"STPRS\", DoubleType(), True),\n            StructField(\"PEINH\", LongType(), True),\n            StructField(\"BKLAS\", StringType(), True),\n        ]\n    )\n\n    sap_marc_schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    sap_t001k_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n        ]\n    )\n\n    sap_t001w_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"NAME1\", StringType(), True),\n        ]\n    )\n\n    sap_t001_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n            StructField(\"WAERS\", StringType(), True),\n        ]\n    )\n\n    # Create empty DataFrames with schemas\n    sap_mara = spark.createDataFrame([], sap_mara_schema)\n    sap_mbew = spark.createDataFrame([], sap_mbew_schema)\n    sap_marc = spark.createDataFrame([], sap_marc_schema)\n    sap_t001k = spark.createDataFrame([], sap_t001k_schema)\n    sap_t001w = spark.createDataFrame([], sap_t001w_schema)\n    sap_t001 = spark.createDataFrame([], sap_t001_schema)\n\n    # Execute integration\n    result_df = integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)\n\n    # Verify result is empty DataFrame\n    assert result_df.count() == 0\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_integration/#code.tests.test_local_material.test_integration.test_integration_happy_path","title":"<code>test_integration_happy_path(spark)</code>","text":"<p>Test integration with complete matching data.</p> Source code in <code>code\\tests\\test_local_material\\test_integration.py</code> <pre><code>def test_integration_happy_path(spark):\n    \"\"\"Test integration with complete matching data.\"\"\"\n    # Setup test data\n    sap_mara_data = [(\"100\", \"MAT1\", \"UOM1\", \"GMN1\")]\n    sap_mbew_data = [(\"100\", \"MAT1\", \"VAL1\", \"PC1\", 10.0, 20.0, 1, \"VC1\")]\n    sap_marc_data = [(\"ERP1\", \"MAT1\", \"PLANT1\")]\n    sap_t001k_data = [(\"100\", \"VAL1\", \"COMP1\")]\n    sap_t001w_data = [(\"100\", \"PLANT1\", \"VAL1\", \"Plant Name\")]\n    sap_t001_data = [(\"100\", \"COMP1\", \"USD\")]\n\n    # Create DataFrames\n    sap_mara = spark.createDataFrame(sap_mara_data, [\"MANDT\", \"MATNR\", \"MEINS\", \"GLOBAL_MATERIAL_NUMBER\"])\n    sap_mbew = spark.createDataFrame(\n        sap_mbew_data, [\"MANDT\", \"MATNR\", \"BWKEY\", \"VPRSV\", \"VERPR\", \"STPRS\", \"PEINH\", \"BKLAS\"]\n    )\n    sap_marc = spark.createDataFrame(sap_marc_data, [\"SOURCE_SYSTEM_ERP\", \"MATNR\", \"WERKS\"])\n    sap_t001k = spark.createDataFrame(sap_t001k_data, [\"MANDT\", \"BWKEY\", \"BUKRS\"])\n    sap_t001w = spark.createDataFrame(sap_t001w_data, [\"MANDT\", \"WERKS\", \"BWKEY\", \"NAME1\"])\n    sap_t001 = spark.createDataFrame(sap_t001_data, [\"MANDT\", \"BUKRS\", \"WAERS\"])\n\n    # Execute integration\n    result_df = integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)\n\n    # Expected data\n    expected_data = [\n        (\n            \"ERP1\",\n            \"MAT1\",\n            \"PLANT1\",\n            \"100\",\n            \"UOM1\",\n            \"GMN1\",\n            \"VAL1\",\n            \"Plant Name\",\n            \"PC1\",\n            10.0,\n            20.0,\n            1,\n            \"VC1\",\n            \"COMP1\",\n            \"USD\",\n        )\n    ]\n\n    # Create expected DataFrame with explicit column order\n    columns = [\n        \"SOURCE_SYSTEM_ERP\",\n        \"MATNR\",\n        \"WERKS\",\n        \"MANDT\",\n        \"MEINS\",\n        \"GLOBAL_MATERIAL_NUMBER\",\n        \"BWKEY\",\n        \"NAME1\",\n        \"VPRSV\",\n        \"VERPR\",\n        \"STPRS\",\n        \"PEINH\",\n        \"BKLAS\",\n        \"BUKRS\",\n        \"WAERS\",\n    ]\n\n    expected_df = spark.createDataFrame(expected_data, columns)\n\n    # Ensure result has same column order before comparison\n    result_df_ordered = result_df.select(*columns)\n\n    assertDataFrameEqual(result_df_ordered, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_integration/#code.tests.test_local_material.test_integration.test_integration_null_values","title":"<code>test_integration_null_values(spark)</code>","text":"<p>Test integration with null values.</p> Source code in <code>code\\tests\\test_local_material\\test_integration.py</code> <pre><code>def test_integration_null_values(spark):\n    \"\"\"Test integration with null values.\"\"\"\n    # Define schemas with explicit types\n    sap_mara_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"MEINS\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n        ]\n    )\n\n    sap_mbew_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"VPRSV\", StringType(), True),\n            StructField(\"VERPR\", DoubleType(), True),\n            StructField(\"STPRS\", DoubleType(), True),\n            StructField(\"PEINH\", LongType(), True),\n            StructField(\"BKLAS\", StringType(), True),\n        ]\n    )\n\n    sap_marc_schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n        ]\n    )\n\n    sap_t001k_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n        ]\n    )\n\n    sap_t001w_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n            StructField(\"BWKEY\", StringType(), True),\n            StructField(\"NAME1\", StringType(), True),\n        ]\n    )\n\n    sap_t001_schema = StructType(\n        [\n            StructField(\"MANDT\", StringType(), True),\n            StructField(\"BUKRS\", StringType(), True),\n            StructField(\"WAERS\", StringType(), True),\n        ]\n    )\n\n    # Test data with nulls\n    sap_mara = spark.createDataFrame([(\"100\", \"MAT1\", None, None)], sap_mara_schema)\n    sap_mbew = spark.createDataFrame([(\"100\", \"MAT1\", \"VAL1\", None, None, None, None, None)], sap_mbew_schema)\n    sap_marc = spark.createDataFrame([(\"ERP1\", \"MAT1\", \"PLANT1\")], sap_marc_schema)\n    sap_t001k = spark.createDataFrame([(\"100\", \"VAL1\", None)], sap_t001k_schema)\n    sap_t001w = spark.createDataFrame([(\"100\", \"PLANT1\", \"VAL1\", None)], sap_t001w_schema)\n    sap_t001 = spark.createDataFrame([(\"100\", None, None)], sap_t001_schema)\n\n    # Execute integration\n    result_df = integration(sap_mara, sap_mbew, sap_marc, sap_t001k, sap_t001w, sap_t001)\n\n    # Verify results\n    assert result_df.count() &gt; 0, \"Result DataFrame should not be empty\"\n\n    # Get first row\n    first_row = result_df.first()\n    assert first_row is not None, \"First row should not be None\"\n\n    # Verify null values in specific columns\n    assert first_row[\"MEINS\"] is None, \"MEINS column should be None\"\n    assert first_row[\"GLOBAL_MATERIAL_NUMBER\"] is None, \"GLOBAL_MATERIAL_NUMBER column should be None\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_post_prep_local_material/","title":"test_post_prep_local_material","text":""},{"location":"reference/code/tests/test_local_material/test_post_prep_local_material/#code.tests.test_local_material.test_post_prep_local_material.test_post_prep_local_material","title":"<code>test_post_prep_local_material(spark)</code>","text":"<p>Test post-preparation for Local Material data.</p> Source code in <code>code\\tests\\test_local_material\\test_post_prep_local_material.py</code> <pre><code>def test_post_prep_local_material(spark):\n    \"\"\"Test post-preparation for Local Material data.\"\"\"\n    # Setup test schema\n    schema = StructType(\n        [\n            StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True),\n            StructField(\"MATNR\", StringType(), True),\n            StructField(\"WERKS\", StringType(), True),\n            StructField(\"NAME1\", StringType(), True),\n            StructField(\"GLOBAL_MATERIAL_NUMBER\", StringType(), True),\n        ]\n    )\n\n    # Test case 1: Normal case\n    test_data = [(\"SYS1\", \"MAT1\", \"PLANT1\", \"Plant Name 1\", \"GMAT1\"), (\"SYS1\", \"MAT2\", \"PLANT2\", \"Plant Name 2\", None)]\n    input_df = spark.createDataFrame(test_data, schema)\n    result = post_prep_local_material(input_df)\n\n    # Verify results\n    assert result.count() == 2\n    assert \"mtl_plant_emd\" in result.columns\n    assert \"global_mtl_id\" in result.columns\n    assert \"primary_key_intra\" in result.columns\n    assert \"primary_key_inter\" in result.columns\n\n    # Test case 2: Duplicate handling\n    dup_data = [\n        (\"SYS1\", \"MAT1\", \"PLANT1\", \"Plant Name 1\", \"GMAT1\"),\n        (\"SYS1\", \"MAT1\", \"PLANT1\", \"Plant Name 1\", \"GMAT1\"),\n    ]\n    dup_df = spark.createDataFrame(dup_data, schema)\n    dup_result = post_prep_local_material(dup_df)\n    assert dup_result.count() == 1\n\n    # Test case 3: Null values\n    null_data = [(\"SYS1\", \"MAT1\", \"PLANT1\", None, None)]\n    null_df = spark.createDataFrame(null_data, schema)\n    null_result = post_prep_local_material(null_df)\n    assert null_result.filter(F.col(\"global_mtl_id\") == \"MAT1\").count() == 1\n\n    # Test case 4: Empty DataFrame\n    empty_df = spark.createDataFrame([], schema)\n    empty_result = post_prep_local_material(empty_df)\n    assert empty_result.count() == 0\n\n    # Test case 5: Column validation\n    invalid_schema = StructType(\n        [StructField(\"SOURCE_SYSTEM_ERP\", StringType(), True), StructField(\"MATNR\", StringType(), True)]\n    )\n    invalid_df = spark.createDataFrame([], invalid_schema)\n    with pytest.raises(Exception):\n        post_prep_local_material(invalid_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/","title":"test_prep_general_material_data","text":""},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_bismt_filtering","title":"<code>test_prep_general_material_data_bismt_filtering(spark)</code>","text":"<p>Test prep_general_material_data with BISMT filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_bismt_filtering(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with BISMT filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\", None, None),\n        (\"100\", \"10000002\", \"EA\", \"10000002\", \"ARCHIVE\", None),\n        (\"100\", \"10000003\", \"EA\", \"10000003\", \"DUPLICATE\", None),\n        (\"100\", \"10000004\", \"EA\", \"10000004\", \"RENUMBERED\", None),\n        (\"100\", \"10000005\", \"EA\", \"10000005\", \"VALID\", None),\n    ]\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\"),\n        (\"100\", \"10000005\", \"EA\", \"10000005\"),\n    ]\n    expected_schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_general_material_data(\n        input_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_empty_df","title":"<code>test_prep_general_material_data_empty_df(spark)</code>","text":"<p>Test prep_general_material_data with empty DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_empty_df(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with empty DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    empty_df = spark.createDataFrame([], schema=schema)\n\n    result_df = prep_general_material_data(\n        empty_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assert result_df.count() == 0\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_invalid_input","title":"<code>test_prep_general_material_data_invalid_input()</code>","text":"<p>Test prep_general_material_data with invalid input type.</p> <p>This test should raise a TypeError.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_invalid_input():\n    \"\"\"Test prep_general_material_data with invalid input type.\n\n    This test should raise a TypeError.\n    \"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_general_material_data(\n            None,\n            col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n            check_old_material_number_is_valid=True,\n            check_material_is_not_deleted=True,\n        )\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_lvorm_filtering","title":"<code>test_prep_general_material_data_lvorm_filtering(spark)</code>","text":"<p>Test prep_general_material_data with LVORM filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_lvorm_filtering(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with LVORM filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\", None, None),\n        (\"100\", \"10000002\", \"EA\", \"10000002\", None, \"X\"),\n        (\"100\", \"10000003\", \"EA\", \"10000003\", None, \"\"),\n    ]\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"EA\", \"10000001\"),\n        (\"100\", \"10000003\", \"EA\", \"10000003\"),\n    ]\n    expected_schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_general_material_data(\n        input_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_general_material_data/#code.tests.test_local_material.test_prep_general_material_data.test_prep_general_material_data_valid_input","title":"<code>test_prep_general_material_data_valid_input(spark)</code>","text":"<p>Test prep_general_material_data with valid input data.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_general_material_data.py</code> <pre><code>def test_prep_general_material_data_valid_input(spark: SparkSession):\n    \"\"\"Test prep_general_material_data with valid input data.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\n            \"100\",\n            \"10000001\",\n            \"EA\",\n            \"10000001\",\n            None,\n            None,\n        ),  # Should pass - null BISMT, null LVORM\n        (\n            \"100\",\n            \"10000002\",\n            \"EA\",\n            \"10000002\",\n            \"ARCHIVE\",\n            None,\n        ),  # Should fail - ARCHIVE in BISMT\n        (\n            \"100\",\n            \"10000003\",\n            \"EA\",\n            \"10000003\",\n            None,\n            \"X\",\n        ),  # Should fail - non-empty LVORM\n    ]\n    schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING, BISMT STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [(\"100\", \"10000001\", \"EA\", \"10000001\")]\n    expected_schema = \"MANDT STRING, MATNR STRING, MEINS STRING, GLOBAL_MATERIAL_NUMBER STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_general_material_data(\n        input_df,\n        col_mara_global_material_number=\"GLOBAL_MATERIAL_NUMBER\",\n        check_old_material_number_is_valid=True,\n        check_material_is_not_deleted=True,\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/","title":"test_prep_material_valuation","text":""},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_bwtar_filtering","title":"<code>test_prep_material_valuation_bwtar_filtering(spark)</code>","text":"<p>Test prep_material_valuation with BWTAR filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_bwtar_filtering(spark):\n    \"\"\"Test prep_material_valuation with BWTAR filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\n            \"100\",\n            \"10000001\",\n            \"1000\",\n            \"S\",\n            15.0,\n            25.0,\n            1,\n            \"3000\",\n            None,\n            \"001\",\n            \"20240102\",\n        ),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\"),\n    ]\n    expected_schema = (\n        \"MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING, VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\"\n    )\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_complex_scenario","title":"<code>test_prep_material_valuation_complex_scenario(spark)</code>","text":"<p>Test combination of filtering, row numbering and deduplication.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_complex_scenario(spark):\n    \"\"\"Test combination of filtering, row numbering and deduplication.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        # Group 1: Valid rows with different LAEPR\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        # Group 1: Duplicate of latest record\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        # Group 1: Row with LVORM flag (should be filtered)\n        (\"100\", \"10000001\", \"1000\", \"S\", 20.0, 30.0, 1, \"3000\", \"X\", None, \"20240103\"),\n        # Group 1: Row with BWTAR (should be filtered)\n        (\n            \"100\",\n            \"10000001\",\n            \"1000\",\n            \"S\",\n            25.0,\n            35.0,\n            1,\n            \"3000\",\n            None,\n            \"001\",\n            \"20240104\",\n        ),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    # Should select only the latest valid record\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\"),\n    ]\n    expected_schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_deduplication","title":"<code>test_prep_material_valuation_deduplication(spark)</code>","text":"<p>Test deduplication of identical rows.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_deduplication(spark):\n    \"\"\"Test deduplication of identical rows.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        # Duplicate rows with same values\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    # Should remove duplicates\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\"),\n    ]\n    expected_schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_empty_df","title":"<code>test_prep_material_valuation_empty_df(spark)</code>","text":"<p>Test prep_material_valuation with empty DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_empty_df(spark):\n    \"\"\"Test prep_material_valuation with empty DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    empty_df = spark.createDataFrame([], schema=schema)\n    result_df = prep_material_valuation(empty_df)\n    assert result_df.count() == 0\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_invalid_input","title":"<code>test_prep_material_valuation_invalid_input()</code>","text":"<p>Test prep_material_valuation with invalid input type.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_invalid_input():\n    \"\"\"Test prep_material_valuation with invalid input type.\"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_material_valuation(None)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_lvorm_filtering","title":"<code>test_prep_material_valuation_lvorm_filtering(spark)</code>","text":"<p>Test prep_material_valuation with LVORM filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_lvorm_filtering(spark):\n    \"\"\"Test prep_material_valuation with LVORM filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", \"X\", None, \"20240102\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\"),\n    ]\n    expected_schema = (\n        \"MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING, VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\"\n    )\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_row_numbering","title":"<code>test_prep_material_valuation_row_numbering(spark)</code>","text":"<p>Test row numbering logic based on LAEPR ordering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_row_numbering(spark):\n    \"\"\"Test row numbering logic based on LAEPR ordering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        # Same MATNR/BWKEY group with different LAEPR dates\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        # Different MATNR/BWKEY group\n        (\"100\", \"10000002\", \"2000\", \"V\", 5.0, 10.0, 1, \"4000\", None, None, \"20240101\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\", None, None, \"20240102\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    # Should select latest LAEPR for each MATNR/BWKEY group\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\"),\n    ]\n    expected_schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_material_valuation/#code.tests.test_local_material.test_prep_material_valuation.test_prep_material_valuation_valid_input","title":"<code>test_prep_material_valuation_valid_input(spark)</code>","text":"<p>Test prep_material_valuation with valid input data.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_material_valuation.py</code> <pre><code>def test_prep_material_valuation_valid_input(spark):\n    \"\"\"Test prep_material_valuation with valid input data.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 10.0, 20.0, 1, \"3000\", None, None, \"20240101\"),\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\", None, None, \"20240102\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 5.0, 10.0, 1, \"4000\", None, None, \"20240101\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\", None, None, \"20240102\"),\n    ]\n    schema = \"\"\"\n        MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING,\n        VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING,\n        LVORM STRING, BWTAR STRING, LAEPR STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"10000001\", \"1000\", \"S\", 15.0, 25.0, 1, \"3000\"),\n        (\"100\", \"10000002\", \"2000\", \"V\", 7.0, 12.0, 1, \"4000\"),\n    ]\n    expected_schema = (\n        \"MANDT STRING, MATNR STRING, BWKEY STRING, VPRSV STRING, VERPR DOUBLE, STPRS DOUBLE, PEINH INT, BKLAS STRING\"\n    )\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_material_valuation(input_df)\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/","title":"test_prep_plant_and_branches","text":""},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/#code.tests.test_local_material.test_prep_plant_and_branches.test_prep_plant_and_branches_invalid_input","title":"<code>test_prep_plant_and_branches_invalid_input()</code>","text":"<p>Test prep_plant_and_branches with invalid input type.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_plant_and_branches.py</code> <pre><code>def test_prep_plant_and_branches_invalid_input():\n    \"\"\"Test prep_plant_and_branches with invalid input type.\"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_plant_and_branches(None)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/#code.tests.test_local_material.test_prep_plant_and_branches.test_prep_plant_and_branches_missing_columns","title":"<code>test_prep_plant_and_branches_missing_columns(spark)</code>","text":"<p>Test prep_plant_and_branches with missing columns in input DataFrame.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_plant_and_branches.py</code> <pre><code>def test_prep_plant_and_branches_missing_columns(spark):\n    \"\"\"Test prep_plant_and_branches with missing columns in input DataFrame.\"\"\"\n    data = [\n        (\"100\", \"PLANT1\", \"VAL1\"),\n        (\"100\", \"PLANT2\", \"VAL2\"),\n    ]\n    schema = \"MANDT STRING, WERKS STRING, BWKEY STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    with pytest.raises(Exception):\n        prep_plant_and_branches(input_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_and_branches/#code.tests.test_local_material.test_prep_plant_and_branches.test_prep_plant_and_branches_valid_input","title":"<code>test_prep_plant_and_branches_valid_input(spark)</code>","text":"<p>Test prep_plant_and_branches with valid input data.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_plant_and_branches.py</code> <pre><code>def test_prep_plant_and_branches_valid_input(spark):\n    \"\"\"Test prep_plant_and_branches with valid input data.\"\"\"\n    data = [\n        (\"100\", \"PLANT1\", \"VAL1\", \"Plant 1\"),\n        (\"100\", \"PLANT2\", \"VAL2\", \"Plant 2\"),\n    ]\n    schema = \"MANDT STRING, WERKS STRING, BWKEY STRING, NAME1 STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"PLANT1\", \"VAL1\", \"Plant 1\"),\n        (\"100\", \"PLANT2\", \"VAL2\", \"Plant 2\"),\n    ]\n    expected_schema = \"MANDT STRING, WERKS STRING, BWKEY STRING, NAME1 STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_and_branches(input_df)\n\n    assertDataFrameEqual(result_df, expected_df), \"DataFrames are not equal\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/","title":"test_prep_plant_data_for_material","text":""},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_additional_fields","title":"<code>test_prep_plant_data_for_material_additional_fields(spark)</code>","text":"<p>Test additional fields handling.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_additional_fields(spark):\n    \"\"\"Test additional fields handling.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None, \"CAT1\"),\n        (\"SYS1\", \"10000002\", \"PLANT2\", None, \"CAT2\"),\n    ]\n    schema = \"\"\"\n        SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING,\n        LVORM STRING, CATEGORY STRING\n    \"\"\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", \"CAT1\"),\n        (\"SYS1\", \"10000002\", \"PLANT2\", \"CAT2\"),\n    ]\n    expected_schema = \"\"\"\n        SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING,\n        CATEGORY STRING\n    \"\"\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df,\n        check_deletion_flag_is_null=True,\n        drop_duplicate_records=False,\n        additional_fields=[\"CATEGORY\"],\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_deletion_flag","title":"<code>test_prep_plant_data_for_material_deletion_flag(spark)</code>","text":"<p>Test deletion flag filtering.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_deletion_flag(spark):\n    \"\"\"Test deletion flag filtering.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n        (\"SYS1\", \"10000002\", \"PLANT2\", \"X\"),\n    ]\n    schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [(\"SYS1\", \"10000001\", \"PLANT1\")]\n    expected_schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df, check_deletion_flag_is_null=True, drop_duplicate_records=False\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_duplicates","title":"<code>test_prep_plant_data_for_material_duplicates(spark)</code>","text":"<p>Test duplicate record handling.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_duplicates(spark):\n    \"\"\"Test duplicate record handling.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n    ]\n    schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [(\"SYS1\", \"10000001\", \"PLANT1\")]\n    expected_schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df, check_deletion_flag_is_null=True, drop_duplicate_records=True\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_invalid_input","title":"<code>test_prep_plant_data_for_material_invalid_input(spark)</code>","text":"<p>Test with invalid input types.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_invalid_input(spark):\n    \"\"\"Test with invalid input types.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_plant_data_for_material(\n            None, check_deletion_flag_is_null=True, drop_duplicate_records=False\n        )\n\n    with pytest.raises(\n        TypeError, match=\"check_deletion_flag_is_null must be a boolean\"\n    ):\n        prep_plant_data_for_material(\n            spark.createDataFrame([], \"SOURCE_SYSTEM_ERP STRING\"),\n            check_deletion_flag_is_null=\"True\",\n            drop_duplicate_records=False,\n        )\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_plant_data_for_material/#code.tests.test_local_material.test_prep_plant_data_for_material.test_prep_plant_data_for_material_valid_input","title":"<code>test_prep_plant_data_for_material_valid_input(spark)</code>","text":"<p>Test with valid input and default parameters.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session object.</p> required Source code in <code>code\\tests\\test_local_material\\test_prep_plant_data_for_material.py</code> <pre><code>def test_prep_plant_data_for_material_valid_input(spark):\n    \"\"\"Test with valid input and default parameters.\n\n    Args:\n        spark (SparkSession): Spark session object.\n    \"\"\"\n    data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\", None),\n        (\"SYS1\", \"10000002\", \"PLANT2\", None),\n    ]\n    schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING, LVORM STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"SYS1\", \"10000001\", \"PLANT1\"),\n        (\"SYS1\", \"10000002\", \"PLANT2\"),\n    ]\n    expected_schema = \"SOURCE_SYSTEM_ERP STRING, MATNR STRING, WERKS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_plant_data_for_material(\n        input_df, check_deletion_flag_is_null=True, drop_duplicate_records=False\n    )\n\n    assertDataFrameEqual(result_df, expected_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/","title":"test_prep_valuation_data","text":"<p>Tests for prep_valuation_data function in local_material module.</p>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_duplicates","title":"<code>test_prep_valuation_data_duplicates(spark)</code>","text":"<p>Test prep_valuation_data with duplicate records.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_duplicates(spark):\n    \"\"\"Test prep_valuation_data with duplicate records.\"\"\"\n    data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n        (\"100\", \"VAL1\", \"COMP1\"),\n    ]\n    schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n    ]\n    expected_schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_valuation_data(input_df)\n\n    assertDataFrameEqual(result_df, expected_df), \"DataFrames are not equal\"\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_invalid_input","title":"<code>test_prep_valuation_data_invalid_input()</code>","text":"<p>Test prep_valuation_data with invalid input type.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_invalid_input():\n    \"\"\"Test prep_valuation_data with invalid input type.\"\"\"\n    with pytest.raises(TypeError, match=\"df must be a DataFrame\"):\n        prep_valuation_data(None)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_missing_columns","title":"<code>test_prep_valuation_data_missing_columns(spark)</code>","text":"<p>Test prep_valuation_data with missing columns in input DataFrame.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_missing_columns(spark):\n    \"\"\"Test prep_valuation_data with missing columns in input DataFrame.\"\"\"\n    data = [\n        (\"100\", \"VAL1\"),\n        (\"100\", \"VAL2\"),\n    ]\n    schema = \"MANDT STRING, BWKEY STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    with pytest.raises(Exception):\n        prep_valuation_data(input_df)\n</code></pre>"},{"location":"reference/code/tests/test_local_material/test_prep_valuation_data/#code.tests.test_local_material.test_prep_valuation_data.test_prep_valuation_data_valid_input","title":"<code>test_prep_valuation_data_valid_input(spark)</code>","text":"<p>Test prep_valuation_data with valid input data.</p> Source code in <code>code\\tests\\test_local_material\\test_prep_valuation_data.py</code> <pre><code>def test_prep_valuation_data_valid_input(spark):\n    \"\"\"Test prep_valuation_data with valid input data.\"\"\"\n    data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n        (\"100\", \"VAL2\", \"COMP2\"),\n    ]\n    schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    input_df = spark.createDataFrame(data, schema=schema)\n\n    expected_data = [\n        (\"100\", \"VAL1\", \"COMP1\"),\n        (\"100\", \"VAL2\", \"COMP2\"),\n    ]\n    expected_schema = \"MANDT STRING, BWKEY STRING, BUKRS STRING\"\n    expected_df = spark.createDataFrame(expected_data, schema=expected_schema)\n\n    result_df = prep_valuation_data(input_df)\n\n    assertDataFrameEqual(result_df, expected_df), \"DataFrames are not equal\"\n</code></pre>"},{"location":"reference/code/tests/test_process_order/","title":"test_process_order","text":""}]}